{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vblhh9zwb19Q",
        "outputId": "489150b7-f605-4fa4-9261-d9a34db1f3d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 35184, done.\u001b[K\n",
            "remote: Counting objects: 100% (105/105), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 35184 (delta 61), reused 72 (delta 47), pack-reused 35079\u001b[K\n",
            "Receiving objects: 100% (35184/35184), 25.22 MiB | 25.47 MiB/s, done.\n",
            "Resolving deltas: 100% (25548/25548), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/pytorch/fairseq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlaUVwgWJ8iz",
        "outputId": "89c51e0f-70d3-45b3-9b68-144758b7480d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxeSYwoIiDK4",
        "outputId": "55dcc777-f109-49da-829b-625a77bc4e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.10)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==0.12.2)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq==0.12.2)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.25.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2023.12.25)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq==0.12.2)\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.66.2)\n",
            "Collecting bitarray (from fairseq==0.12.2)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.2.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (24.0)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.11.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq==0.12.2)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq==0.12.2)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.22)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->fairseq==0.12.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->fairseq==0.12.2) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building editable for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-0.editable-cp310-cp310-linux_x86_64.whl size=9394 sha256=15f92b65c67dcdca33e97f3d3cb4b9a4cb6691d78350ce7ce4effbb635259eda\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-309adbdl/wheels/c6/d7/db/bc419b1daa8266aa8de2a7c4d29f62dbfa814e8701fe4695a2\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141211 sha256=2841ecda9098d53f844c5184072b2fc2f279c0878184fd3624e496628187a773\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, nvidia-cusolver-cu12, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.4.2\n"
          ]
        }
      ],
      "source": [
        "!cd /content/fairseq && pip install --editable ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSCYDFCfjoVn",
        "outputId": "dc961a42-5dda-488d-a132-af75561051c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running build\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-cpython-310\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/quantization_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/checkpoint_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/speech_generator.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/iterative_refinement_generator.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/version.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/hub_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/file_io.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/binarizer.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/file_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/sequence_generator.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/tokenizer.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/file_chunker_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/registry.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/incremental_decoding_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/sequence_scorer.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/options.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/nan_detector.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/pdb.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/ngram_repeat_block.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/utils.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/trainer.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/search.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "copying fairseq/token_generation_constraints.py -> build/lib.linux-x86_64-cpython-310/fairseq\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq_cli\n",
            "copying fairseq_cli/validate.py -> build/lib.linux-x86_64-cpython-310/fairseq_cli\n",
            "copying fairseq_cli/eval_lm.py -> build/lib.linux-x86_64-cpython-310/fairseq_cli\n",
            "copying fairseq_cli/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq_cli\n",
            "copying fairseq_cli/interactive.py -> build/lib.linux-x86_64-cpython-310/fairseq_cli\n",
            "copying fairseq_cli/hydra_validate.py -> build/lib.linux-x86_64-cpython-310/fairseq_cli\n",
            "copying fairseq_cli/hydra_train.py -> build/lib.linux-x86_64-cpython-310/fairseq_cli\n",
            "copying fairseq_cli/train.py -> build/lib.linux-x86_64-cpython-310/fairseq_cli\n",
            "copying fairseq_cli/generate.py -> build/lib.linux-x86_64-cpython-310/fairseq_cli\n",
            "copying fairseq_cli/preprocess.py -> build/lib.linux-x86_64-cpython-310/fairseq_cli\n",
            "copying fairseq_cli/score.py -> build/lib.linux-x86_64-cpython-310/fairseq_cli\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples\n",
            "copying fairseq/examples/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/distributed\n",
            "copying fairseq/distributed/fully_sharded_data_parallel.py -> build/lib.linux-x86_64-cpython-310/fairseq/distributed\n",
            "copying fairseq/distributed/module_proxy_wrapper.py -> build/lib.linux-x86_64-cpython-310/fairseq/distributed\n",
            "copying fairseq/distributed/tpu_distributed_data_parallel.py -> build/lib.linux-x86_64-cpython-310/fairseq/distributed\n",
            "copying fairseq/distributed/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/distributed\n",
            "copying fairseq/distributed/distributed_timeout_wrapper.py -> build/lib.linux-x86_64-cpython-310/fairseq/distributed\n",
            "copying fairseq/distributed/utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/distributed\n",
            "copying fairseq/distributed/legacy_distributed_data_parallel.py -> build/lib.linux-x86_64-cpython-310/fairseq/distributed\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/dataclass\n",
            "copying fairseq/dataclass/initialize.py -> build/lib.linux-x86_64-cpython-310/fairseq/dataclass\n",
            "copying fairseq/dataclass/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/dataclass\n",
            "copying fairseq/dataclass/configs.py -> build/lib.linux-x86_64-cpython-310/fairseq/dataclass\n",
            "copying fairseq/dataclass/utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/dataclass\n",
            "copying fairseq/dataclass/constants.py -> build/lib.linux-x86_64-cpython-310/fairseq/dataclass\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/multilingual_language_modeling.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/language_modeling.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/nlu_finetuning.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/sentence_prediction_adapters.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/audio_pretraining.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/fairseq_task.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/legacy_masked_lm.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/online_backtranslation.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/multires_hubert_pretraining.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/speech_dlm_task.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/span_masked_lm.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/multilingual_denoising.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/multilingual_translation.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/cross_lingual_lm.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/translation.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/sentence_prediction.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/translation_lev.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/speech_ulm_task.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/speech_to_speech.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/sentence_ranking.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/simultaneous_translation.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/translation_from_pretrained_xlm.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/translation_from_pretrained_bart.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/hubert_pretraining.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/masked_lm.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/audio_finetuning.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/frm_text_to_speech.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/audio_classification.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/speech_to_text.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/semisupervised_translation.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/denoising.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/multilingual_masked_lm.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/text_to_speech.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "copying fairseq/tasks/translation_multi_simple_epoch.py -> build/lib.linux-x86_64-cpython-310/fairseq/tasks\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/logging\n",
            "copying fairseq/logging/metrics.py -> build/lib.linux-x86_64-cpython-310/fairseq/logging\n",
            "copying fairseq/logging/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/logging\n",
            "copying fairseq/logging/progress_bar.py -> build/lib.linux-x86_64-cpython-310/fairseq/logging\n",
            "copying fairseq/logging/meters.py -> build/lib.linux-x86_64-cpython-310/fairseq/logging\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/benchmark\n",
            "copying fairseq/benchmark/dummy_lm.py -> build/lib.linux-x86_64-cpython-310/fairseq/benchmark\n",
            "copying fairseq/benchmark/dummy_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/benchmark\n",
            "copying fairseq/benchmark/dummy_model.py -> build/lib.linux-x86_64-cpython-310/fairseq/benchmark\n",
            "copying fairseq/benchmark/benchmark_multihead_attention.py -> build/lib.linux-x86_64-cpython-310/fairseq/benchmark\n",
            "copying fairseq/benchmark/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/benchmark\n",
            "copying fairseq/benchmark/dummy_masked_lm.py -> build/lib.linux-x86_64-cpython-310/fairseq/benchmark\n",
            "copying fairseq/benchmark/dummy_mt.py -> build/lib.linux-x86_64-cpython-310/fairseq/benchmark\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/config\n",
            "copying fairseq/config/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/cpu_adam.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/fp16_optimizer.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/adadelta.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/fused_adam.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/amp_optimizer.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/adam.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/composite.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/dynamic_loss_scaler.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/sgd.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/bmuf.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/shard.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/adagrad.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/adafactor.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/adamax.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/fairseq_optimizer.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/fused_lamb.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "copying fairseq/optim/nag.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/model_parallel\n",
            "copying fairseq/model_parallel/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/model_parallel\n",
            "copying fairseq/model_parallel/megatron_trainer.py -> build/lib.linux-x86_64-cpython-310/fairseq/model_parallel\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/scoring\n",
            "copying fairseq/scoring/bleu.py -> build/lib.linux-x86_64-cpython-310/fairseq/scoring\n",
            "copying fairseq/scoring/meteor.py -> build/lib.linux-x86_64-cpython-310/fairseq/scoring\n",
            "copying fairseq/scoring/wer.py -> build/lib.linux-x86_64-cpython-310/fairseq/scoring\n",
            "copying fairseq/scoring/tokenizer.py -> build/lib.linux-x86_64-cpython-310/fairseq/scoring\n",
            "copying fairseq/scoring/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/scoring\n",
            "copying fairseq/scoring/chrf.py -> build/lib.linux-x86_64-cpython-310/fairseq/scoring\n",
            "copying fairseq/scoring/bertscore.py -> build/lib.linux-x86_64-cpython-310/fairseq/scoring\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/lightconv_lm.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/fconv_self_att.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/fconv.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/fairseq_incremental_decoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/transformer_from_pretrained_xlm.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/lightconv.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/transformer_ulm.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/fairseq_decoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/multilingual_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/lstm_lm.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/lstm.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/masked_lm.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/transformer_align.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/fairseq_encoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/distributed_fairseq_model.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/composite_encoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/fconv_lm.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/transformer_lm.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/fairseq_model.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "copying fairseq/models/model_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/models\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/adaptive_softmax.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/unfold.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/kmeans_vector_quantizer.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/sparse_transformer_sentence_encoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/checkpoint_activations.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/positional_encoding.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/beamable_mm.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/same_pad.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/cross_entropy.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/location_attention.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/base_layer.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/fp32_batch_norm.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/character_token_embedder.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/layer_norm.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/lstm_cell_with_zoneout.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/conv_tbc.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/sparse_transformer_sentence_encoder_layer.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/transpose_last.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/transformer_layer_aug.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/espnet_multihead_attention.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/transformer_layer.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/transformer_sentence_encoder_layer.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/sinusoidal_positional_embedding.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/downsampled_multihead_attention.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/scalar_bias.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/vggblock.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/multihead_attention.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/learned_positional_embedding.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/ema_module.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/gelu.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/conformer_layer.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/dynamic_crf_layer.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/rotary_positional_embedding.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/sparse_multihead_attention.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/lightweight_convolution.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/transformer_sentence_encoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/adaptive_input.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/dynamic_convolution.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/fairseq_dropout.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/fp32_group_norm.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/fp32_instance_norm.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/quant_noise.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/kmeans_attention.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/positional_embedding.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/grad_multiply.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/linearized_convolution.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/gumbel_vector_quantizer.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "copying fairseq/modules/layer_drop.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/label_smoothed_cross_entropy_with_rdrop.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/ctc.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/sentence_prediction_adapters.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/model_criterion.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/fastspeech2_loss.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/legacy_masked_lm.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/cross_entropy.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/label_smoothed_cross_entropy.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/label_smoothed_cross_entropy_with_ctc.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/hubert_criterion.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/sentence_prediction.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/label_smoothed_cross_entropy_with_alignment.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/nat_loss.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/sentence_ranking.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/fairseq_criterion.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/tacotron2_loss.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/masked_lm.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/speech_dlm_criterion.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/wav2vec_criterion.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/adaptive_loss.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/label_smoothed_cross_entropy_latency_augmented.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/speech_ulm_criterion.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/composite_loss.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "copying fairseq/criterions/speech_to_speech_criterion.py -> build/lib.linux-x86_64-cpython-310/fairseq/criterions\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/lm_context_window_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/colorize_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/shorten_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/concat_sentences_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/multi_corpus_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/add_class_target_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/padding_mask_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/transform_eos_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/round_robin_zip_datasets.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/transform_eos_concat_langpair_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/bucket_pad_length_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/backtranslation_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/concat_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/data_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/prepend_token_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/iterators.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/mask_tokens_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/transform_eos_lang_pair_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/strip_token_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/text_compressor.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/resampling_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/indexed_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/roll_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/lru_cache_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/language_pair_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/raw_label_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/subsample_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/sort_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/append_token_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/nested_dictionary_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/add_target_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/monolingual_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/multi_corpus_sampled_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/fairseq_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/base_wrapper_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/dictionary.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/codedataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/denoising_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/numel_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/span_mask_tokens_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/speech_dlm_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/token_block_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/id_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/pad_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/offset_tokens_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/list_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/prepend_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/replace_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/plasma_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/num_samples_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/fasta_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "copying fairseq/data/noising.py -> build/lib.linux-x86_64-cpython-310/fairseq/data\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/noisychannel\n",
            "copying fairseq/examples/noisychannel/rerank_tune.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/noisychannel\n",
            "copying fairseq/examples/noisychannel/rerank.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/noisychannel\n",
            "copying fairseq/examples/noisychannel/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/noisychannel\n",
            "copying fairseq/examples/noisychannel/rerank_options.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/noisychannel\n",
            "copying fairseq/examples/noisychannel/rerank_generate.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/noisychannel\n",
            "copying fairseq/examples/noisychannel/rerank_score_lm.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/noisychannel\n",
            "copying fairseq/examples/noisychannel/rerank_score_bw.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/noisychannel\n",
            "copying fairseq/examples/noisychannel/rerank_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/noisychannel\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/truncated_bptt\n",
            "copying fairseq/examples/truncated_bptt/truncated_bptt_lm_task.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/truncated_bptt\n",
            "copying fairseq/examples/truncated_bptt/transformer_xl_model.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/truncated_bptt\n",
            "copying fairseq/examples/truncated_bptt/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/truncated_bptt\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis\n",
            "copying fairseq/examples/speech_synthesis/generate_waveform.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis\n",
            "copying fairseq/examples/speech_synthesis/data_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis\n",
            "copying fairseq/examples/speech_synthesis/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis\n",
            "copying fairseq/examples/speech_synthesis/utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/discriminative_reranking_nmt\n",
            "copying fairseq/examples/discriminative_reranking_nmt/drnmt_rerank.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/discriminative_reranking_nmt\n",
            "copying fairseq/examples/discriminative_reranking_nmt/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/discriminative_reranking_nmt\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/fast_noisy_channel\n",
            "copying fairseq/examples/fast_noisy_channel/noisy_channel_translation.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/fast_noisy_channel\n",
            "copying fairseq/examples/fast_noisy_channel/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/fast_noisy_channel\n",
            "copying fairseq/examples/fast_noisy_channel/noisy_channel_beam_search.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/fast_noisy_channel\n",
            "copying fairseq/examples/fast_noisy_channel/noisy_channel_sequence_generator.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/fast_noisy_channel\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec\n",
            "copying fairseq/examples/wav2vec/wav2vec_manifest.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec\n",
            "copying fairseq/examples/wav2vec/libri_labels.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec\n",
            "copying fairseq/examples/wav2vec/wav2vec_featurize.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec\n",
            "copying fairseq/examples/wav2vec/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec\n",
            "copying fairseq/examples/wav2vec/vq-wav2vec_featurize.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech\n",
            "copying fairseq/examples/speech_to_speech/generate_waveform_from_code.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech\n",
            "copying fairseq/examples/speech_to_speech/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec\n",
            "copying fairseq/examples/data2vec/fb_convert_beit_cp.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec\n",
            "copying fairseq/examples/data2vec/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/rxf\n",
            "copying fairseq/examples/rxf/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/rxf\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/adaptive_span\n",
            "copying fairseq/examples/adaptive_span/adaptive_span_attention.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/adaptive_span\n",
            "copying fairseq/examples/adaptive_span/truncated_bptt_lm_task.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/adaptive_span\n",
            "copying fairseq/examples/adaptive_span/adaptive_span_model.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/adaptive_span\n",
            "copying fairseq/examples/adaptive_span/adagrad_with_grad_clip.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/adaptive_span\n",
            "copying fairseq/examples/adaptive_span/adaptive_span_model_wrapper.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/adaptive_span\n",
            "copying fairseq/examples/adaptive_span/adaptive_span_loss.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/adaptive_span\n",
            "copying fairseq/examples/adaptive_span/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/adaptive_span\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text\n",
            "copying fairseq/examples/speech_text_joint_to_text/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition\n",
            "copying fairseq/examples/speech_recognition/infer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition\n",
            "copying fairseq/examples/speech_recognition/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition\n",
            "copying fairseq/examples/speech_recognition/w2l_decoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation\n",
            "copying fairseq/examples/simultaneous_translation/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/evaluation\n",
            "copying fairseq/examples/speech_synthesis/evaluation/get_eval_manifest.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/evaluation\n",
            "copying fairseq/examples/speech_synthesis/evaluation/eval_sp.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/evaluation\n",
            "copying fairseq/examples/speech_synthesis/evaluation/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/evaluation\n",
            "copying fairseq/examples/speech_synthesis/evaluation/eval_f0.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/evaluation\n",
            "copying fairseq/examples/speech_synthesis/evaluation/eval_asr.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/evaluation\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing\n",
            "copying fairseq/examples/speech_synthesis/preprocessing/get_ljspeech_audio_manifest.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing\n",
            "copying fairseq/examples/speech_synthesis/preprocessing/get_common_voice_audio_manifest.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing\n",
            "copying fairseq/examples/speech_synthesis/preprocessing/get_speaker_embedding.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing\n",
            "copying fairseq/examples/speech_synthesis/preprocessing/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing\n",
            "copying fairseq/examples/speech_synthesis/preprocessing/get_vctk_audio_manifest.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing\n",
            "copying fairseq/examples/speech_synthesis/preprocessing/denoise_and_vad_audio.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing\n",
            "copying fairseq/examples/speech_synthesis/preprocessing/get_feature_manifest.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing/vad\n",
            "copying fairseq/examples/speech_synthesis/preprocessing/vad/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing/vad\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing/speaker_embedder\n",
            "copying fairseq/examples/speech_synthesis/preprocessing/speaker_embedder/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing/speaker_embedder\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing/denoiser\n",
            "copying fairseq/examples/speech_synthesis/preprocessing/denoiser/pretrained.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing/denoiser\n",
            "copying fairseq/examples/speech_synthesis/preprocessing/denoiser/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing/denoiser\n",
            "copying fairseq/examples/speech_synthesis/preprocessing/denoiser/resample.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing/denoiser\n",
            "copying fairseq/examples/speech_synthesis/preprocessing/denoiser/utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing/denoiser\n",
            "copying fairseq/examples/speech_synthesis/preprocessing/denoiser/demucs.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/preprocessing/denoiser\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/discriminative_reranking_nmt/tasks\n",
            "copying fairseq/examples/discriminative_reranking_nmt/tasks/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/discriminative_reranking_nmt/tasks\n",
            "copying fairseq/examples/discriminative_reranking_nmt/tasks/discriminative_reranking_task.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/discriminative_reranking_nmt/tasks\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/discriminative_reranking_nmt/models\n",
            "copying fairseq/examples/discriminative_reranking_nmt/models/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/discriminative_reranking_nmt/models\n",
            "copying fairseq/examples/discriminative_reranking_nmt/models/discriminative_reranking_model.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/discriminative_reranking_nmt/models\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/discriminative_reranking_nmt/criterions\n",
            "copying fairseq/examples/discriminative_reranking_nmt/criterions/discriminative_reranking_criterion.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/discriminative_reranking_nmt/criterions\n",
            "copying fairseq/examples/discriminative_reranking_nmt/criterions/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/discriminative_reranking_nmt/criterions\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised\n",
            "copying fairseq/examples/wav2vec/unsupervised/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised\n",
            "copying fairseq/examples/wav2vec/unsupervised/w2vu_generate.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/tasks\n",
            "copying fairseq/examples/wav2vec/unsupervised/tasks/unpaired_audio_text.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/tasks\n",
            "copying fairseq/examples/wav2vec/unsupervised/tasks/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/tasks\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/models\n",
            "copying fairseq/examples/wav2vec/unsupervised/models/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/models\n",
            "copying fairseq/examples/wav2vec/unsupervised/models/wav2vec_u.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/models\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/data\n",
            "copying fairseq/examples/wav2vec/unsupervised/data/extracted_features_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/data\n",
            "copying fairseq/examples/wav2vec/unsupervised/data/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/data\n",
            "copying fairseq/examples/wav2vec/unsupervised/data/random_input_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/data\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/asr_bleu\n",
            "copying fairseq/examples/speech_to_speech/asr_bleu/compute_asr_bleu.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/asr_bleu\n",
            "copying fairseq/examples/speech_to_speech/asr_bleu/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/asr_bleu\n",
            "copying fairseq/examples/speech_to_speech/asr_bleu/utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/asr_bleu\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/preprocessing\n",
            "copying fairseq/examples/speech_to_speech/preprocessing/prep_s2ut_data.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/preprocessing\n",
            "copying fairseq/examples/speech_to_speech/preprocessing/prep_s2spect_data.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/preprocessing\n",
            "copying fairseq/examples/speech_to_speech/preprocessing/data_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/preprocessing\n",
            "copying fairseq/examples/speech_to_speech/preprocessing/prep_sn_output_data.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/preprocessing\n",
            "copying fairseq/examples/speech_to_speech/preprocessing/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/preprocessing\n",
            "copying fairseq/examples/speech_to_speech/preprocessing/prep_sn_data.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/preprocessing\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/unity\n",
            "copying fairseq/examples/speech_to_speech/unity/sequence_generator.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/unity\n",
            "copying fairseq/examples/speech_to_speech/unity/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/unity\n",
            "copying fairseq/examples/speech_to_speech/unity/sequence_generator_multi_decoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/unity\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/tasks\n",
            "copying fairseq/examples/data2vec/tasks/image_classification.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/tasks\n",
            "copying fairseq/examples/data2vec/tasks/image_pretraining.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/tasks\n",
            "copying fairseq/examples/data2vec/tasks/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/tasks\n",
            "copying fairseq/examples/data2vec/tasks/mae_image_classification.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/tasks\n",
            "copying fairseq/examples/data2vec/tasks/multimodal.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/tasks\n",
            "copying fairseq/examples/data2vec/tasks/mae_image_pretraining.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/tasks\n",
            "copying fairseq/examples/data2vec/tasks/audio_classification.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/tasks\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models\n",
            "copying fairseq/examples/data2vec/models/data2vec_vision.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models\n",
            "copying fairseq/examples/data2vec/models/mae.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models\n",
            "copying fairseq/examples/data2vec/models/data2vec_image_classification.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models\n",
            "copying fairseq/examples/data2vec/models/data2vec_audio.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models\n",
            "copying fairseq/examples/data2vec/models/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models\n",
            "copying fairseq/examples/data2vec/models/mae_image_classification.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models\n",
            "copying fairseq/examples/data2vec/models/data2vec_text_classification.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models\n",
            "copying fairseq/examples/data2vec/models/utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models\n",
            "copying fairseq/examples/data2vec/models/audio_classification.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models\n",
            "copying fairseq/examples/data2vec/models/data2vec2.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models\n",
            "copying fairseq/examples/data2vec/models/data2vec_text.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/data\n",
            "copying fairseq/examples/data2vec/data/add_class_target_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/data\n",
            "copying fairseq/examples/data2vec/data/path_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/data\n",
            "copying fairseq/examples/data2vec/data/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/data\n",
            "copying fairseq/examples/data2vec/data/image_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/data\n",
            "copying fairseq/examples/data2vec/data/mae_image_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/data\n",
            "copying fairseq/examples/data2vec/data/mae_finetuning_image_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/data\n",
            "copying fairseq/examples/data2vec/data/modality.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/data\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models/modalities\n",
            "copying fairseq/examples/data2vec/models/modalities/audio.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models/modalities\n",
            "copying fairseq/examples/data2vec/models/modalities/base.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models/modalities\n",
            "copying fairseq/examples/data2vec/models/modalities/modules.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models/modalities\n",
            "copying fairseq/examples/data2vec/models/modalities/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models/modalities\n",
            "copying fairseq/examples/data2vec/models/modalities/text.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models/modalities\n",
            "copying fairseq/examples/data2vec/models/modalities/images.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/models/modalities\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/rxf/rxf_src\n",
            "copying fairseq/examples/rxf/rxf_src/sentence_prediction_r3f.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/rxf/rxf_src\n",
            "copying fairseq/examples/rxf/rxf_src/label_smoothed_cross_entropy_r3f.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/rxf/rxf_src\n",
            "copying fairseq/examples/rxf/rxf_src/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/rxf/rxf_src\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/tasks\n",
            "copying fairseq/examples/speech_text_joint_to_text/tasks/pair_denoising.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/tasks\n",
            "copying fairseq/examples/speech_text_joint_to_text/tasks/speech_text_joint.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/tasks\n",
            "copying fairseq/examples/speech_text_joint_to_text/tasks/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/tasks\n",
            "copying fairseq/examples/speech_text_joint_to_text/tasks/speech_text_denoise_pretrain.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/tasks\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/models\n",
            "copying fairseq/examples/speech_text_joint_to_text/models/s2t_dualinputwavtransformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/models\n",
            "copying fairseq/examples/speech_text_joint_to_text/models/joint_speech_text_pretrain_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/models\n",
            "copying fairseq/examples/speech_text_joint_to_text/models/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/models\n",
            "copying fairseq/examples/speech_text_joint_to_text/models/s2t_dualinputxmtransformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/models\n",
            "copying fairseq/examples/speech_text_joint_to_text/models/s2t_dualinputtransformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/models\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/criterions\n",
            "copying fairseq/examples/speech_text_joint_to_text/criterions/multi_modality_cross_entropy.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/criterions\n",
            "copying fairseq/examples/speech_text_joint_to_text/criterions/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/criterions\n",
            "copying fairseq/examples/speech_text_joint_to_text/criterions/text_guide_cross_entropy_acc.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/criterions\n",
            "copying fairseq/examples/speech_text_joint_to_text/criterions/multi_modality_compound.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/criterions\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/kaldi\n",
            "copying fairseq/examples/speech_recognition/kaldi/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/kaldi\n",
            "copying fairseq/examples/speech_recognition/kaldi/kaldi_decoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/kaldi\n",
            "copying fairseq/examples/speech_recognition/kaldi/kaldi_initializer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/kaldi\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/tasks\n",
            "copying fairseq/examples/speech_recognition/tasks/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/tasks\n",
            "copying fairseq/examples/speech_recognition/tasks/speech_recognition.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/tasks\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/models\n",
            "copying fairseq/examples/speech_recognition/models/w2l_conv_glu_enc.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/models\n",
            "copying fairseq/examples/speech_recognition/models/vggtransformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/models\n",
            "copying fairseq/examples/speech_recognition/models/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/models\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/criterions\n",
            "copying fairseq/examples/speech_recognition/criterions/cross_entropy_acc.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/criterions\n",
            "copying fairseq/examples/speech_recognition/criterions/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/criterions\n",
            "copying fairseq/examples/speech_recognition/criterions/ASG_loss.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/criterions\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new\n",
            "copying fairseq/examples/speech_recognition/new/infer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new\n",
            "copying fairseq/examples/speech_recognition/new/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/data\n",
            "copying fairseq/examples/speech_recognition/data/asr_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/data\n",
            "copying fairseq/examples/speech_recognition/data/data_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/data\n",
            "copying fairseq/examples/speech_recognition/data/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/data\n",
            "copying fairseq/examples/speech_recognition/data/replabels.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/data\n",
            "copying fairseq/examples/speech_recognition/data/collaters.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/data\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new/decoders\n",
            "copying fairseq/examples/speech_recognition/new/decoders/decoder_config.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new/decoders\n",
            "copying fairseq/examples/speech_recognition/new/decoders/base_decoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new/decoders\n",
            "copying fairseq/examples/speech_recognition/new/decoders/decoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new/decoders\n",
            "copying fairseq/examples/speech_recognition/new/decoders/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new/decoders\n",
            "copying fairseq/examples/speech_recognition/new/decoders/viterbi_decoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new/decoders\n",
            "copying fairseq/examples/speech_recognition/new/decoders/flashlight_decoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new/decoders\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/utils\n",
            "copying fairseq/examples/simultaneous_translation/utils/functions.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/utils\n",
            "copying fairseq/examples/simultaneous_translation/utils/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/utils\n",
            "copying fairseq/examples/simultaneous_translation/utils/monotonic_attention.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/utils\n",
            "copying fairseq/examples/simultaneous_translation/utils/p_choose_strategy.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/utils\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/models\n",
            "copying fairseq/examples/simultaneous_translation/models/transformer_monotonic_attention.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/models\n",
            "copying fairseq/examples/simultaneous_translation/models/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/models\n",
            "copying fairseq/examples/simultaneous_translation/models/convtransformer_simul_trans.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/models\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/modules\n",
            "copying fairseq/examples/simultaneous_translation/modules/fixed_pre_decision.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/modules\n",
            "copying fairseq/examples/simultaneous_translation/modules/monotonic_transformer_layer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/modules\n",
            "copying fairseq/examples/simultaneous_translation/modules/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/modules\n",
            "copying fairseq/examples/simultaneous_translation/modules/monotonic_multihead_attention.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/modules\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/fairseq_lr_scheduler.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/tri_stage_lr_scheduler.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/triangular_lr_scheduler.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/manual_lr_scheduler.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/polynomial_decay_schedule.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/reduce_lr_on_plateau.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/fixed_schedule.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/step_lr_scheduler.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/cosine_lr_scheduler.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/inverse_square_root_schedule.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim/lr_scheduler\n",
            "copying fairseq/optim/lr_scheduler/pass_through.py -> build/lib.linux-x86_64-cpython-310/fairseq/optim/lr_scheduler\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/models\n",
            "copying fairseq/model_parallel/models/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/models\n",
            "copying fairseq/model_parallel/models/transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/models\n",
            "copying fairseq/model_parallel/models/transformer_lm.py -> build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/models\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/modules\n",
            "copying fairseq/model_parallel/modules/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/modules\n",
            "copying fairseq/model_parallel/modules/transformer_layer.py -> build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/modules\n",
            "copying fairseq/model_parallel/modules/multihead_attention.py -> build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/modules\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/criterions\n",
            "copying fairseq/model_parallel/criterions/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/criterions\n",
            "copying fairseq/model_parallel/criterions/vocab_parallel_cross_entropy.py -> build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/criterions\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/models/pipeline_parallel_transformer\n",
            "copying fairseq/model_parallel/models/pipeline_parallel_transformer/model.py -> build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/models/pipeline_parallel_transformer\n",
            "copying fairseq/model_parallel/models/pipeline_parallel_transformer/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/models/pipeline_parallel_transformer\n",
            "copying fairseq/model_parallel/models/pipeline_parallel_transformer/layers.py -> build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/models/pipeline_parallel_transformer\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/models/roberta\n",
            "copying fairseq/model_parallel/models/roberta/model.py -> build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/models/roberta\n",
            "copying fairseq/model_parallel/models/roberta/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/model_parallel/models/roberta\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/nat\n",
            "copying fairseq/models/nat/nonautoregressive_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/nat\n",
            "copying fairseq/models/nat/levenshtein_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/nat\n",
            "copying fairseq/models/nat/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/nat\n",
            "copying fairseq/models/nat/insertion_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/nat\n",
            "copying fairseq/models/nat/iterative_nonautoregressive_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/nat\n",
            "copying fairseq/models/nat/cmlm_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/nat\n",
            "copying fairseq/models/nat/fairseq_nat_model.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/nat\n",
            "copying fairseq/models/nat/nonautoregressive_ensembles.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/nat\n",
            "copying fairseq/models/nat/levenshtein_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/nat\n",
            "copying fairseq/models/nat/nat_crf_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/nat\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/hubert\n",
            "copying fairseq/models/hubert/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/hubert\n",
            "copying fairseq/models/hubert/hubert.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/hubert\n",
            "copying fairseq/models/hubert/hubert_asr.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/hubert\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/speech_dlm\n",
            "copying fairseq/models/speech_dlm/speech_dlm.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_dlm\n",
            "copying fairseq/models/speech_dlm/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_dlm\n",
            "copying fairseq/models/speech_dlm/hub_interface.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_dlm\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/transformer\n",
            "copying fairseq/models/transformer/transformer_base.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/transformer\n",
            "copying fairseq/models/transformer/transformer_config.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/transformer\n",
            "copying fairseq/models/transformer/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/transformer\n",
            "copying fairseq/models/transformer/transformer_decoder_aug.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/transformer\n",
            "copying fairseq/models/transformer/transformer_encoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/transformer\n",
            "copying fairseq/models/transformer/transformer_decoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/transformer\n",
            "copying fairseq/models/transformer/transformer_legacy.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/transformer\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/wav2vec\n",
            "copying fairseq/models/wav2vec/wav2vec2_asr.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/wav2vec\n",
            "copying fairseq/models/wav2vec/wav2vec2_classification.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/wav2vec\n",
            "copying fairseq/models/wav2vec/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/wav2vec\n",
            "copying fairseq/models/wav2vec/wav2vec2_laser.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/wav2vec\n",
            "copying fairseq/models/wav2vec/wav2vec.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/wav2vec\n",
            "copying fairseq/models/wav2vec/wav2vec2.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/wav2vec\n",
            "copying fairseq/models/wav2vec/utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/wav2vec\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/multires_hubert\n",
            "copying fairseq/models/multires_hubert/multires_hubert_asr.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/multires_hubert\n",
            "copying fairseq/models/multires_hubert/multires_hubert.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/multires_hubert\n",
            "copying fairseq/models/multires_hubert/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/multires_hubert\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_speech\n",
            "copying fairseq/models/speech_to_speech/s2s_conformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_speech\n",
            "copying fairseq/models/speech_to_speech/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_speech\n",
            "copying fairseq/models/speech_to_speech/s2s_conformer_translatotron2.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_speech\n",
            "copying fairseq/models/speech_to_speech/s2s_conformer_unity.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_speech\n",
            "copying fairseq/models/speech_to_speech/s2s_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_speech\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/huggingface\n",
            "copying fairseq/models/huggingface/hf_gpt2.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/huggingface\n",
            "copying fairseq/models/huggingface/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/huggingface\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/ema\n",
            "copying fairseq/models/ema/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/ema\n",
            "copying fairseq/models/ema/ema.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/ema\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/roberta\n",
            "copying fairseq/models/roberta/alignment_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/roberta\n",
            "copying fairseq/models/roberta/model_gottbert.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/roberta\n",
            "copying fairseq/models/roberta/model.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/roberta\n",
            "copying fairseq/models/roberta/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/roberta\n",
            "copying fairseq/models/roberta/model_camembert.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/roberta\n",
            "copying fairseq/models/roberta/hub_interface.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/roberta\n",
            "copying fairseq/models/roberta/model_xlmr.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/roberta\n",
            "copying fairseq/models/roberta/enc_dec.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/roberta\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/bart\n",
            "copying fairseq/models/bart/model.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/bart\n",
            "copying fairseq/models/bart/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/bart\n",
            "copying fairseq/models/bart/hub_interface.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/bart\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/xmod\n",
            "copying fairseq/models/xmod/model.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/xmod\n",
            "copying fairseq/models/xmod/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/xmod\n",
            "copying fairseq/models/xmod/hub_interface.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/xmod\n",
            "copying fairseq/models/xmod/transformer_layer_xmod.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/xmod\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/text_to_speech\n",
            "copying fairseq/models/text_to_speech/hifigan.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/text_to_speech\n",
            "copying fairseq/models/text_to_speech/codehifigan.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/text_to_speech\n",
            "copying fairseq/models/text_to_speech/fastspeech2.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/text_to_speech\n",
            "copying fairseq/models/text_to_speech/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/text_to_speech\n",
            "copying fairseq/models/text_to_speech/hub_interface.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/text_to_speech\n",
            "copying fairseq/models/text_to_speech/tts_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/text_to_speech\n",
            "copying fairseq/models/text_to_speech/vocoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/text_to_speech\n",
            "copying fairseq/models/text_to_speech/tacotron2.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/text_to_speech\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_text\n",
            "copying fairseq/models/speech_to_text/xm_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_text\n",
            "copying fairseq/models/speech_to_text/xm_transformer_unity.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_text\n",
            "copying fairseq/models/speech_to_text/convtransformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_text\n",
            "copying fairseq/models/speech_to_text/multi_modality_model.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_text\n",
            "copying fairseq/models/speech_to_text/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_text\n",
            "copying fairseq/models/speech_to_text/s2t_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_text\n",
            "copying fairseq/models/speech_to_text/s2t_conformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_text\n",
            "copying fairseq/models/speech_to_text/s2t_wav_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_text\n",
            "copying fairseq/models/speech_to_text/hub_interface.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_text\n",
            "copying fairseq/models/speech_to_text/utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_text\n",
            "copying fairseq/models/speech_to_text/berard.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_text\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/speech_dlm/sequence_generator\n",
            "copying fairseq/models/speech_dlm/sequence_generator/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_dlm/sequence_generator\n",
            "copying fairseq/models/speech_dlm/sequence_generator/multichannel_search.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_dlm/sequence_generator\n",
            "copying fairseq/models/speech_dlm/sequence_generator/multichannel_sequence_generator.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_dlm/sequence_generator\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/speech_dlm/modules\n",
            "copying fairseq/models/speech_dlm/modules/speech_dlm_decoder_layer.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_dlm/modules\n",
            "copying fairseq/models/speech_dlm/modules/speech_dlm_decoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_dlm/modules\n",
            "copying fairseq/models/speech_dlm/modules/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_dlm/modules\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_speech/modules\n",
            "copying fairseq/models/speech_to_speech/modules/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_speech/modules\n",
            "copying fairseq/models/speech_to_speech/modules/transformer_decoder_aug.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_speech/modules\n",
            "copying fairseq/models/speech_to_speech/modules/ctc_decoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_speech/modules\n",
            "copying fairseq/models/speech_to_speech/modules/transformer_encoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_speech/modules\n",
            "copying fairseq/models/speech_to_speech/modules/stacked_embedding.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_speech/modules\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_text/modules\n",
            "copying fairseq/models/speech_to_text/modules/augmented_memory_attention.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_text/modules\n",
            "copying fairseq/models/speech_to_text/modules/convolution.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_text/modules\n",
            "copying fairseq/models/speech_to_text/modules/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_text/modules\n",
            "copying fairseq/models/speech_to_text/modules/emformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/models/speech_to_text/modules\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization\n",
            "copying fairseq/modules/quantization/quantization_options.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization\n",
            "copying fairseq/modules/quantization/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/modules/lightconv_layer\n",
            "copying fairseq/modules/lightconv_layer/cuda_function_gen.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/lightconv_layer\n",
            "copying fairseq/modules/lightconv_layer/setup.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/lightconv_layer\n",
            "copying fairseq/modules/lightconv_layer/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/lightconv_layer\n",
            "copying fairseq/modules/lightconv_layer/lightconv_layer.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/lightconv_layer\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/modules/dynamicconv_layer\n",
            "copying fairseq/modules/dynamicconv_layer/cuda_function_gen.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/dynamicconv_layer\n",
            "copying fairseq/modules/dynamicconv_layer/setup.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/dynamicconv_layer\n",
            "copying fairseq/modules/dynamicconv_layer/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/dynamicconv_layer\n",
            "copying fairseq/modules/dynamicconv_layer/dynamicconv_layer.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/dynamicconv_layer\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/pq\n",
            "copying fairseq/modules/quantization/pq/pq.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/pq\n",
            "copying fairseq/modules/quantization/pq/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/pq\n",
            "copying fairseq/modules/quantization/pq/utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/pq\n",
            "copying fairseq/modules/quantization/pq/em.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/pq\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/scalar\n",
            "copying fairseq/modules/quantization/scalar/ops.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/scalar\n",
            "copying fairseq/modules/quantization/scalar/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/scalar\n",
            "copying fairseq/modules/quantization/scalar/utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/scalar\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/pq/modules\n",
            "copying fairseq/modules/quantization/pq/modules/qemb.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/pq/modules\n",
            "copying fairseq/modules/quantization/pq/modules/qlinear.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/pq/modules\n",
            "copying fairseq/modules/quantization/pq/modules/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/pq/modules\n",
            "copying fairseq/modules/quantization/pq/modules/qconv.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/pq/modules\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/scalar/modules\n",
            "copying fairseq/modules/quantization/scalar/modules/qemb.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/scalar/modules\n",
            "copying fairseq/modules/quantization/scalar/modules/qact.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/scalar/modules\n",
            "copying fairseq/modules/quantization/scalar/modules/qlinear.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/scalar/modules\n",
            "copying fairseq/modules/quantization/scalar/modules/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/scalar/modules\n",
            "copying fairseq/modules/quantization/scalar/modules/qconv.py -> build/lib.linux-x86_64-cpython-310/fairseq/modules/quantization/scalar/modules\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/data/legacy\n",
            "copying fairseq/data/legacy/masked_lm_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/legacy\n",
            "copying fairseq/data/legacy/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/legacy\n",
            "copying fairseq/data/legacy/block_pair_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/legacy\n",
            "copying fairseq/data/legacy/masked_lm_dictionary.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/legacy\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/data/audio\n",
            "copying fairseq/data/audio/hubert_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio\n",
            "copying fairseq/data/audio/audio_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio\n",
            "copying fairseq/data/audio/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio\n",
            "copying fairseq/data/audio/frm_text_to_speech_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio\n",
            "copying fairseq/data/audio/speech_to_text_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio\n",
            "copying fairseq/data/audio/raw_audio_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio\n",
            "copying fairseq/data/audio/speech_to_text_joint_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio\n",
            "copying fairseq/data/audio/data_cfg.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio\n",
            "copying fairseq/data/audio/text_to_speech_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio\n",
            "copying fairseq/data/audio/speech_to_speech_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio\n",
            "copying fairseq/data/audio/multi_modality_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/data/multilingual\n",
            "copying fairseq/data/multilingual/sampling_method.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/multilingual\n",
            "copying fairseq/data/multilingual/sampled_multi_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/multilingual\n",
            "copying fairseq/data/multilingual/sampled_multi_epoch_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/multilingual\n",
            "copying fairseq/data/multilingual/multilingual_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/multilingual\n",
            "copying fairseq/data/multilingual/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/multilingual\n",
            "copying fairseq/data/multilingual/multilingual_data_manager.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/multilingual\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/data/encoders\n",
            "copying fairseq/data/encoders/fastbpe.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/encoders\n",
            "copying fairseq/data/encoders/gpt2_bpe.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/encoders\n",
            "copying fairseq/data/encoders/space_tokenizer.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/encoders\n",
            "copying fairseq/data/encoders/byte_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/encoders\n",
            "copying fairseq/data/encoders/byte_bpe.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/encoders\n",
            "copying fairseq/data/encoders/bytes.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/encoders\n",
            "copying fairseq/data/encoders/moses_tokenizer.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/encoders\n",
            "copying fairseq/data/encoders/subword_nmt_bpe.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/encoders\n",
            "copying fairseq/data/encoders/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/encoders\n",
            "copying fairseq/data/encoders/hf_bert_bpe.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/encoders\n",
            "copying fairseq/data/encoders/utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/encoders\n",
            "copying fairseq/data/encoders/hf_byte_bpe.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/encoders\n",
            "copying fairseq/data/encoders/gpt2_bpe_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/encoders\n",
            "copying fairseq/data/encoders/nltk_tokenizer.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/encoders\n",
            "copying fairseq/data/encoders/sentencepiece_bpe.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/encoders\n",
            "copying fairseq/data/encoders/characters.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/encoders\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/data/huffman\n",
            "copying fairseq/data/huffman/huffman_coder.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/huffman\n",
            "copying fairseq/data/huffman/huffman_mmap_indexed_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/huffman\n",
            "copying fairseq/data/huffman/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/huffman\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/data/audio/dataset_transforms\n",
            "copying fairseq/data/audio/dataset_transforms/concataugment.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio/dataset_transforms\n",
            "copying fairseq/data/audio/dataset_transforms/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio/dataset_transforms\n",
            "copying fairseq/data/audio/dataset_transforms/noisyoverlapaugment.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio/dataset_transforms\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/data/audio/feature_transforms\n",
            "copying fairseq/data/audio/feature_transforms/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio/feature_transforms\n",
            "copying fairseq/data/audio/feature_transforms/global_cmvn.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio/feature_transforms\n",
            "copying fairseq/data/audio/feature_transforms/delta_deltas.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio/feature_transforms\n",
            "copying fairseq/data/audio/feature_transforms/utterance_cmvn.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio/feature_transforms\n",
            "copying fairseq/data/audio/feature_transforms/specaugment.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio/feature_transforms\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/data/audio/waveform_transforms\n",
            "copying fairseq/data/audio/waveform_transforms/noiseaugment.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio/waveform_transforms\n",
            "copying fairseq/data/audio/waveform_transforms/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/data/audio/waveform_transforms\n",
            "copying fairseq/examples/.gitignore -> build/lib.linux-x86_64-cpython-310/fairseq/examples\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/latent_depth\n",
            "copying fairseq/examples/latent_depth/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/latent_depth\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/latent_depth/latent_depth_src\n",
            "copying fairseq/examples/latent_depth/latent_depth_src/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/latent_depth/latent_depth_src\n",
            "copying fairseq/examples/latent_depth/latent_depth_src/multilingual_translation_latent_depth.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/latent_depth/latent_depth_src\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/latent_depth/latent_depth_src/loss\n",
            "copying fairseq/examples/latent_depth/latent_depth_src/loss/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/latent_depth/latent_depth_src/loss\n",
            "copying fairseq/examples/latent_depth/latent_depth_src/loss/latent_depth.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/latent_depth/latent_depth_src/loss\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/latent_depth/latent_depth_src/models\n",
            "copying fairseq/examples/latent_depth/latent_depth_src/models/latent_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/latent_depth/latent_depth_src/models\n",
            "copying fairseq/examples/latent_depth/latent_depth_src/models/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/latent_depth/latent_depth_src/models\n",
            "copying fairseq/examples/latent_depth/latent_depth_src/models/latent_multilingual_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/latent_depth/latent_depth_src/models\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/latent_depth/latent_depth_src/modules\n",
            "copying fairseq/examples/latent_depth/latent_depth_src/modules/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/latent_depth/latent_depth_src/modules\n",
            "copying fairseq/examples/latent_depth/latent_depth_src/modules/latent_layers.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/latent_depth/latent_depth_src/modules\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/scaling_nmt\n",
            "copying fairseq/examples/scaling_nmt/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/scaling_nmt\n",
            "copying fairseq/examples/noisychannel/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/noisychannel\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert\n",
            "copying fairseq/examples/hubert/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert\n",
            "copying fairseq/examples/hubert/measure_teacher_quality.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert\n",
            "copying fairseq/examples/hubert/update_ckpt.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/finetune\n",
            "copying fairseq/examples/hubert/config/finetune/base_10h.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/finetune\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/finetune/lm\n",
            "copying fairseq/examples/hubert/config/finetune/lm/ls_4gram.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/finetune/lm\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/finetune/run\n",
            "copying fairseq/examples/hubert/config/finetune/run/submitit_reg.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/finetune/run\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/finetune/ckpt\n",
            "copying fairseq/examples/hubert/config/finetune/ckpt/it1.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/finetune/ckpt\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/pretrain\n",
            "copying fairseq/examples/hubert/config/pretrain/hubert_base_librispeech.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/pretrain\n",
            "copying fairseq/examples/hubert/config/pretrain/hubert_large_librivox.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/pretrain\n",
            "copying fairseq/examples/hubert/config/pretrain/hubert_xlarge_librivox.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/pretrain\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/pretrain/run\n",
            "copying fairseq/examples/hubert/config/pretrain/run/submitit_reg.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/pretrain/run\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/pretrain/data\n",
            "copying fairseq/examples/hubert/config/pretrain/data/iter1.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/pretrain/data\n",
            "copying fairseq/examples/hubert/config/pretrain/data/iter2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/pretrain/data\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/decode\n",
            "copying fairseq/examples/hubert/config/decode/infer_viterbi.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/decode\n",
            "copying fairseq/examples/hubert/config/decode/infer_kenlm.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/decode\n",
            "copying fairseq/examples/hubert/config/decode/infer_fsqlm.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/decode\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/decode/ax_sweep\n",
            "copying fairseq/examples/hubert/config/decode/ax_sweep/transformer.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/decode/ax_sweep\n",
            "copying fairseq/examples/hubert/config/decode/ax_sweep/ngram.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/decode/ax_sweep\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/decode/run\n",
            "copying fairseq/examples/hubert/config/decode/run/submitit_slurm.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/decode/run\n",
            "copying fairseq/examples/hubert/config/decode/run/submitit_slurm_8gpu.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/config/decode/run\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/simple_kmeans\n",
            "copying fairseq/examples/hubert/simple_kmeans/dump_mfcc_feature.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/simple_kmeans\n",
            "copying fairseq/examples/hubert/simple_kmeans/dump_hubert_feature_s2t.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/simple_kmeans\n",
            "copying fairseq/examples/hubert/simple_kmeans/learn_kmeans.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/simple_kmeans\n",
            "copying fairseq/examples/hubert/simple_kmeans/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/simple_kmeans\n",
            "copying fairseq/examples/hubert/simple_kmeans/feature_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/simple_kmeans\n",
            "copying fairseq/examples/hubert/simple_kmeans/dump_hubert_feature.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/simple_kmeans\n",
            "copying fairseq/examples/hubert/simple_kmeans/dump_km_label.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/simple_kmeans\n",
            "copying fairseq/examples/hubert/simple_kmeans/dump_w2v2_feature.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/simple_kmeans\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/tests\n",
            "copying fairseq/examples/hubert/tests/sample.xlarge.hypo.word -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/tests\n",
            "copying fairseq/examples/hubert/tests/test_feature_and_unit.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/tests\n",
            "copying fairseq/examples/hubert/tests/sample.base.L9.npy -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/tests\n",
            "copying fairseq/examples/hubert/tests/sample.base.L9.km500.km -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/tests\n",
            "copying fairseq/examples/hubert/tests/sample.large.hypo.word -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/tests\n",
            "copying fairseq/examples/hubert/tests/6313-76958-0021.flac -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/tests\n",
            "copying fairseq/examples/hubert/tests/test_finetuned_asr.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/tests\n",
            "copying fairseq/examples/hubert/tests/sample.xlarge.L30.npy -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/tests\n",
            "copying fairseq/examples/hubert/tests/sample.base.L9.len -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/tests\n",
            "copying fairseq/examples/hubert/tests/sample.large.L20.npy -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/tests\n",
            "copying fairseq/examples/hubert/tests/sample.large.L20.len -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/tests\n",
            "copying fairseq/examples/hubert/tests/sample.xlarge.L30.len -> build/lib.linux-x86_64-cpython-310/fairseq/examples/hubert/tests\n",
            "copying fairseq/examples/truncated_bptt/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/truncated_bptt\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/pay_less_attention_paper\n",
            "copying fairseq/examples/pay_less_attention_paper/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/pay_less_attention_paper\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wmt19\n",
            "copying fairseq/examples/wmt19/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wmt19\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/quant_noise\n",
            "copying fairseq/examples/quant_noise/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/quant_noise\n",
            "copying fairseq/examples/quant_noise/transformer_quantization_config.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/quant_noise\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mms\n",
            "copying fairseq/examples/mms/MODEL_CARD.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mms\n",
            "copying fairseq/examples/mms/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mms\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/tts\n",
            "copying fairseq/examples/mms/tts/infer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/tts\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/tts/tutorial\n",
            "copying fairseq/examples/mms/tts/tutorial/MMS_TTS_Inference_Colab.ipynb -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/tts/tutorial\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/data_prep\n",
            "copying fairseq/examples/mms/data_prep/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/data_prep\n",
            "copying fairseq/examples/mms/data_prep/norm_config.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/data_prep\n",
            "copying fairseq/examples/mms/data_prep/punctuations.lst -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/data_prep\n",
            "copying fairseq/examples/mms/data_prep/text_normalization.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/data_prep\n",
            "copying fairseq/examples/mms/data_prep/align_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/data_prep\n",
            "copying fairseq/examples/mms/data_prep/align_and_segment.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/data_prep\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/misc\n",
            "copying fairseq/examples/mms/misc/get_sample_size.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/misc\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/lid\n",
            "copying fairseq/examples/mms/lid/infer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/lid\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/lid/tutorial\n",
            "copying fairseq/examples/mms/lid/tutorial/MMS_LID_Inference_Colab.ipynb -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/lid/tutorial\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/asr\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/asr/config\n",
            "copying fairseq/examples/mms/asr/config/infer_common.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/asr/config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/asr/infer\n",
            "copying fairseq/examples/mms/asr/infer/mms_infer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/asr/infer\n",
            "copying fairseq/examples/mms/asr/infer/example_infer_adapter.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/asr/infer\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/asr/tutorial\n",
            "copying fairseq/examples/mms/asr/tutorial/MMS_ASR_Inference_Colab.ipynb -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mms/asr/tutorial\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/nonautoregressive_translation\n",
            "copying fairseq/examples/nonautoregressive_translation/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/nonautoregressive_translation\n",
            "copying fairseq/examples/nonautoregressive_translation/scripts.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/nonautoregressive_translation\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/fully_sharded_data_parallel\n",
            "copying fairseq/examples/fully_sharded_data_parallel/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/fully_sharded_data_parallel\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/megatron_11b\n",
            "copying fairseq/examples/megatron_11b/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/megatron_11b\n",
            "copying fairseq/examples/megatron_11b/detok.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/megatron_11b\n",
            "copying fairseq/examples/speech_synthesis/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/docs\n",
            "copying fairseq/examples/speech_synthesis/docs/ljspeech_example.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/docs\n",
            "copying fairseq/examples/speech_synthesis/docs/vctk_example.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/docs\n",
            "copying fairseq/examples/speech_synthesis/docs/common_voice_example.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_synthesis/docs\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/backtranslation\n",
            "copying fairseq/examples/backtranslation/prepare-de-monolingual.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/backtranslation\n",
            "copying fairseq/examples/backtranslation/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/backtranslation\n",
            "copying fairseq/examples/backtranslation/tokenized_bleu.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/backtranslation\n",
            "copying fairseq/examples/backtranslation/extract_bt_data.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/backtranslation\n",
            "copying fairseq/examples/backtranslation/sacrebleu.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/backtranslation\n",
            "copying fairseq/examples/backtranslation/prepare-wmt18en2de.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/backtranslation\n",
            "copying fairseq/examples/backtranslation/deduplicate_lines.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/backtranslation\n",
            "copying fairseq/examples/discriminative_reranking_nmt/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/discriminative_reranking_nmt\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/discriminative_reranking_nmt/config\n",
            "copying fairseq/examples/discriminative_reranking_nmt/config/deen.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/discriminative_reranking_nmt/config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/discriminative_reranking_nmt/scripts\n",
            "copying fairseq/examples/discriminative_reranking_nmt/scripts/prep_data.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/discriminative_reranking_nmt/scripts\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/camembert\n",
            "copying fairseq/examples/camembert/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/camembert\n",
            "copying fairseq/examples/fast_noisy_channel/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/fast_noisy_channel\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100\n",
            "copying fairseq/examples/m2m_100/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100\n",
            "copying fairseq/examples/m2m_100/install_dependecies.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100\n",
            "copying fairseq/examples/m2m_100/tok.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100/tokenizers\n",
            "copying fairseq/examples/m2m_100/tokenizers/tokenize_indic.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100/tokenizers\n",
            "copying fairseq/examples/m2m_100/tokenizers/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100/tokenizers\n",
            "copying fairseq/examples/m2m_100/tokenizers/tokenizer_ar.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100/tokenizers\n",
            "copying fairseq/examples/m2m_100/tokenizers/seg_ko.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100/tokenizers\n",
            "copying fairseq/examples/m2m_100/tokenizers/seg_ja.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100/tokenizers\n",
            "copying fairseq/examples/m2m_100/tokenizers/tokenize_thai.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100/tokenizers\n",
            "copying fairseq/examples/m2m_100/tokenizers/tokenize_zh.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100/tokenizers\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100/tokenizers/thirdparty\n",
            "copying fairseq/examples/m2m_100/tokenizers/thirdparty/.gitignore -> build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100/tokenizers/thirdparty\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100/process_data\n",
            "copying fairseq/examples/m2m_100/process_data/dedup_data.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100/process_data\n",
            "copying fairseq/examples/m2m_100/process_data/clean_histogram.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100/process_data\n",
            "copying fairseq/examples/m2m_100/process_data/remove_too_much_punc.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/m2m_100/process_data\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/layerdrop\n",
            "copying fairseq/examples/layerdrop/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/layerdrop\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/laser\n",
            "copying fairseq/examples/laser/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/laser\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/laser/laser_src\n",
            "copying fairseq/examples/laser/laser_src/laser_lstm.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/laser/laser_src\n",
            "copying fairseq/examples/laser/laser_src/multitask_data_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/laser/laser_src\n",
            "copying fairseq/examples/laser/laser_src/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/laser/laser_src\n",
            "copying fairseq/examples/laser/laser_src/laser_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/laser/laser_src\n",
            "copying fairseq/examples/laser/laser_src/laser_task.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/laser/laser_src\n",
            "copying fairseq/examples/wav2vec/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/pretraining\n",
            "copying fairseq/examples/wav2vec/config/pretraining/wav2vec2_conformer_base_librispeech.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/pretraining\n",
            "copying fairseq/examples/wav2vec/config/pretraining/wav2vec2_large_librivox_tpu.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/pretraining\n",
            "copying fairseq/examples/wav2vec/config/pretraining/wav2vec2_large_librivox.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/pretraining\n",
            "copying fairseq/examples/wav2vec/config/pretraining/wav2vec2_large_librivox_tpu-pod.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/pretraining\n",
            "copying fairseq/examples/wav2vec/config/pretraining/wav2vec2_base_librispeech.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/pretraining\n",
            "copying fairseq/examples/wav2vec/config/pretraining/wav2vec2_conformer_large_librivox.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/pretraining\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_1h_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_1h_4.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_10h_2_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_10h_2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_10m_3.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/base_100h.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_1h.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_960h_3.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_100h_3.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_1h_3.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_10h_aws_v100.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_10m_2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_1h_2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_960h_2_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_960h.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_10m_2_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_10h_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_100h_2_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_100h.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_100h_2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_960h_2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_10h.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_10m.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/base_960h.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/base_10h.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/base_1h.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/base_10m.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "copying fairseq/examples/wav2vec/config/finetuning/vox_1h_2_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning/run_config\n",
            "copying fairseq/examples/wav2vec/config/finetuning/run_config/slurm_2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning/run_config\n",
            "copying fairseq/examples/wav2vec/config/finetuning/run_config/slurm_1_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning/run_config\n",
            "copying fairseq/examples/wav2vec/config/finetuning/run_config/slurm_2_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning/run_config\n",
            "copying fairseq/examples/wav2vec/config/finetuning/run_config/slurm_4g.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning/run_config\n",
            "copying fairseq/examples/wav2vec/config/finetuning/run_config/slurm_16.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning/run_config\n",
            "copying fairseq/examples/wav2vec/config/finetuning/run_config/slurm_3.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning/run_config\n",
            "copying fairseq/examples/wav2vec/config/finetuning/run_config/slurm_8.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning/run_config\n",
            "copying fairseq/examples/wav2vec/config/finetuning/run_config/slurm_4g_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning/run_config\n",
            "copying fairseq/examples/wav2vec/config/finetuning/run_config/slurm_1_old.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning/run_config\n",
            "copying fairseq/examples/wav2vec/config/finetuning/run_config/slurm_1.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning/run_config\n",
            "copying fairseq/examples/wav2vec/config/finetuning/run_config/slurm_2g.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/config/finetuning/run_config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/scripts\n",
            "copying fairseq/examples/wav2vec/scripts/binarize_manifest.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/scripts\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/xlsr\n",
            "copying fairseq/examples/wav2vec/xlsr/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/xlsr\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/xlsr/config\n",
            "copying fairseq/examples/wav2vec/xlsr/config/finetune.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/xlsr/config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/xlsr/scripts\n",
            "copying fairseq/examples/wav2vec/xlsr/scripts/eval_speaker_clf_task.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/xlsr/scripts\n",
            "copying fairseq/examples/wav2vec/xlsr/scripts/gen_audio_embedding.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/xlsr/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config/timit_matched\n",
            "copying fairseq/examples/wav2vec/unsupervised/config/timit_matched/test.uid -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config/timit_matched\n",
            "copying fairseq/examples/wav2vec/unsupervised/config/timit_matched/train.uid -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config/timit_matched\n",
            "copying fairseq/examples/wav2vec/unsupervised/config/timit_matched/train_text.uid -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config/timit_matched\n",
            "copying fairseq/examples/wav2vec/unsupervised/config/timit_matched/valid.uid -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config/timit_matched\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config/generate\n",
            "copying fairseq/examples/wav2vec/unsupervised/config/generate/viterbi.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config/generate\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config/gan\n",
            "copying fairseq/examples/wav2vec/unsupervised/config/gan/w2vu.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config/gan\n",
            "copying fairseq/examples/wav2vec/unsupervised/config/gan/w2vu2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config/gan\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config/timit_unmatched\n",
            "copying fairseq/examples/wav2vec/unsupervised/config/timit_unmatched/test.uid -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config/timit_unmatched\n",
            "copying fairseq/examples/wav2vec/unsupervised/config/timit_unmatched/train.uid -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config/timit_unmatched\n",
            "copying fairseq/examples/wav2vec/unsupervised/config/timit_unmatched/train_text.uid -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config/timit_unmatched\n",
            "copying fairseq/examples/wav2vec/unsupervised/config/timit_unmatched/valid.uid -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config/timit_unmatched\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config/finetuning\n",
            "copying fairseq/examples/wav2vec/unsupervised/config/finetuning/w2v_finetune.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/config/finetuning\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/path.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/decode_word_step2.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/decode_phone.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/cmd.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/train.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/decode_word_step1.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/decode.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_lang_word.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_lang.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/unsup_select_decode_word.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/train_subset_lgbeam.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/unsup_select_decode.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/score.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/copy_aligned_text.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/show_wer.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_lm.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/unsup_select.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local/prepare_data_from_w2v.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/local\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan/train_lda_mllt.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan/train_deltas.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan\n",
            "copying fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan/train_sat.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/kaldi_self_train/st/steps_gan\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/wav2vec_apply_cluster_faiss.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/vads.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/pca.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/ltr_to_wrd.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/normalize_text.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/wer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/remove_silence.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/mean_pool.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/merge_clusters.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/wav2vec_cluster_faiss.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/copy_labels.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/wrd_to_ltr.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/prepare_text.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/g2p_wrd_to_phn.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/filter_tsv.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/prepare_timit.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/normalize_and_filter_text.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/filter_lexicon.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/prepare_audio_v2.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/apply_pca.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/phonemize_with_sil.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/wav2vec_extract_features.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "copying fairseq/examples/wav2vec/unsupervised/scripts/prepare_audio.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wav2vec/unsupervised/scripts\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection\n",
            "copying fairseq/examples/attention_head_selection/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src\n",
            "copying fairseq/examples/attention_head_selection/src/speech_to_text_head_selection.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src\n",
            "copying fairseq/examples/attention_head_selection/src/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src/loss\n",
            "copying fairseq/examples/attention_head_selection/src/loss/attention_head_selection.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src/loss\n",
            "copying fairseq/examples/attention_head_selection/src/loss/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src/loss\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src/models\n",
            "copying fairseq/examples/attention_head_selection/src/models/head_selection_s2t_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src/models\n",
            "copying fairseq/examples/attention_head_selection/src/models/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src/models\n",
            "copying fairseq/examples/attention_head_selection/src/models/head_selection_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src/models\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src/modules\n",
            "copying fairseq/examples/attention_head_selection/src/modules/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src/modules\n",
            "copying fairseq/examples/attention_head_selection/src/modules/attn_head_selector.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src/modules\n",
            "copying fairseq/examples/attention_head_selection/src/modules/head_selection_transformer_layer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src/modules\n",
            "copying fairseq/examples/attention_head_selection/src/modules/multihead_attention_selection.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src/modules\n",
            "copying fairseq/examples/attention_head_selection/src/modules/multihead_functional.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src/modules\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src/data\n",
            "copying fairseq/examples/attention_head_selection/src/data/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src/data\n",
            "copying fairseq/examples/attention_head_selection/src/data/speech_to_text_dataset_with_domain.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/attention_head_selection/src/data\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/translation_moe\n",
            "copying fairseq/examples/translation_moe/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/translation_moe\n",
            "copying fairseq/examples/translation_moe/score.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/translation_moe\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/translation_moe/translation_moe_src\n",
            "copying fairseq/examples/translation_moe/translation_moe_src/mean_pool_gating_network.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/translation_moe/translation_moe_src\n",
            "copying fairseq/examples/translation_moe/translation_moe_src/translation_moe.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/translation_moe/translation_moe_src\n",
            "copying fairseq/examples/translation_moe/translation_moe_src/logsumexp_moe.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/translation_moe/translation_moe_src\n",
            "copying fairseq/examples/translation_moe/translation_moe_src/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/translation_moe/translation_moe_src\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/xlmr\n",
            "copying fairseq/examples/xlmr/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/xlmr\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/xformers\n",
            "copying fairseq/examples/xformers/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/xformers\n",
            "copying fairseq/examples/speech_to_speech/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/docs\n",
            "copying fairseq/examples/speech_to_speech/docs/data_augmentation.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/docs\n",
            "copying fairseq/examples/speech_to_speech/docs/enhanced_direct_s2st_discrete_units.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/docs\n",
            "copying fairseq/examples/speech_to_speech/docs/textless_s2st_real_data.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/docs\n",
            "copying fairseq/examples/speech_to_speech/docs/direct_s2st_discrete_units.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/docs\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/benchmarking\n",
            "copying fairseq/examples/speech_to_speech/benchmarking/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/benchmarking\n",
            "copying fairseq/examples/speech_to_speech/benchmarking/data_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/benchmarking\n",
            "copying fairseq/examples/speech_to_speech/benchmarking/core.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/benchmarking\n",
            "copying fairseq/examples/speech_to_speech/benchmarking/get_metrics.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/benchmarking\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/benchmarking/configs\n",
            "copying fairseq/examples/speech_to_speech/benchmarking/configs/DirectS2U.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/benchmarking/configs\n",
            "copying fairseq/examples/speech_to_speech/benchmarking/configs/2StageS2ST.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/benchmarking/configs\n",
            "copying fairseq/examples/speech_to_speech/benchmarking/configs/3StageS2ST.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/benchmarking/configs\n",
            "copying fairseq/examples/speech_to_speech/benchmarking/configs/S2T.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/benchmarking/configs\n",
            "copying fairseq/examples/speech_to_speech/asr_bleu/asr_model_cfgs.json -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/asr_bleu\n",
            "copying fairseq/examples/speech_to_speech/asr_bleu/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/asr_bleu\n",
            "copying fairseq/examples/speech_to_speech/asr_bleu/requirements.txt -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_speech/asr_bleu\n",
            "copying fairseq/examples/data2vec/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2\n",
            "copying fairseq/examples/data2vec/config/v2/base_text_only_task.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2\n",
            "copying fairseq/examples/data2vec/config/v2/huge_images_only_task.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2\n",
            "copying fairseq/examples/data2vec/config/v2/huge_images14_only_task.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2\n",
            "copying fairseq/examples/data2vec/config/v2/large_images_only_task.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2\n",
            "copying fairseq/examples/data2vec/config/v2/base_images_only_task.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2\n",
            "copying fairseq/examples/data2vec/config/v2/large_text_only_task_pgrp_1M.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2\n",
            "copying fairseq/examples/data2vec/config/v2/large_text_only_task.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2\n",
            "copying fairseq/examples/data2vec/config/v2/large_audio_only_task.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2\n",
            "copying fairseq/examples/data2vec/config/v2/base_audio_only_task.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/text_finetuning\n",
            "copying fairseq/examples/data2vec/config/v2/text_finetuning/cola.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/text_finetuning\n",
            "copying fairseq/examples/data2vec/config/v2/text_finetuning/rte.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/text_finetuning\n",
            "copying fairseq/examples/data2vec/config/v2/text_finetuning/qnli.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/text_finetuning\n",
            "copying fairseq/examples/data2vec/config/v2/text_finetuning/qqp.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/text_finetuning\n",
            "copying fairseq/examples/data2vec/config/v2/text_finetuning/mrpc.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/text_finetuning\n",
            "copying fairseq/examples/data2vec/config/v2/text_finetuning/mnli.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/text_finetuning\n",
            "copying fairseq/examples/data2vec/config/v2/text_finetuning/sts_b.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/text_finetuning\n",
            "copying fairseq/examples/data2vec/config/v2/text_finetuning/sst_2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/text_finetuning\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/text_finetuning/run_config\n",
            "copying fairseq/examples/data2vec/config/v2/text_finetuning/run_config/local.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/text_finetuning/run_config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/run_config\n",
            "copying fairseq/examples/data2vec/config/v2/run_config/slurm_4_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/run_config\n",
            "copying fairseq/examples/data2vec/config/v2/run_config/local.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/run_config\n",
            "copying fairseq/examples/data2vec/config/v2/run_config/slurm_2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/run_config\n",
            "copying fairseq/examples/data2vec/config/v2/run_config/slurm_1_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/run_config\n",
            "copying fairseq/examples/data2vec/config/v2/run_config/slurm_2_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/run_config\n",
            "copying fairseq/examples/data2vec/config/v2/run_config/slurm_3.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/run_config\n",
            "copying fairseq/examples/data2vec/config/v2/run_config/slurm_6_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/run_config\n",
            "copying fairseq/examples/data2vec/config/v2/run_config/slurm_8.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/run_config\n",
            "copying fairseq/examples/data2vec/config/v2/run_config/slurm_1.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/run_config\n",
            "copying fairseq/examples/data2vec/config/v2/run_config/slurm_4.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/run_config\n",
            "copying fairseq/examples/data2vec/config/v2/run_config/slurm_8_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/v2/run_config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/pretraining\n",
            "copying fairseq/examples/data2vec/config/vision/pretraining/base_mae_imagenet.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/pretraining\n",
            "copying fairseq/examples/data2vec/config/vision/pretraining/base_imagenet_d2v1.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/pretraining\n",
            "copying fairseq/examples/data2vec/config/vision/pretraining/base_imagenet.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/pretraining\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/pretraining/run_config/slurm_4_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/pretraining/run_config/local.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/pretraining/run_config/slurm_2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/pretraining/run_config/slurm_1_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/pretraining/run_config/slurm_2_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/pretraining/run_config/slurm_3.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/pretraining/run_config/slurm_6_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/pretraining/run_config/slurm_1.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/pretraining/run_config/slurm_4.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/pretraining/run_config/slurm_8_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/pretraining/run_config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/finetuning\n",
            "copying fairseq/examples/data2vec/config/vision/finetuning/imagenet.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/finetuning\n",
            "copying fairseq/examples/data2vec/config/vision/finetuning/mae_imagenet_huge_clean.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/finetuning\n",
            "copying fairseq/examples/data2vec/config/vision/finetuning/mae_imagenet_large_clean.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/finetuning\n",
            "copying fairseq/examples/data2vec/config/vision/finetuning/mae_imagenet_clean.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/finetuning\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/finetuning/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/finetuning/run_config/slurm_4_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/finetuning/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/finetuning/run_config/local.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/finetuning/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/finetuning/run_config/slurm_2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/finetuning/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/finetuning/run_config/slurm_1_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/finetuning/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/finetuning/run_config/slurm_2_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/finetuning/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/finetuning/run_config/slurm_3.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/finetuning/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/finetuning/run_config/slurm_6_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/finetuning/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/finetuning/run_config/slurm_1.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/finetuning/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/finetuning/run_config/slurm_4.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/finetuning/run_config\n",
            "copying fairseq/examples/data2vec/config/vision/finetuning/run_config/slurm_8_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/vision/finetuning/run_config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/text\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/text/pretraining\n",
            "copying fairseq/examples/data2vec/config/text/pretraining/base.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/text/pretraining\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/text/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/text/pretraining/run_config/slurm_4_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/text/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/text/pretraining/run_config/local.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/text/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/text/pretraining/run_config/slurm_2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/text/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/text/pretraining/run_config/slurm_1_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/text/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/text/pretraining/run_config/slurm_2_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/text/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/text/pretraining/run_config/slurm_3.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/text/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/text/pretraining/run_config/slurm_4.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/text/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/text/pretraining/run_config/slurm_8_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/text/pretraining/run_config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/classification\n",
            "copying fairseq/examples/data2vec/config/audio/classification/base_classification.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/classification\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/classification/run_config\n",
            "copying fairseq/examples/data2vec/config/audio/classification/run_config/slurm_2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/classification/run_config\n",
            "copying fairseq/examples/data2vec/config/audio/classification/run_config/slurm_1g.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/classification/run_config\n",
            "copying fairseq/examples/data2vec/config/audio/classification/run_config/slurm_1.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/classification/run_config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/pretraining\n",
            "copying fairseq/examples/data2vec/config/audio/pretraining/audioset.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/pretraining\n",
            "copying fairseq/examples/data2vec/config/audio/pretraining/base_librispeech.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/pretraining\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/audio/pretraining/run_config/slurm_4_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/audio/pretraining/run_config/local.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/audio/pretraining/run_config/slurm_2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/audio/pretraining/run_config/slurm_1_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/audio/pretraining/run_config/slurm_2_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/audio/pretraining/run_config/slurm_3.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/audio/pretraining/run_config/slurm_6_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/audio/pretraining/run_config/slurm_1.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/audio/pretraining/run_config/slurm_4.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/pretraining/run_config\n",
            "copying fairseq/examples/data2vec/config/audio/pretraining/run_config/slurm_8_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/config/audio/pretraining/run_config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts\n",
            "copying fairseq/examples/data2vec/scripts/convert_audioset_labels.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/multi\n",
            "copying fairseq/examples/data2vec/scripts/multi/finetune_all_fair_aws_local_lr.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/multi\n",
            "copying fairseq/examples/data2vec/scripts/multi/finetune_all_fair_aws_local_lr_nodep.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/multi\n",
            "copying fairseq/examples/data2vec/scripts/multi/finetune_all_fair_local_lr.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/multi\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/finetune_all_fair_aws_lr.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/finetune_all_fair_nodep_aws_lr_nopos.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/finetune_all_fair_aws_local_lr.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/finetune_sst2_qnli_sweep_fair_nodep.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/finetune_all_large_fair_local_lr.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/unprocess_data.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/finetune_all_fair_nodep_aws_lr.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/finetune_all_char_fair_aws_local_lr.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/glue.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/valids.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/finetune_all_fair_nodep.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/finetune_all_fair_nodep_aws_local_lr.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/finetune_all_fair_local_lr.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/finetune_all_large_fair_aws_local_lr.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/finetune_all_fair_aws.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/finetune_all_large_fair_nodep_aws_local_lr.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/finetune_all_fair.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/finetune_all_fair_nodep_aws.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "copying fairseq/examples/data2vec/scripts/text/glue_lr.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/data2vec/scripts/text\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/translation\n",
            "copying fairseq/examples/translation/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/translation\n",
            "copying fairseq/examples/translation/prepare-wmt14en2fr.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/translation\n",
            "copying fairseq/examples/translation/prepare-iwslt14.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/translation\n",
            "copying fairseq/examples/translation/prepare-wmt14en2de.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/translation\n",
            "copying fairseq/examples/translation/prepare-iwslt17-multilingual.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/translation\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/cross_lingual_language_model\n",
            "copying fairseq/examples/cross_lingual_language_model/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/cross_lingual_language_model\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/constrained_decoding\n",
            "copying fairseq/examples/constrained_decoding/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/constrained_decoding\n",
            "copying fairseq/examples/constrained_decoding/tok.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/constrained_decoding\n",
            "copying fairseq/examples/constrained_decoding/normalize.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/constrained_decoding\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/shuffled_word_order\n",
            "copying fairseq/examples/shuffled_word_order/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/shuffled_word_order\n",
            "copying fairseq/examples/shuffled_word_order/README.finetuning.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/shuffled_word_order\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wmt21\n",
            "copying fairseq/examples/wmt21/eval.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wmt21\n",
            "copying fairseq/examples/wmt21/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wmt21\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wmt21/scripts\n",
            "copying fairseq/examples/wmt21/scripts/replace-unicode-punctuation.perl -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wmt21/scripts\n",
            "copying fairseq/examples/wmt21/scripts/normalize-punctuation.perl -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wmt21/scripts\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT\n",
            "copying fairseq/examples/MMPT/setup.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT\n",
            "copying fairseq/examples/MMPT/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT\n",
            "copying fairseq/examples/MMPT/pretraining.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT\n",
            "copying fairseq/examples/MMPT/CONFIG.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT\n",
            "copying fairseq/examples/MMPT/videoclip.png -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT\n",
            "copying fairseq/examples/MMPT/vlm.png -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT\n",
            "copying fairseq/examples/MMPT/DATASET.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT\n",
            "copying fairseq/examples/MMPT/.gitignore -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT\n",
            "copying fairseq/examples/MMPT/locallaunch.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT\n",
            "copying fairseq/examples/MMPT/endtask.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects\n",
            "copying fairseq/examples/MMPT/projects/mfmmlm.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_crosstask_zs_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_crosstask_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_youcook_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_vtt_zs.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_vttqa.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_youcook.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_youcookcap.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_didemo_zs.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/youcookcap.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/coin_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/vtt_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/vtt.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/coin.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_coin_zs.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_crosstask.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/how2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_coin_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/youcook.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/vttqa_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_crosstask_zs.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_coin.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_youcook_zs.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/crosstask.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_vtt.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/ft.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/youcook_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_vttqa_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/vttqa.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_vtt_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/crosstask_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/default.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "copying fairseq/examples/MMPT/projects/task/test_vttqa_zs.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/task\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri\n",
            "copying fairseq/examples/MMPT/projects/retri/videoretri.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip/test_crosstask_zs_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip/test_crosstask_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip/test_youcook_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip/test_vtt_zs.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip/test_didemo_zs.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip/coin_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip/vtt_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip/test_coin_zs.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip/how2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip/test_coin_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip/vttqa_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip/test_youcook_zs.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip/youcook_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip/test_vttqa_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip/test_vtt_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip/crosstask_videoclip.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "copying fairseq/examples/MMPT/projects/retri/videoclip/test_vttqa_zs.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/retri/videoclip\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm\n",
            "copying fairseq/examples/MMPT/projects/mtm/mmfusionmtm.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm\n",
            "copying fairseq/examples/MMPT/projects/mtm/vlm.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm/vlm\n",
            "copying fairseq/examples/MMPT/projects/mtm/vlm/test_vttqa.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm/vlm\n",
            "copying fairseq/examples/MMPT/projects/mtm/vlm/test_youcook.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm/vlm\n",
            "copying fairseq/examples/MMPT/projects/mtm/vlm/test_youcookcap.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm/vlm\n",
            "copying fairseq/examples/MMPT/projects/mtm/vlm/youcookcap.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm/vlm\n",
            "copying fairseq/examples/MMPT/projects/mtm/vlm/vtt.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm/vlm\n",
            "copying fairseq/examples/MMPT/projects/mtm/vlm/coin.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm/vlm\n",
            "copying fairseq/examples/MMPT/projects/mtm/vlm/test_crosstask.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm/vlm\n",
            "copying fairseq/examples/MMPT/projects/mtm/vlm/how2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm/vlm\n",
            "copying fairseq/examples/MMPT/projects/mtm/vlm/youcook.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm/vlm\n",
            "copying fairseq/examples/MMPT/projects/mtm/vlm/test_crosstask_zs.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm/vlm\n",
            "copying fairseq/examples/MMPT/projects/mtm/vlm/test_coin.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm/vlm\n",
            "copying fairseq/examples/MMPT/projects/mtm/vlm/crosstask.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm/vlm\n",
            "copying fairseq/examples/MMPT/projects/mtm/vlm/test_vtt.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm/vlm\n",
            "copying fairseq/examples/MMPT/projects/mtm/vlm/vttqa.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/projects/mtm/vlm\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt\n",
            "copying fairseq/examples/MMPT/mmpt/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/losses\n",
            "copying fairseq/examples/MMPT/mmpt/losses/nce.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/losses\n",
            "copying fairseq/examples/MMPT/mmpt/losses/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/losses\n",
            "copying fairseq/examples/MMPT/mmpt/losses/fairseqmmloss.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/losses\n",
            "copying fairseq/examples/MMPT/mmpt/losses/loss.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/losses\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/tasks\n",
            "copying fairseq/examples/MMPT/mmpt/tasks/fairseqmmtask.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/tasks\n",
            "copying fairseq/examples/MMPT/mmpt/tasks/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/tasks\n",
            "copying fairseq/examples/MMPT/mmpt/tasks/milncetask.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/tasks\n",
            "copying fairseq/examples/MMPT/mmpt/tasks/retritask.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/tasks\n",
            "copying fairseq/examples/MMPT/mmpt/tasks/vlmtask.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/tasks\n",
            "copying fairseq/examples/MMPT/mmpt/tasks/task.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/tasks\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/datasets\n",
            "copying fairseq/examples/MMPT/mmpt/datasets/fairseqmmdataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/datasets\n",
            "copying fairseq/examples/MMPT/mmpt/datasets/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/datasets\n",
            "copying fairseq/examples/MMPT/mmpt/datasets/mmdataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/datasets\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/evaluators\n",
            "copying fairseq/examples/MMPT/mmpt/evaluators/metric.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/evaluators\n",
            "copying fairseq/examples/MMPT/mmpt/evaluators/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/evaluators\n",
            "copying fairseq/examples/MMPT/mmpt/evaluators/evaluator.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/evaluators\n",
            "copying fairseq/examples/MMPT/mmpt/evaluators/predictor.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/evaluators\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/utils\n",
            "copying fairseq/examples/MMPT/mmpt/utils/shardedtensor.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/utils\n",
            "copying fairseq/examples/MMPT/mmpt/utils/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/utils\n",
            "copying fairseq/examples/MMPT/mmpt/utils/load_config.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/utils\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/models\n",
            "copying fairseq/examples/MMPT/mmpt/models/fairseqmmmodel.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/models\n",
            "copying fairseq/examples/MMPT/mmpt/models/mmfusionnlg.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/models\n",
            "copying fairseq/examples/MMPT/mmpt/models/transformermodel.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/models\n",
            "copying fairseq/examples/MMPT/mmpt/models/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/models\n",
            "copying fairseq/examples/MMPT/mmpt/models/mmfusion.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/models\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/modules\n",
            "copying fairseq/examples/MMPT/mmpt/modules/retri.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/modules\n",
            "copying fairseq/examples/MMPT/mmpt/modules/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/modules\n",
            "copying fairseq/examples/MMPT/mmpt/modules/mm.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/modules\n",
            "copying fairseq/examples/MMPT/mmpt/modules/vectorpool.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/modules\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/processors\n",
            "copying fairseq/examples/MMPT/mmpt/processors/how2processor.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/processors\n",
            "copying fairseq/examples/MMPT/mmpt/processors/dsprocessor.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/processors\n",
            "copying fairseq/examples/MMPT/mmpt/processors/how2retriprocessor.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/processors\n",
            "copying fairseq/examples/MMPT/mmpt/processors/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/processors\n",
            "copying fairseq/examples/MMPT/mmpt/processors/dedupprocessor.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/processors\n",
            "copying fairseq/examples/MMPT/mmpt/processors/processor.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/processors\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/processors/models\n",
            "copying fairseq/examples/MMPT/mmpt/processors/models/s3dg.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt/processors/models\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/scripts\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/scripts/text_token_extractor\n",
            "copying fairseq/examples/MMPT/scripts/text_token_extractor/pretokenization.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/scripts/text_token_extractor\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/scripts/text_token_extractor/configs\n",
            "copying fairseq/examples/MMPT/scripts/text_token_extractor/configs/bert-base-uncased.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/scripts/text_token_extractor/configs\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/scripts/video_feature_extractor\n",
            "copying fairseq/examples/MMPT/scripts/video_feature_extractor/shard_feature.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/scripts/video_feature_extractor\n",
            "copying fairseq/examples/MMPT/scripts/video_feature_extractor/random_sequence_shuffler.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/scripts/video_feature_extractor\n",
            "copying fairseq/examples/MMPT/scripts/video_feature_extractor/extract.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/scripts/video_feature_extractor\n",
            "copying fairseq/examples/MMPT/scripts/video_feature_extractor/pathbuilder.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/scripts/video_feature_extractor\n",
            "copying fairseq/examples/MMPT/scripts/video_feature_extractor/model.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/scripts/video_feature_extractor\n",
            "copying fairseq/examples/MMPT/scripts/video_feature_extractor/preprocessing.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/scripts/video_feature_extractor\n",
            "copying fairseq/examples/MMPT/scripts/video_feature_extractor/videoreader.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/scripts/video_feature_extractor\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/scripts/video_feature_extractor/how2\n",
            "copying fairseq/examples/MMPT/scripts/video_feature_extractor/how2/s3d.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/scripts/video_feature_extractor/how2\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt_cli\n",
            "copying fairseq/examples/MMPT/mmpt_cli/localjob.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt_cli\n",
            "copying fairseq/examples/MMPT/mmpt_cli/predict.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/MMPT/mmpt_cli\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/joint_alignment_translation\n",
            "copying fairseq/examples/joint_alignment_translation/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/joint_alignment_translation\n",
            "copying fairseq/examples/joint_alignment_translation/prepare-wmt18en2de_no_norm_no_escape_no_agressive.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/joint_alignment_translation\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/unsupervised_quality_estimation\n",
            "copying fairseq/examples/unsupervised_quality_estimation/meteor.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/unsupervised_quality_estimation\n",
            "copying fairseq/examples/unsupervised_quality_estimation/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/unsupervised_quality_estimation\n",
            "copying fairseq/examples/unsupervised_quality_estimation/repeat_lines.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/unsupervised_quality_estimation\n",
            "copying fairseq/examples/unsupervised_quality_estimation/aggregate_scores.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/unsupervised_quality_estimation\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta\n",
            "copying fairseq/examples/roberta/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta\n",
            "copying fairseq/examples/roberta/multiprocessing_bpe_encoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta\n",
            "copying fairseq/examples/roberta/preprocess_RACE.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta\n",
            "copying fairseq/examples/roberta/README.custom_classification.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta\n",
            "copying fairseq/examples/roberta/README.race.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta\n",
            "copying fairseq/examples/roberta/README.glue.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta\n",
            "copying fairseq/examples/roberta/preprocess_RACE.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta\n",
            "copying fairseq/examples/roberta/preprocess_GLUE_tasks.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta\n",
            "copying fairseq/examples/roberta/README.pretraining.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/fb_multilingual\n",
            "copying fairseq/examples/roberta/fb_multilingual/README.multilingual.pretraining.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/fb_multilingual\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/pretraining\n",
            "copying fairseq/examples/roberta/config/pretraining/base.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/pretraining\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/pretraining/run_config\n",
            "copying fairseq/examples/roberta/config/pretraining/run_config/local.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/pretraining/run_config\n",
            "copying fairseq/examples/roberta/config/pretraining/run_config/slurm_2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/pretraining/run_config\n",
            "copying fairseq/examples/roberta/config/pretraining/run_config/slurm_2_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/pretraining/run_config\n",
            "copying fairseq/examples/roberta/config/pretraining/run_config/slurm_3.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/pretraining/run_config\n",
            "copying fairseq/examples/roberta/config/pretraining/run_config/slurm_4.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/pretraining/run_config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/finetuning\n",
            "copying fairseq/examples/roberta/config/finetuning/cola.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/finetuning\n",
            "copying fairseq/examples/roberta/config/finetuning/rte.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/finetuning\n",
            "copying fairseq/examples/roberta/config/finetuning/qnli.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/finetuning\n",
            "copying fairseq/examples/roberta/config/finetuning/qqp.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/finetuning\n",
            "copying fairseq/examples/roberta/config/finetuning/mrpc.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/finetuning\n",
            "copying fairseq/examples/roberta/config/finetuning/mnli.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/finetuning\n",
            "copying fairseq/examples/roberta/config/finetuning/sts_b.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/finetuning\n",
            "copying fairseq/examples/roberta/config/finetuning/sst_2.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/finetuning\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/finetuning/run_config\n",
            "copying fairseq/examples/roberta/config/finetuning/run_config/local.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/finetuning/run_config\n",
            "copying fairseq/examples/roberta/config/finetuning/run_config/slurm_1g.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/finetuning/run_config\n",
            "copying fairseq/examples/roberta/config/finetuning/run_config/slurm_1g_aws.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/config/finetuning/run_config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/commonsense_qa\n",
            "copying fairseq/examples/roberta/commonsense_qa/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/commonsense_qa\n",
            "copying fairseq/examples/roberta/commonsense_qa/download_cqa_data.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/commonsense_qa\n",
            "copying fairseq/examples/roberta/commonsense_qa/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/commonsense_qa\n",
            "copying fairseq/examples/roberta/commonsense_qa/commonsense_qa_task.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/commonsense_qa\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/wsc\n",
            "copying fairseq/examples/roberta/wsc/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/wsc\n",
            "copying fairseq/examples/roberta/wsc/wsc_criterion.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/wsc\n",
            "copying fairseq/examples/roberta/wsc/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/wsc\n",
            "copying fairseq/examples/roberta/wsc/wsc_task.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/wsc\n",
            "copying fairseq/examples/roberta/wsc/wsc_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/roberta/wsc\n",
            "copying fairseq/examples/rxf/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/rxf\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/dgslm\n",
            "copying fairseq/examples/textless_nlp/dgslm/sample_speech_dlm.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/dgslm\n",
            "copying fairseq/examples/textless_nlp/dgslm/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/dgslm\n",
            "copying fairseq/examples/textless_nlp/dgslm/dgslm_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/dgslm\n",
            "copying fairseq/examples/textless_nlp/dgslm/create_code_file.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/dgslm\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/dgslm/vocoder_hifigan\n",
            "copying fairseq/examples/textless_nlp/dgslm/vocoder_hifigan/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/dgslm/vocoder_hifigan\n",
            "copying fairseq/examples/textless_nlp/dgslm/vocoder_hifigan/generate_stereo_waveform.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/dgslm/vocoder_hifigan\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/dgslm/hubert_fisher\n",
            "copying fairseq/examples/textless_nlp/dgslm/hubert_fisher/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/dgslm/hubert_fisher\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/speech-resynth\n",
            "copying fairseq/examples/textless_nlp/speech-resynth/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/speech-resynth\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/speech-resynth/img\n",
            "copying fairseq/examples/textless_nlp/speech-resynth/img/fig.png -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/speech-resynth/img\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm\n",
            "copying fairseq/examples/textless_nlp/gslm/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/ulm\n",
            "copying fairseq/examples/textless_nlp/gslm/ulm/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/ulm\n",
            "copying fairseq/examples/textless_nlp/gslm/ulm/sample.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/ulm\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/tools\n",
            "copying fairseq/examples/textless_nlp/gslm/tools/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/tools\n",
            "copying fairseq/examples/textless_nlp/gslm/tools/resynthesize_speech.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/tools\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/metrics\n",
            "copying fairseq/examples/textless_nlp/gslm/metrics/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/metrics\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/metrics/asr_metrics\n",
            "copying fairseq/examples/textless_nlp/gslm/metrics/asr_metrics/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/metrics/asr_metrics\n",
            "copying fairseq/examples/textless_nlp/gslm/metrics/asr_metrics/ppx.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/metrics/asr_metrics\n",
            "copying fairseq/examples/textless_nlp/gslm/metrics/asr_metrics/continuation_eval.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/metrics/asr_metrics\n",
            "copying fairseq/examples/textless_nlp/gslm/metrics/asr_metrics/self_auto_bleu.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/metrics/asr_metrics\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/metrics/asr_metrics/misc\n",
            "copying fairseq/examples/textless_nlp/gslm/metrics/asr_metrics/misc/bleu_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/metrics/asr_metrics/misc\n",
            "copying fairseq/examples/textless_nlp/gslm/metrics/asr_metrics/misc/dict.ltr.txt -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/metrics/asr_metrics/misc\n",
            "copying fairseq/examples/textless_nlp/gslm/metrics/asr_metrics/misc/cut_as.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/metrics/asr_metrics/misc\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/metrics/abx_metrics\n",
            "copying fairseq/examples/textless_nlp/gslm/metrics/abx_metrics/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/metrics/abx_metrics\n",
            "copying fairseq/examples/textless_nlp/gslm/metrics/abx_metrics/dump_abx_feats.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/metrics/abx_metrics\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/speech2unit\n",
            "copying fairseq/examples/textless_nlp/gslm/speech2unit/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/speech2unit\n",
            "copying fairseq/examples/textless_nlp/gslm/speech2unit/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/speech2unit\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/speech2unit/pretrained\n",
            "copying fairseq/examples/textless_nlp/gslm/speech2unit/pretrained/w2v2_feature_reader.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/speech2unit/pretrained\n",
            "copying fairseq/examples/textless_nlp/gslm/speech2unit/pretrained/hubert_feature_reader.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/speech2unit/pretrained\n",
            "copying fairseq/examples/textless_nlp/gslm/speech2unit/pretrained/cpc_feature_reader.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/speech2unit/pretrained\n",
            "copying fairseq/examples/textless_nlp/gslm/speech2unit/pretrained/logmel_feature_reader.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/speech2unit/pretrained\n",
            "copying fairseq/examples/textless_nlp/gslm/speech2unit/pretrained/utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/speech2unit/pretrained\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/speech2unit/clustering\n",
            "copying fairseq/examples/textless_nlp/gslm/speech2unit/clustering/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/speech2unit/clustering\n",
            "copying fairseq/examples/textless_nlp/gslm/speech2unit/clustering/cluster_kmeans.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/speech2unit/clustering\n",
            "copying fairseq/examples/textless_nlp/gslm/speech2unit/clustering/utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/speech2unit/clustering\n",
            "copying fairseq/examples/textless_nlp/gslm/speech2unit/clustering/quantize_with_kmeans.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/speech2unit/clustering\n",
            "copying fairseq/examples/textless_nlp/gslm/speech2unit/clustering/dump_feats.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/speech2unit/clustering\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/glow.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/synthesize_audio_from_units.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/tts_data.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/multiproc.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/convert_to_16k.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2/waveglow_denoiser.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2/model.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2/cleaners.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2/audio_processing.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2/symbols.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2/text.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2/utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2/cmudict.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2/stft.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2/numbers.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2\n",
            "copying fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2/layers.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/gslm/unit2speech/tacotron2\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm\n",
            "copying fairseq/examples/textless_nlp/pgslm/generate_waveform.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm\n",
            "copying fairseq/examples/textless_nlp/pgslm/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm\n",
            "copying fairseq/examples/textless_nlp/pgslm/quantize_f0.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm\n",
            "copying fairseq/examples/textless_nlp/pgslm/inference_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm\n",
            "copying fairseq/examples/textless_nlp/pgslm/data_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm\n",
            "copying fairseq/examples/textless_nlp/pgslm/prepare_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm\n",
            "copying fairseq/examples/textless_nlp/pgslm/naive_decoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm\n",
            "copying fairseq/examples/textless_nlp/pgslm/truncated_laplace.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm\n",
            "copying fairseq/examples/textless_nlp/pgslm/preprocess_f0.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm/scripts\n",
            "copying fairseq/examples/textless_nlp/pgslm/scripts/join_units_manifest.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm/scripts\n",
            "copying fairseq/examples/textless_nlp/pgslm/scripts/prepare_f0_quantization.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm/scripts\n",
            "copying fairseq/examples/textless_nlp/pgslm/scripts/prepare_data.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm/scripts\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm/sample\n",
            "copying fairseq/examples/textless_nlp/pgslm/sample/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm/sample\n",
            "copying fairseq/examples/textless_nlp/pgslm/sample/sample.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm/sample\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm/eval\n",
            "copying fairseq/examples/textless_nlp/pgslm/eval/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm/eval\n",
            "copying fairseq/examples/textless_nlp/pgslm/eval/cont_metrics.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/textless_nlp/pgslm/eval\n",
            "copying fairseq/examples/adaptive_span/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/adaptive_span\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert\n",
            "copying fairseq/examples/mr_hubert/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert\n",
            "copying fairseq/examples/mr_hubert/decode.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert\n",
            "copying fairseq/examples/mr_hubert/finetune.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert\n",
            "copying fairseq/examples/mr_hubert/train.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/finetune\n",
            "copying fairseq/examples/mr_hubert/config/finetune/base_1h_large.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/finetune\n",
            "copying fairseq/examples/mr_hubert/config/finetune/base_100h.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/finetune\n",
            "copying fairseq/examples/mr_hubert/config/finetune/base_10h_large.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/finetune\n",
            "copying fairseq/examples/mr_hubert/config/finetune/base_100h_large.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/finetune\n",
            "copying fairseq/examples/mr_hubert/config/finetune/base_10h.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/finetune\n",
            "copying fairseq/examples/mr_hubert/config/finetune/base_1h.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/finetune\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/pretrain\n",
            "copying fairseq/examples/mr_hubert/config/pretrain/mrhubert_base_librispeech.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/pretrain\n",
            "copying fairseq/examples/mr_hubert/config/pretrain/mrhubert_large_librilight.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/pretrain\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/pretrain/run\n",
            "copying fairseq/examples/mr_hubert/config/pretrain/run/submitit_reg.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/pretrain/run\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/decode\n",
            "copying fairseq/examples/mr_hubert/config/decode/infer.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/decode\n",
            "copying fairseq/examples/mr_hubert/config/decode/infer_lm.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/decode\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/decode/run\n",
            "copying fairseq/examples/mr_hubert/config/decode/run/submitit_slurm.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/decode/run\n",
            "copying fairseq/examples/mr_hubert/config/decode/run/submitit_slurm_8gpu.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/config/decode/run\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/simple_kmeans\n",
            "copying fairseq/examples/mr_hubert/simple_kmeans/dump_mfcc_feature.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/simple_kmeans\n",
            "copying fairseq/examples/mr_hubert/simple_kmeans/dump_hubert_feature_s2t.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/simple_kmeans\n",
            "copying fairseq/examples/mr_hubert/simple_kmeans/learn_kmeans.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/simple_kmeans\n",
            "copying fairseq/examples/mr_hubert/simple_kmeans/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/simple_kmeans\n",
            "copying fairseq/examples/mr_hubert/simple_kmeans/feature_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/simple_kmeans\n",
            "copying fairseq/examples/mr_hubert/simple_kmeans/dump_hubert_feature.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/simple_kmeans\n",
            "copying fairseq/examples/mr_hubert/simple_kmeans/dump_km_label.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/simple_kmeans\n",
            "copying fairseq/examples/mr_hubert/simple_kmeans/dump_w2v2_feature.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mr_hubert/simple_kmeans\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/bart\n",
            "copying fairseq/examples/bart/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/bart\n",
            "copying fairseq/examples/bart/README.summarization.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/bart\n",
            "copying fairseq/examples/bart/README.glue.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/bart\n",
            "copying fairseq/examples/bart/summarize.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/bart\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/gottbert\n",
            "copying fairseq/examples/gottbert/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/gottbert\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/normformer\n",
            "copying fairseq/examples/normformer/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/normformer\n",
            "copying fairseq/examples/normformer/train_lm.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/normformer\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/language_model\n",
            "copying fairseq/examples/language_model/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/language_model\n",
            "copying fairseq/examples/language_model/prepare-wikitext-103.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/language_model\n",
            "copying fairseq/examples/language_model/README.adaptive_inputs.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/language_model\n",
            "copying fairseq/examples/language_model/README.conv.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/language_model\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/criss\n",
            "copying fairseq/examples/criss/download_and_preprocess_tatoeba.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/criss\n",
            "copying fairseq/examples/criss/download_and_preprocess_flores_test.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/criss\n",
            "copying fairseq/examples/criss/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/criss\n",
            "copying fairseq/examples/criss/save_encoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/criss\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/criss/sentence_retrieval\n",
            "copying fairseq/examples/criss/sentence_retrieval/sentence_retrieval_tatoeba.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/criss/sentence_retrieval\n",
            "copying fairseq/examples/criss/sentence_retrieval/encoder_analysis.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/criss/sentence_retrieval\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/criss/mining\n",
            "copying fairseq/examples/criss/mining/mine_example.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/criss/mining\n",
            "copying fairseq/examples/criss/mining/mine.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/criss/mining\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/criss/unsupervised_mt\n",
            "copying fairseq/examples/criss/unsupervised_mt/eval.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/criss/unsupervised_mt\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/xmod\n",
            "copying fairseq/examples/xmod/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/xmod\n",
            "copying fairseq/examples/xmod/preprocess_nli.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/xmod\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/flores101\n",
            "copying fairseq/examples/flores101/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/flores101\n",
            "copying fairseq/examples/flores101/flores_logo.png -> build/lib.linux-x86_64-cpython-310/fairseq/examples/flores101\n",
            "copying fairseq/examples/speech_text_joint_to_text/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/docs\n",
            "copying fairseq/examples/speech_text_joint_to_text/docs/pre-training.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/docs\n",
            "copying fairseq/examples/speech_text_joint_to_text/docs/iwslt2021.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/docs\n",
            "copying fairseq/examples/speech_text_joint_to_text/docs/ende-mustc.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/docs\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/scripts\n",
            "copying fairseq/examples/speech_text_joint_to_text/scripts/convert_model.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/scripts\n",
            "copying fairseq/examples/speech_text_joint_to_text/scripts/g2p_encode.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/scripts\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/configs\n",
            "copying fairseq/examples/speech_text_joint_to_text/configs/mustc_noise.list -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/configs\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/data\n",
            "copying fairseq/examples/speech_text_joint_to_text/data/pair_denoising_dataset.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_text_joint_to_text/data\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/wmt20\n",
            "copying fairseq/examples/wmt20/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/wmt20\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/womens_bios\n",
            "copying fairseq/examples/womens_bios/query_occupations_from_wikidata.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/womens_bios\n",
            "copying fairseq/examples/womens_bios/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/womens_bios\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/linformer\n",
            "copying fairseq/examples/linformer/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/linformer\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/linformer/linformer_src\n",
            "copying fairseq/examples/linformer/linformer_src/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/linformer/linformer_src\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/linformer/linformer_src/models\n",
            "copying fairseq/examples/linformer/linformer_src/models/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/linformer/linformer_src/models\n",
            "copying fairseq/examples/linformer/linformer_src/models/linformer_roberta.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/linformer/linformer_src/models\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/linformer/linformer_src/modules\n",
            "copying fairseq/examples/linformer/linformer_src/modules/linformer_sentence_encoder.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/linformer/linformer_src/modules\n",
            "copying fairseq/examples/linformer/linformer_src/modules/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/linformer/linformer_src/modules\n",
            "copying fairseq/examples/linformer/linformer_src/modules/linformer_sentence_encoder_layer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/linformer/linformer_src/modules\n",
            "copying fairseq/examples/linformer/linformer_src/modules/multihead_linear_attention.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/linformer/linformer_src/modules\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/pointer_generator\n",
            "copying fairseq/examples/pointer_generator/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/pointer_generator\n",
            "copying fairseq/examples/pointer_generator/preprocess.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/pointer_generator\n",
            "copying fairseq/examples/pointer_generator/postprocess.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/pointer_generator\n",
            "copying fairseq/examples/pointer_generator/README.xsum.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/pointer_generator\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/pointer_generator/pointer_generator_src\n",
            "copying fairseq/examples/pointer_generator/pointer_generator_src/transformer_pg.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/pointer_generator/pointer_generator_src\n",
            "copying fairseq/examples/pointer_generator/pointer_generator_src/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/pointer_generator/pointer_generator_src\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/conv_seq2seq\n",
            "copying fairseq/examples/conv_seq2seq/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/conv_seq2seq\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/stories\n",
            "copying fairseq/examples/stories/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/stories\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/moe_lm\n",
            "copying fairseq/examples/moe_lm/model_card.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/moe_lm\n",
            "copying fairseq/examples/moe_lm/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/moe_lm\n",
            "copying fairseq/examples/moe_lm/data_card.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/moe_lm\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/audio_nlp\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/audio_nlp/nlu\n",
            "copying fairseq/examples/audio_nlp/nlu/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/audio_nlp/nlu\n",
            "copying fairseq/examples/audio_nlp/nlu/create_dict_stop.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/audio_nlp/nlu\n",
            "copying fairseq/examples/audio_nlp/nlu/generate_manifests.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/audio_nlp/nlu\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/audio_nlp/nlu/configs\n",
            "copying fairseq/examples/audio_nlp/nlu/configs/nlu_finetuning.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/audio_nlp/nlu/configs\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion\n",
            "copying fairseq/examples/emotion_conversion/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion\n",
            "copying fairseq/examples/emotion_conversion/synthesize.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion\n",
            "copying fairseq/examples/emotion_conversion/requirements.txt -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/preprocess\n",
            "copying fairseq/examples/emotion_conversion/preprocess/split_emov_km_tsv_by_uttid.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/preprocess\n",
            "copying fairseq/examples/emotion_conversion/preprocess/build_hifigan_manifest.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/preprocess\n",
            "copying fairseq/examples/emotion_conversion/preprocess/process_km.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/preprocess\n",
            "copying fairseq/examples/emotion_conversion/preprocess/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/preprocess\n",
            "copying fairseq/examples/emotion_conversion/preprocess/split_km.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/preprocess\n",
            "copying fairseq/examples/emotion_conversion/preprocess/build_translation_manifests.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/preprocess\n",
            "copying fairseq/examples/emotion_conversion/preprocess/split_km_tsv.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/preprocess\n",
            "copying fairseq/examples/emotion_conversion/preprocess/extract_f0.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/preprocess\n",
            "copying fairseq/examples/emotion_conversion/preprocess/create_core_manifest.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/preprocess\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/emotion_models\n",
            "copying fairseq/examples/emotion_conversion/emotion_models/pitch_predictor.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/emotion_models\n",
            "copying fairseq/examples/emotion_conversion/emotion_models/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/emotion_models\n",
            "copying fairseq/examples/emotion_conversion/emotion_models/duration_predictor.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/emotion_models\n",
            "copying fairseq/examples/emotion_conversion/emotion_models/utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/emotion_models\n",
            "copying fairseq/examples/emotion_conversion/emotion_models/pitch_predictor.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/emotion_models\n",
            "copying fairseq/examples/emotion_conversion/emotion_models/duration_predictor.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/emotion_models\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/fairseq_models\n",
            "copying fairseq/examples/emotion_conversion/fairseq_models/__init__.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/emotion_conversion/fairseq_models\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/mbart\n",
            "copying fairseq/examples/mbart/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/mbart\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/xglm\n",
            "copying fairseq/examples/xglm/model_card.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/xglm\n",
            "copying fairseq/examples/xglm/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/xglm\n",
            "copying fairseq/examples/xglm/XStoryCloze.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/xglm\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/paraphraser\n",
            "copying fairseq/examples/paraphraser/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/paraphraser\n",
            "copying fairseq/examples/paraphraser/paraphrase.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/paraphraser\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual\n",
            "copying fairseq/examples/multilingual/finetune_multilingual_model.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual\n",
            "copying fairseq/examples/multilingual/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual\n",
            "copying fairseq/examples/multilingual/train_multilingual_model.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual\n",
            "copying fairseq/examples/multilingual/multilingual_fairseq_gen.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual\n",
            "copying fairseq/examples/multilingual/ML50_langs.txt -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/check_valid_test_overlaps.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/check_self_overlaps.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/download_lotus.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/dedup_all.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/remove_valid_test_in_train.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/requirement.txt -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/download_iitb.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/binarize.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/download_af_xh.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/download_wat19_my.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/download_wmt20.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/download_iwslt_and_extract.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/preprocess_ML50_v1.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/download_ML50_v1.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/download_wmt19_and_before.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/download_flores_data.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/check_iswlt_test_data.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "copying fairseq/examples/multilingual/data_scripts/download_ted_and_extract.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts/utils\n",
            "copying fairseq/examples/multilingual/data_scripts/utils/strip_sgm.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts/utils\n",
            "copying fairseq/examples/multilingual/data_scripts/utils/fasttext_multi_filter.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts/utils\n",
            "copying fairseq/examples/multilingual/data_scripts/utils/dedup.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/multilingual/data_scripts/utils\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/operators\n",
            "copying fairseq/examples/operators/alignment_train_cpu.cpp -> build/lib.linux-x86_64-cpython-310/fairseq/examples/operators\n",
            "copying fairseq/examples/operators/alignment_train_cuda.h -> build/lib.linux-x86_64-cpython-310/fairseq/examples/operators\n",
            "copying fairseq/examples/operators/alignment_train_cuda.cpp -> build/lib.linux-x86_64-cpython-310/fairseq/examples/operators\n",
            "copying fairseq/examples/operators/alignment_train_kernel.cu -> build/lib.linux-x86_64-cpython-310/fairseq/examples/operators\n",
            "copying fairseq/examples/operators/utils.h -> build/lib.linux-x86_64-cpython-310/fairseq/examples/operators\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_text\n",
            "copying fairseq/examples/speech_to_text/prep_mustc_data.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_text\n",
            "copying fairseq/examples/speech_to_text/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_text\n",
            "copying fairseq/examples/speech_to_text/data_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_text\n",
            "copying fairseq/examples/speech_to_text/prep_covost_data.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_text\n",
            "copying fairseq/examples/speech_to_text/prep_librispeech_data.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_text\n",
            "copying fairseq/examples/speech_to_text/prep_mtedx_data.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_text\n",
            "copying fairseq/examples/speech_to_text/seg_mustc_data.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_text\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_text/docs\n",
            "copying fairseq/examples/speech_to_text/docs/mustc_example.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_text/docs\n",
            "copying fairseq/examples/speech_to_text/docs/covost_example.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_text/docs\n",
            "copying fairseq/examples/speech_to_text/docs/mtedx_example.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_text/docs\n",
            "copying fairseq/examples/speech_to_text/docs/librispeech_example.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_text/docs\n",
            "copying fairseq/examples/speech_to_text/docs/simulst_mustc_example.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_text/docs\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_text/simultaneous_translation\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_text/simultaneous_translation/agents\n",
            "copying fairseq/examples/speech_to_text/simultaneous_translation/agents/fairseq_simul_st_agent.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_to_text/simultaneous_translation/agents\n",
            "copying fairseq/examples/speech_recognition/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition\n",
            "copying fairseq/examples/speech_recognition/kaldi/add-self-loop-simple.cc -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/kaldi\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/kaldi/config\n",
            "copying fairseq/examples/speech_recognition/kaldi/config/kaldi_initializer.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/kaldi/config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/datasets\n",
            "copying fairseq/examples/speech_recognition/datasets/asr_prep_json.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/datasets\n",
            "copying fairseq/examples/speech_recognition/datasets/prepare-librispeech.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/datasets\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/utils\n",
            "copying fairseq/examples/speech_recognition/utils/wer_utils.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/utils\n",
            "copying fairseq/examples/speech_recognition/new/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new/conf\n",
            "copying fairseq/examples/speech_recognition/new/conf/infer.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new/conf\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new/conf/hydra\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new/conf/hydra/sweeper\n",
            "copying fairseq/examples/speech_recognition/new/conf/hydra/sweeper/ax_sil.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new/conf/hydra/sweeper\n",
            "copying fairseq/examples/speech_recognition/new/conf/hydra/sweeper/ax.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new/conf/hydra/sweeper\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new/conf/run_config\n",
            "copying fairseq/examples/speech_recognition/new/conf/run_config/fb_slurm_1.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new/conf/run_config\n",
            "copying fairseq/examples/speech_recognition/new/conf/run_config/fb_slurm_2g.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/examples/speech_recognition/new/conf/run_config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/byte_level_bpe\n",
            "copying fairseq/examples/byte_level_bpe/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/byte_level_bpe\n",
            "copying fairseq/examples/byte_level_bpe/get_bitext.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/byte_level_bpe\n",
            "copying fairseq/examples/byte_level_bpe/get_data.sh -> build/lib.linux-x86_64-cpython-310/fairseq/examples/byte_level_bpe\n",
            "copying fairseq/examples/byte_level_bpe/gru_transformer.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/byte_level_bpe\n",
            "copying fairseq/examples/simultaneous_translation/README.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/docs\n",
            "copying fairseq/examples/simultaneous_translation/docs/ende-mma.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/docs\n",
            "copying fairseq/examples/simultaneous_translation/docs/enja-waitk.md -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/docs\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/eval\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/eval/agents\n",
            "copying fairseq/examples/simultaneous_translation/eval/agents/simul_t2t_enja.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/eval/agents\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/tests\n",
            "copying fairseq/examples/simultaneous_translation/tests/test_alignment_train.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/tests\n",
            "copying fairseq/examples/simultaneous_translation/tests/test_text_models.py -> build/lib.linux-x86_64-cpython-310/fairseq/examples/simultaneous_translation/tests\n",
            "copying fairseq/config/config.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/config\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/config/model\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/config/model/wav2vec\n",
            "copying fairseq/config/model/wav2vec/vq_wav2vec_gumbel.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/config/model/wav2vec\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/config/model/transformer_lm\n",
            "copying fairseq/config/model/transformer_lm/transformer_lm_baevski_wiki103.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/config/model/transformer_lm\n",
            "copying fairseq/config/model/transformer_lm/transformer_lm_gbw.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/config/model/transformer_lm\n",
            "copying fairseq/config/model/transformer_lm/transformer_lm_gpt2_small.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/config/model/transformer_lm\n",
            "copying fairseq/config/model/transformer_lm/transformer_lm_wiki103.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/config/model/transformer_lm\n",
            "copying fairseq/config/model/transformer_lm/transformer_lm_baevski_gbw.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/config/model/transformer_lm\n",
            "copying fairseq/config/model/transformer_lm/transformer_lm_big.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/config/model/transformer_lm\n",
            "copying fairseq/config/model/transformer_lm/transformer_lm_gpt2_big.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/config/model/transformer_lm\n",
            "copying fairseq/config/model/transformer_lm/transformer_lm_gpt.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/config/model/transformer_lm\n",
            "copying fairseq/config/model/transformer_lm/transformer_lm_gpt2_medium.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/config/model/transformer_lm\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/config/model/wav2vec2\n",
            "copying fairseq/config/model/wav2vec2/wav2vec2_base.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/config/model/wav2vec2\n",
            "copying fairseq/config/model/wav2vec2/wav2vec2_large.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/config/model/wav2vec2\n",
            "creating build/lib.linux-x86_64-cpython-310/fairseq/config/fb_run_config\n",
            "copying fairseq/config/fb_run_config/slurm.yaml -> build/lib.linux-x86_64-cpython-310/fairseq/config/fb_run_config\n",
            "running build_ext\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/cpp_extension.py:500: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "  warnings.warn(msg.format('we could not find ninja.'))\n",
            "building 'fairseq.libbleu' extension\n",
            "creating build/temp.linux-x86_64-cpython-310\n",
            "creating build/temp.linux-x86_64-cpython-310/fairseq\n",
            "creating build/temp.linux-x86_64-cpython-310/fairseq/clib\n",
            "creating build/temp.linux-x86_64-cpython-310/fairseq/clib/libbleu\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/python3.10 -c fairseq/clib/libbleu/libbleu.cpp -o build/temp.linux-x86_64-cpython-310/fairseq/clib/libbleu/libbleu.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/python3.10 -c fairseq/clib/libbleu/module.cpp -o build/temp.linux-x86_64-cpython-310/fairseq/clib/libbleu/module.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fairseq/clib/libbleu/libbleu.o build/temp.linux-x86_64-cpython-310/fairseq/clib/libbleu/module.o -L/usr/lib/x86_64-linux-gnu -o build/lib.linux-x86_64-cpython-310/fairseq/libbleu.cpython-310-x86_64-linux-gnu.so\n",
            "building 'fairseq.data.data_utils_fast' extension\n",
            "creating build/temp.linux-x86_64-cpython-310/fairseq/data\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c fairseq/data/data_utils_fast.cpp -o build/temp.linux-x86_64-cpython-310/fairseq/data/data_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=data_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1929\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/arrayobject.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairseq/data/data_utils_fast.cpp:1273\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcpp\u0007-Wcpp\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   17 | #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
            "      |  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fairseq/data/data_utils_fast.o -L/usr/lib/x86_64-linux-gnu -o build/lib.linux-x86_64-cpython-310/fairseq/data/data_utils_fast.cpython-310-x86_64-linux-gnu.so\n",
            "building 'fairseq.data.token_block_utils_fast' extension\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/local/lib/python3.10/dist-packages/numpy/core/include -I/usr/include/python3.10 -c fairseq/data/token_block_utils_fast.cpp -o build/temp.linux-x86_64-cpython-310/fairseq/data/token_block_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=token_block_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n",
            "In file included from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/ndarraytypes.h:1929\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/ndarrayobject.h:12\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/arrayobject.h:5\u001b[m\u001b[K,\n",
            "                 from \u001b[01m\u001b[Kfairseq/data/token_block_utils_fast.cpp:1274\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[K/usr/local/lib/python3.10/dist-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K#warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wcpp\u0007-Wcpp\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "   17 | #\u001b[01;35m\u001b[Kwarning\u001b[m\u001b[K \"Using deprecated NumPy API, disable it with \" \\\n",
            "      |  \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fairseq/data/token_block_utils_fast.o -L/usr/lib/x86_64-linux-gnu -o build/lib.linux-x86_64-cpython-310/fairseq/data/token_block_utils_fast.cpython-310-x86_64-linux-gnu.so\n",
            "building 'fairseq.libbase' extension\n",
            "creating build/temp.linux-x86_64-cpython-310/fairseq/clib/libbase\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c fairseq/clib/libbase/balanced_assignment.cpp -o build/temp.linux-x86_64-cpython-310/fairseq/clib/libbase/balanced_assignment.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=libbase -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fairseq/clib/libbase/balanced_assignment.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/fairseq/libbase.cpython-310-x86_64-linux-gnu.so\n",
            "building 'fairseq.libnat' extension\n",
            "creating build/temp.linux-x86_64-cpython-310/fairseq/clib/libnat\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c fairseq/clib/libnat/edit_dist.cpp -o build/temp.linux-x86_64-cpython-310/fairseq/clib/libnat/edit_dist.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=libnat -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/fairseq/clib/libnat/edit_dist.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/fairseq/libnat.cpython-310-x86_64-linux-gnu.so\n",
            "building 'alignment_train_cpu_binding' extension\n",
            "creating build/temp.linux-x86_64-cpython-310/examples\n",
            "creating build/temp.linux-x86_64-cpython-310/examples/operators\n",
            "x86_64-linux-gnu-gcc -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/local/lib/python3.10/dist-packages/torch/include -I/usr/local/lib/python3.10/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.10/dist-packages/torch/include/TH -I/usr/local/lib/python3.10/dist-packages/torch/include/THC -I/usr/include/python3.10 -c examples/operators/alignment_train_cpu.cpp -o build/temp.linux-x86_64-cpython-310/examples/operators/alignment_train_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -DTORCH_EXTENSION_NAME=alignment_train_cpu_binding -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
            "In function ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPUImpl(const T*, T*, uint32_t, uint32_t, uint32_t, float) [with T = double]\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[K{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>::<lambda()>\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[K{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:131:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid free(void*)\u001b[m\u001b[K’ called on pointer returned from a mismatched allocation function [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmismatched-new-delete\u0007-Wmismatched-new-delete\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  131 |   \u001b[01;35m\u001b[Kfree(cumprod_1mp)\u001b[m\u001b[K;\n",
            "      |   \u001b[01;35m\u001b[K~~~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:93:20:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kreturned from ‘\u001b[01m\u001b[Kvoid* operator new [](std::size_t)\u001b[m\u001b[K’\n",
            "   93 |   T* cumprod_1mp = \u001b[01;36m\u001b[Knew T[elements]\u001b[m\u001b[K;\n",
            "      |                    \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In function ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPUImpl(const T*, T*, uint32_t, uint32_t, uint32_t, float) [with T = double]\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[K{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>::<lambda()>\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[K{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:132:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid free(void*)\u001b[m\u001b[K’ called on pointer returned from a mismatched allocation function [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmismatched-new-delete\u0007-Wmismatched-new-delete\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  132 |   \u001b[01;35m\u001b[Kfree(cumprod_1mp_clamp)\u001b[m\u001b[K;\n",
            "      |   \u001b[01;35m\u001b[K~~~~^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:94:26:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kreturned from ‘\u001b[01m\u001b[Kvoid* operator new [](std::size_t)\u001b[m\u001b[K’\n",
            "   94 |   T* cumprod_1mp_clamp = \u001b[01;36m\u001b[Knew T[elements]\u001b[m\u001b[K;\n",
            "      |                          \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In function ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPUImpl(const T*, T*, uint32_t, uint32_t, uint32_t, float) [with T = float]\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[K{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>::<lambda()>\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[K{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:131:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid free(void*)\u001b[m\u001b[K’ called on pointer returned from a mismatched allocation function [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmismatched-new-delete\u0007-Wmismatched-new-delete\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  131 |   \u001b[01;35m\u001b[Kfree(cumprod_1mp)\u001b[m\u001b[K;\n",
            "      |   \u001b[01;35m\u001b[K~~~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:93:20:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kreturned from ‘\u001b[01m\u001b[Kvoid* operator new [](std::size_t)\u001b[m\u001b[K’\n",
            "   93 |   T* cumprod_1mp = \u001b[01;36m\u001b[Knew T[elements]\u001b[m\u001b[K;\n",
            "      |                    \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In function ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPUImpl(const T*, T*, uint32_t, uint32_t, uint32_t, float) [with T = float]\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[K{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>::<lambda()>\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[K{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:132:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid free(void*)\u001b[m\u001b[K’ called on pointer returned from a mismatched allocation function [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmismatched-new-delete\u0007-Wmismatched-new-delete\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  132 |   \u001b[01;35m\u001b[Kfree(cumprod_1mp_clamp)\u001b[m\u001b[K;\n",
            "      |   \u001b[01;35m\u001b[K~~~~^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:94:26:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kreturned from ‘\u001b[01m\u001b[Kvoid* operator new [](std::size_t)\u001b[m\u001b[K’\n",
            "   94 |   T* cumprod_1mp_clamp = \u001b[01;36m\u001b[Knew T[elements]\u001b[m\u001b[K;\n",
            "      |                          \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In function ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPUImpl(const T*, T*, uint32_t, uint32_t, uint32_t, float) [with T = c10::Half]\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[K{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>::<lambda()>\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[K{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:131:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid free(void*)\u001b[m\u001b[K’ called on pointer returned from a mismatched allocation function [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmismatched-new-delete\u0007-Wmismatched-new-delete\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  131 |   \u001b[01;35m\u001b[Kfree(cumprod_1mp)\u001b[m\u001b[K;\n",
            "      |   \u001b[01;35m\u001b[K~~~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:93:20:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kreturned from ‘\u001b[01m\u001b[Kvoid* operator new [](std::size_t)\u001b[m\u001b[K’\n",
            "   93 |   T* cumprod_1mp = \u001b[01;36m\u001b[Knew T[elements]\u001b[m\u001b[K;\n",
            "      |                    \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In function ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPUImpl(const T*, T*, uint32_t, uint32_t, uint32_t, float) [with T = c10::Half]\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[K{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>::<lambda()>\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[K{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:132:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid free(void*)\u001b[m\u001b[K’ called on pointer returned from a mismatched allocation function [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmismatched-new-delete\u0007-Wmismatched-new-delete\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  132 |   \u001b[01;35m\u001b[Kfree(cumprod_1mp_clamp)\u001b[m\u001b[K;\n",
            "      |   \u001b[01;35m\u001b[K~~~~^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:94:26:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kreturned from ‘\u001b[01m\u001b[Kvoid* operator new [](std::size_t)\u001b[m\u001b[K’\n",
            "   94 |   T* cumprod_1mp_clamp = \u001b[01;36m\u001b[Knew T[elements]\u001b[m\u001b[K;\n",
            "      |                          \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In function ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPUImpl(const T*, T*, uint32_t, uint32_t, uint32_t, float) [with T = c10::BFloat16]\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[K{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>::<lambda()>\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[K{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:131:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid free(void*)\u001b[m\u001b[K’ called on pointer returned from a mismatched allocation function [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmismatched-new-delete\u0007-Wmismatched-new-delete\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  131 |   \u001b[01;35m\u001b[Kfree(cumprod_1mp)\u001b[m\u001b[K;\n",
            "      |   \u001b[01;35m\u001b[K~~~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:93:20:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kreturned from ‘\u001b[01m\u001b[Kvoid* operator new [](std::size_t)\u001b[m\u001b[K’\n",
            "   93 |   T* cumprod_1mp = \u001b[01;36m\u001b[Knew T[elements]\u001b[m\u001b[K;\n",
            "      |                    \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "In function ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPUImpl(const T*, T*, uint32_t, uint32_t, uint32_t, float) [with T = c10::BFloat16]\u001b[m\u001b[K’,\n",
            "    inlined from ‘\u001b[01m\u001b[K{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>::<lambda()>\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[K{anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)::<lambda()>\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K,\n",
            "    inlined from ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)\u001b[m\u001b[K’ at \u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:143:3\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:132:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kvoid free(void*)\u001b[m\u001b[K’ called on pointer returned from a mismatched allocation function [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wmismatched-new-delete\u0007-Wmismatched-new-delete\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  132 |   \u001b[01;35m\u001b[Kfree(cumprod_1mp_clamp)\u001b[m\u001b[K;\n",
            "      |   \u001b[01;35m\u001b[K~~~~^~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid {anonymous}::alignmentTrainCPU(const at::Tensor&, at::Tensor&, float)\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[Kexamples/operators/alignment_train_cpu.cpp:94:26:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[Kreturned from ‘\u001b[01m\u001b[Kvoid* operator new [](std::size_t)\u001b[m\u001b[K’\n",
            "   94 |   T* cumprod_1mp_clamp = \u001b[01;36m\u001b[Knew T[elements]\u001b[m\u001b[K;\n",
            "      |                          \u001b[01;36m\u001b[K^~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-g++ -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-310/examples/operators/alignment_train_cpu.o -L/usr/local/lib/python3.10/dist-packages/torch/lib -L/usr/lib/x86_64-linux-gnu -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-cpython-310/alignment_train_cpu_binding.cpython-310-x86_64-linux-gnu.so\n",
            "running develop\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py:40: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` and ``easy_install``.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  easy_install.initialize_options(self)\n",
            "/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
            "!!\n",
            "\n",
            "        ********************************************************************************\n",
            "        Please avoid running ``setup.py`` directly.\n",
            "        Instead, use pypa/build, pypa/installer, pypa/build or\n",
            "        other standards-based tools.\n",
            "\n",
            "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
            "        ********************************************************************************\n",
            "\n",
            "!!\n",
            "  self.initialize_options()\n",
            "running egg_info\n",
            "writing fairseq.egg-info/PKG-INFO\n",
            "writing dependency_links to fairseq.egg-info/dependency_links.txt\n",
            "writing entry points to fairseq.egg-info/entry_points.txt\n",
            "writing requirements to fairseq.egg-info/requires.txt\n",
            "writing top-level names to fairseq.egg-info/top_level.txt\n",
            "reading manifest file 'fairseq.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'fairseq.egg-info/SOURCES.txt'\n",
            "running build_ext\n",
            "copying build/lib.linux-x86_64-cpython-310/fairseq/libbleu.cpython-310-x86_64-linux-gnu.so -> fairseq\n",
            "copying build/lib.linux-x86_64-cpython-310/fairseq/data/data_utils_fast.cpython-310-x86_64-linux-gnu.so -> fairseq/data\n",
            "copying build/lib.linux-x86_64-cpython-310/fairseq/data/token_block_utils_fast.cpython-310-x86_64-linux-gnu.so -> fairseq/data\n",
            "copying build/lib.linux-x86_64-cpython-310/fairseq/libbase.cpython-310-x86_64-linux-gnu.so -> fairseq\n",
            "copying build/lib.linux-x86_64-cpython-310/fairseq/libnat.cpython-310-x86_64-linux-gnu.so -> fairseq\n",
            "copying build/lib.linux-x86_64-cpython-310/alignment_train_cpu_binding.cpython-310-x86_64-linux-gnu.so -> \n",
            "Creating /usr/local/lib/python3.10/dist-packages/fairseq.egg-link (link to .)\n",
            "Adding fairseq 0.12.2 to easy-install.pth file\n",
            "Installing fairseq-eval-lm script to /usr/local/bin\n",
            "Installing fairseq-generate script to /usr/local/bin\n",
            "Installing fairseq-hydra-train script to /usr/local/bin\n",
            "Installing fairseq-interactive script to /usr/local/bin\n",
            "Installing fairseq-preprocess script to /usr/local/bin\n",
            "Installing fairseq-score script to /usr/local/bin\n",
            "Installing fairseq-train script to /usr/local/bin\n",
            "Installing fairseq-validate script to /usr/local/bin\n",
            "\n",
            "Installed /content/fairseq\n",
            "Processing dependencies for fairseq==0.12.2\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/_vendor/packaging/requirements.py\", line 35, in __init__\n",
            "    parsed = parse_requirement(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/_vendor/packaging/_parser.py\", line 64, in parse_requirement\n",
            "    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/_vendor/packaging/_parser.py\", line 82, in _parse_requirement\n",
            "    url, specifier, marker = _parse_requirement_details(tokenizer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/_vendor/packaging/_parser.py\", line 120, in _parse_requirement_details\n",
            "    specifier = _parse_specifier(tokenizer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/_vendor/packaging/_parser.py\", line 206, in _parse_specifier\n",
            "    with tokenizer.enclosing_tokens(\"LEFT_PARENTHESIS\", \"RIGHT_PARENTHESIS\"):\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/_vendor/packaging/_tokenizer.py\", line 183, in enclosing_tokens\n",
            "    self.raise_syntax_error(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/_vendor/packaging/_tokenizer.py\", line 163, in raise_syntax_error\n",
            "    raise ParserSyntaxError(\n",
            "pkg_resources.extern.packaging._tokenizer.ParserSyntaxError: Expected closing RIGHT_PARENTHESIS\n",
            "    PyYAML (>=5.1.*)\n",
            "           ~~~~~~^\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/fairseq/setup.py\", line 254, in <module>\n",
            "    do_setup(package_data)\n",
            "  File \"/content/fairseq/setup.py\", line 164, in do_setup\n",
            "    setup(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/__init__.py\", line 107, in setup\n",
            "    return distutils.core.setup(**attrs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
            "    return run_commands(dist)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
            "    dist.run_commands()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n",
            "    self.run_command(cmd)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/dist.py\", line 1244, in run_command\n",
            "    super().run_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
            "    cmd_obj.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py\", line 34, in run\n",
            "    self.install_for_development()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/develop.py\", line 130, in install_for_development\n",
            "    self.process_distribution(None, self.dist, not self.no_deps)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/setuptools/command/easy_install.py\", line 750, in process_distribution\n",
            "    distros = WorkingSet([]).resolve(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 832, in resolve\n",
            "    new_requirements = dist.requires(req.extras)[::-1]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/_vendor/packaging/requirements.py\", line 37, in __init__\n",
            "    raise InvalidRequirement(str(e)) from e\n",
            "pkg_resources.extern.packaging.requirements.InvalidRequirement: Expected closing RIGHT_PARENTHESIS\n",
            "    PyYAML (>=5.1.*)\n",
            "           ~~~~~~^\n"
          ]
        }
      ],
      "source": [
        "!cd /content/fairseq && python setup.py build develop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5prhJ-gkK0P",
        "outputId": "9e9f194a-af70-4f7e-90fd-de20dc59731b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/SRC_AUDIO.zip\n",
            "   creating: /content/SRC_AUDIO/dev/\n",
            "  inflating: /content/SRC_AUDIO/dev/1510.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1513.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1514.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1515.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1516.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1517.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1518.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1519.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1520.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1521.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1522.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1523.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1524.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1525.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1526.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1527.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1528.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1529.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1530.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1531.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1532.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1533.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1535.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1536.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1538.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1539.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1540.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1541.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1542.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1543.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1544.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1545.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1546.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1547.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1548.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1549.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1550.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1551.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1552.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1553.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1554.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1555.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1557.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1558.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1559.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1560.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1561.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1562.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1563.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1564.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1565.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1566.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1567.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1568.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1569.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1570.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1571.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1572.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1573.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1574.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1575.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1576.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1577.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1578.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1579.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1580.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1581.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1583.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1584.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1585.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1587.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1588.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1589.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1590.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1591.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1592.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1593.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1594.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1595.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1596.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1597.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1598.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1599.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1600.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1601.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1602.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1603.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1604.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1605.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1606.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1607.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1608.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1609.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1610.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1611.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1612.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1613.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1614.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1615.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1616.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1617.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1618.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1619.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1620.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1621.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1622.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1624.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1625.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1626.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1627.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1628.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1630.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1631.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1632.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1633.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1634.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1635.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1636.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1637.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1638.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1639.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1640.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1641.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1642.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1643.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1644.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1645.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1646.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1647.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1648.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1649.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1650.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1651.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1652.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1654.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1655.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1656.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1657.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1658.wav  \n",
            "  inflating: /content/SRC_AUDIO/dev/1659.wav  \n",
            "   creating: /content/SRC_AUDIO/test/\n",
            "  inflating: /content/SRC_AUDIO/test/1660.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1661.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1662.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1663.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1664.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1665.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1666.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1668.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1669.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1670.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1671.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1672.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1674.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1675.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1677.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1678.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1679.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1680.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1681.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1682.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1683.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1685.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1686.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1687.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1688.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1689.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1690.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1691.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1692.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1693.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1694.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1695.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1696.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1697.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1698.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1699.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1700.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1701.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1702.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1703.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1704.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1707.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1708.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1709.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1710.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1711.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1712.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1713.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1714.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1715.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1717.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1721.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1722.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1723.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1724.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1725.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1726.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1727.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1730.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1731.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1732.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1734.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1735.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1736.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1737.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1738.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1739.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1740.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1741.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1742.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1743.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1744.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1745.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1747.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1748.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1751.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1752.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1753.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1754.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1755.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1756.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1757.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1758.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1760.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1761.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1762.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1763.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1764.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1765.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1766.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1768.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1770.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1772.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1773.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1774.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1775.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1776.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1777.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1778.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1779.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1780.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1781.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1782.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1783.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1786.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1787.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1788.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1789.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1790.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1792.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1794.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1796.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1797.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1798.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1799.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1803.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1804.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1805.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1807.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1808.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1809.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1811.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1812.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1813.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1814.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1815.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1816.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1818.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1820.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1821.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1823.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1824.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1825.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1826.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1827.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1828.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1829.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1830.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1831.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1833.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1834.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1835.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1836.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1837.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1838.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1839.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1840.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1841.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1843.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1844.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1845.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1846.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1847.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1848.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1849.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1850.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1851.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1853.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1854.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1855.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1856.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1857.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1858.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1859.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1860.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1861.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1862.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1863.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1864.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1865.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1867.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1868.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1869.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1870.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1871.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1873.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1874.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1875.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1876.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1877.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1878.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1879.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1880.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1881.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1883.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1884.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1885.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1886.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1889.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1891.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1892.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1893.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1894.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1895.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1896.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1897.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1900.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1901.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1902.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1904.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1905.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1906.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1908.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1909.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1910.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1911.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1912.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1913.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1914.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1916.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1917.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1918.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1919.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1920.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1921.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1922.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1923.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1924.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1925.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1927.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1928.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1929.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1930.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1931.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1932.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1934.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1935.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1938.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1939.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1941.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1942.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1943.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1944.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1945.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1946.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1947.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1948.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1949.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1950.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1951.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1952.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1953.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1954.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1955.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1956.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1957.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1958.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1959.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1960.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1961.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1963.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1964.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1966.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1968.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1969.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1970.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1971.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1972.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1973.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1974.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1975.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1976.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1977.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1978.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1979.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1980.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1981.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1982.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1983.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1984.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1985.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1986.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1987.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1990.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1991.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1992.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1993.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1994.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1995.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1997.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1998.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/1999.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/2000.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/2001.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/2002.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/2003.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/2004.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/2005.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/2006.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/2007.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/2008.wav  \n",
            "  inflating: /content/SRC_AUDIO/test/2009.wav  \n",
            "   creating: /content/SRC_AUDIO/train/\n",
            "  inflating: /content/SRC_AUDIO/train/1.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/10.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1000.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1001.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1005.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1006.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1008.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/101.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1012.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1013.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1015.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1016.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1017.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1018.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1019.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/102.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1020.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1023.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1024.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1025.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1026.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1027.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1028.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1029.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1030.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1031.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1032.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1033.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1034.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1035.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1036.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1037.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1038.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1039.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/104.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1040.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1041.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1042.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1044.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1045.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1046.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1048.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1049.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1050.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1051.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1052.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1053.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1055.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1058.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/106.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1060.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1061.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1062.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1063.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1066.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1067.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1068.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1069.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1070.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1071.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1072.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1074.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1075.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1078.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1079.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/108.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1080.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1081.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1082.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1083.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1084.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1085.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1086.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1087.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1089.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/109.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1090.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1091.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1092.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1093.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1094.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1095.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1096.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1097.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1098.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1099.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/11.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/110.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1102.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1103.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1104.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1105.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1106.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1107.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1109.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/111.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1111.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1112.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1113.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1114.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1115.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1116.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1117.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1118.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1119.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1120.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1121.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1122.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1123.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1124.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1125.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1126.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1127.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1128.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1129.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/113.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1130.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1131.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1132.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1134.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1135.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1136.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1138.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/114.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1140.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1141.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1142.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1144.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1145.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1146.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1147.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1149.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/115.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1151.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1152.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1153.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1154.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1155.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1156.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1157.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1159.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/116.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1160.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1161.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1162.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1163.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1164.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1165.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1166.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1167.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1168.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1169.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/117.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1170.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1171.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1173.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1174.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1175.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1176.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1177.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/118.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1180.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1181.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1182.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1183.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1185.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1188.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/119.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1190.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1191.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1192.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1193.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1195.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1196.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1197.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1198.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1199.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/12.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/120.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1200.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1201.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1202.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1203.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1205.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1207.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1208.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1209.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/121.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1210.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1212.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1213.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1214.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1215.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1216.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1217.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1218.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1219.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/122.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1220.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1221.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1222.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1224.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1225.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1226.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1227.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1228.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1229.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1231.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1233.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1234.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1237.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1238.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1239.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/124.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1240.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1242.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1243.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1244.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1245.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1246.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1247.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1248.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/125.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1250.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1251.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1252.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1253.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1255.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1256.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1257.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1258.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1259.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1260.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1261.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1262.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1263.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1266.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1267.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1268.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1269.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1271.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1272.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1273.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1277.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1278.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1279.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/128.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1281.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1282.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1283.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1284.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1285.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1286.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1288.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1289.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/129.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1290.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1291.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1292.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1293.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1294.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1295.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1296.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1297.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1299.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/13.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/130.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1301.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1302.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1303.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1304.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1305.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1306.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1307.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/131.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1310.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1315.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1316.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1317.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1318.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1319.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/132.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1320.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1321.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1323.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1324.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1326.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1327.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1328.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1329.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/133.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1330.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1331.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1332.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1334.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1335.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1336.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1337.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/134.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1340.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1343.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1344.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1345.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1346.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1347.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1348.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1349.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1350.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1351.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1353.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1354.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1355.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1357.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1359.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/136.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1360.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1361.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1362.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1363.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1364.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1365.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1366.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1367.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1368.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/137.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1371.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1372.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1373.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1375.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1378.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/138.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1380.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1381.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1382.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1383.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1384.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1385.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1388.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1389.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/139.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1390.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1392.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1393.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1396.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1398.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1399.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/14.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/140.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1400.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1401.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1402.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1403.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1404.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1405.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1407.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1408.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/141.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1410.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1412.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1413.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1414.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1415.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1417.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1418.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/142.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1420.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1421.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1422.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1423.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1424.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1425.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1426.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1427.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1429.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/143.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1431.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1432.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1433.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1434.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1435.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1436.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1437.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1438.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1439.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/144.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1440.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1441.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1442.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1443.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1444.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1445.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1446.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1447.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1448.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1449.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/145.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1450.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1451.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1452.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1457.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1458.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1459.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/146.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1460.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1461.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1463.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1464.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1465.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1466.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1467.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1469.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/147.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1470.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1471.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1472.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1473.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1476.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1479.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/148.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1480.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1484.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1486.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1487.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1488.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1489.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/149.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1490.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1492.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1494.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1496.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1498.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/15.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/150.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1500.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1501.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1502.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1503.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1504.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1505.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1506.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1507.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1508.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/1509.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/151.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/152.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/153.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/154.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/155.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/156.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/157.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/158.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/159.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/160.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/161.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/162.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/163.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/164.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/165.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/166.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/167.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/168.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/169.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/17.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/171.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/172.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/173.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/174.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/177.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/178.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/179.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/180.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/181.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/182.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/183.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/184.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/186.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/187.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/188.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/189.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/19.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/190.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/191.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/192.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/193.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/194.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/195.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/196.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/197.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/198.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/199.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/2.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/20.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/200.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/201.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/202.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/203.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/204.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/205.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/206.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/207.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/208.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/209.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/21.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/210.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/211.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/212.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/213.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/214.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/215.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/216.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/217.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/218.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/219.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/22.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/220.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/221.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/223.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/224.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/225.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/226.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/227.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/228.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/229.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/23.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/230.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/231.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/232.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/233.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/234.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/235.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/236.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/237.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/238.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/239.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/24.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/240.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/241.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/242.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/243.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/244.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/245.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/246.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/248.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/249.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/250.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/251.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/252.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/253.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/255.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/256.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/257.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/258.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/259.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/26.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/260.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/261.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/262.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/263.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/264.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/265.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/266.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/267.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/268.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/269.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/27.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/270.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/271.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/272.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/273.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/274.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/275.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/276.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/277.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/278.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/279.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/28.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/280.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/281.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/283.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/284.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/285.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/286.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/287.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/288.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/289.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/290.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/291.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/292.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/293.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/294.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/295.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/296.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/297.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/298.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/299.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/3.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/30.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/300.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/301.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/302.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/304.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/305.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/307.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/308.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/309.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/31.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/310.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/311.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/312.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/313.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/314.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/315.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/317.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/319.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/32.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/320.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/321.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/322.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/323.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/324.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/325.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/326.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/327.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/328.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/329.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/33.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/330.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/331.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/332.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/334.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/335.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/336.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/337.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/338.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/339.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/34.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/340.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/342.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/344.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/345.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/347.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/348.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/349.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/35.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/350.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/351.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/353.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/354.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/356.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/357.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/359.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/36.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/362.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/363.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/364.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/367.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/369.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/37.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/370.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/371.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/372.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/373.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/374.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/375.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/376.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/377.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/378.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/379.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/38.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/381.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/382.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/383.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/384.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/385.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/388.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/389.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/39.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/390.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/391.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/393.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/396.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/398.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/399.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/4.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/40.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/400.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/401.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/402.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/403.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/405.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/406.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/408.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/409.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/411.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/412.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/413.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/416.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/417.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/418.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/419.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/42.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/420.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/421.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/422.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/423.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/424.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/425.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/426.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/427.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/428.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/429.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/431.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/432.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/433.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/434.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/435.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/436.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/437.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/438.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/44.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/440.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/441.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/442.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/443.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/444.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/445.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/446.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/447.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/448.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/449.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/45.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/450.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/451.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/452.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/454.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/455.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/456.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/458.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/46.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/462.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/463.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/464.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/465.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/466.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/467.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/468.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/469.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/47.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/470.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/471.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/473.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/475.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/476.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/477.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/478.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/48.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/480.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/482.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/483.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/484.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/489.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/49.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/490.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/492.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/493.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/495.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/497.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/498.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/499.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/5.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/50.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/500.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/502.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/504.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/505.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/506.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/507.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/508.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/509.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/51.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/511.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/514.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/515.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/516.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/517.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/518.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/519.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/52.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/520.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/521.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/522.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/524.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/525.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/526.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/527.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/528.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/529.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/53.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/530.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/531.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/532.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/533.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/535.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/536.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/537.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/538.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/539.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/54.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/540.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/542.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/544.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/545.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/547.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/548.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/55.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/550.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/551.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/555.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/556.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/557.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/559.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/56.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/560.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/561.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/562.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/565.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/566.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/568.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/57.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/570.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/571.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/572.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/573.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/574.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/575.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/576.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/577.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/578.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/58.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/581.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/582.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/583.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/585.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/586.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/587.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/588.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/589.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/59.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/590.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/591.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/592.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/594.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/595.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/596.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/597.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/598.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/6.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/600.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/602.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/603.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/605.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/606.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/607.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/608.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/609.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/61.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/610.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/611.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/612.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/613.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/614.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/617.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/618.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/619.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/62.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/620.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/622.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/623.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/624.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/625.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/627.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/628.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/629.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/63.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/630.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/631.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/632.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/633.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/634.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/635.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/636.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/638.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/639.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/64.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/640.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/641.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/642.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/643.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/646.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/647.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/649.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/65.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/650.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/651.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/652.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/653.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/654.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/655.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/658.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/659.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/66.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/660.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/661.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/663.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/664.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/665.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/667.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/668.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/669.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/67.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/670.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/671.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/672.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/674.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/675.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/676.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/677.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/679.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/680.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/681.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/682.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/683.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/684.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/686.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/688.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/69.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/690.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/691.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/692.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/693.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/694.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/695.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/696.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/697.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/698.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/699.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/7.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/70.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/700.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/701.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/702.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/703.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/704.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/705.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/706.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/707.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/708.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/709.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/71.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/711.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/712.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/714.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/716.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/717.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/718.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/720.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/721.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/722.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/723.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/724.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/726.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/727.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/728.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/73.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/730.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/731.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/732.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/733.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/734.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/735.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/736.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/738.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/739.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/741.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/742.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/743.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/744.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/745.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/747.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/748.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/749.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/75.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/750.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/751.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/752.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/755.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/756.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/757.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/758.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/759.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/760.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/761.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/763.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/764.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/765.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/766.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/767.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/768.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/769.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/77.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/770.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/771.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/772.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/773.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/774.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/775.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/776.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/777.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/778.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/78.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/781.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/783.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/785.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/786.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/787.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/788.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/789.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/79.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/790.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/791.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/792.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/794.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/795.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/796.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/798.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/8.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/80.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/800.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/802.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/803.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/804.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/805.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/806.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/809.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/81.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/810.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/812.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/814.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/815.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/816.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/817.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/818.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/819.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/821.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/822.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/823.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/824.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/825.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/826.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/827.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/828.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/829.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/83.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/830.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/831.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/832.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/833.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/834.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/835.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/837.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/838.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/839.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/84.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/840.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/842.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/843.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/845.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/847.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/849.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/85.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/850.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/851.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/852.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/853.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/854.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/855.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/856.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/857.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/858.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/859.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/86.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/861.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/862.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/863.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/864.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/865.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/866.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/869.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/87.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/870.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/871.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/872.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/873.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/874.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/875.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/876.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/877.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/878.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/879.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/88.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/881.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/883.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/884.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/885.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/886.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/887.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/888.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/89.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/890.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/891.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/892.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/894.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/895.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/896.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/897.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/898.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/9.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/90.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/900.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/901.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/902.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/903.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/905.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/906.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/908.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/909.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/91.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/911.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/913.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/914.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/915.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/916.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/918.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/919.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/92.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/920.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/921.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/923.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/925.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/926.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/927.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/929.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/931.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/932.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/933.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/935.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/936.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/937.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/938.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/939.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/94.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/940.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/941.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/942.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/943.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/944.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/947.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/948.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/95.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/950.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/952.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/953.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/954.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/955.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/956.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/957.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/958.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/959.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/96.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/960.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/961.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/962.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/963.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/965.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/968.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/969.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/970.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/971.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/972.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/973.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/974.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/976.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/977.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/978.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/979.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/98.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/980.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/981.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/982.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/983.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/984.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/985.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/986.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/987.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/99.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/990.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/991.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/992.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/993.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/994.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/995.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/996.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/997.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/998.wav  \n",
            "  inflating: /content/SRC_AUDIO/train/999.wav  \n",
            "Archive:  /content/drive/MyDrive/TGT_AUDIO.zip\n",
            "   creating: /content/TGT_AUDIO/dev/\n",
            "  inflating: /content/TGT_AUDIO/dev/1510.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1513.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1514.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1515.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1516.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1517.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1518.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1519.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1520.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1521.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1522.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1523.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1524.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1525.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1526.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1527.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1528.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1529.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1530.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1531.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1532.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1533.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1535.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1536.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1538.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1539.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1540.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1541.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1542.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1543.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1544.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1545.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1546.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1547.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1548.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1549.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1550.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1551.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1552.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1553.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1554.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1555.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1557.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1558.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1559.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1560.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1561.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1562.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1563.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1564.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1565.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1566.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1567.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1568.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1569.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1570.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1571.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1572.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1573.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1574.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1575.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1576.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1577.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1578.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1579.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1580.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1581.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1583.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1584.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1585.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1587.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1588.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1589.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1590.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1591.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1592.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1593.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1594.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1595.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1596.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1597.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1598.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1599.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1600.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1601.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1602.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1603.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1604.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1605.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1606.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1607.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1608.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1609.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1610.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1611.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1612.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1613.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1614.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1615.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1616.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1617.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1618.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1619.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1620.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1621.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1622.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1624.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1625.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1626.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1627.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1628.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1630.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1631.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1632.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1633.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1634.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1635.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1636.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1637.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1638.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1639.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1640.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1641.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1642.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1643.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1644.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1645.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1646.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1647.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1648.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1649.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1650.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1651.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1652.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1654.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1655.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1656.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1657.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1658.wav  \n",
            "  inflating: /content/TGT_AUDIO/dev/1659.wav  \n",
            "   creating: /content/TGT_AUDIO/test/\n",
            "  inflating: /content/TGT_AUDIO/test/1660.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1661.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1662.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1663.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1664.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1665.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1666.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1668.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1669.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1670.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1671.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1672.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1674.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1675.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1677.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1678.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1679.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1680.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1681.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1682.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1683.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1685.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1686.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1687.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1688.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1689.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1690.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1691.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1692.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1693.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1694.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1695.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1696.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1697.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1698.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1699.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1700.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1701.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1702.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1703.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1704.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1707.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1708.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1709.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1710.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1711.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1712.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1713.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1714.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1715.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1717.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1721.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1722.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1723.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1724.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1725.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1726.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1727.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1730.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1731.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1732.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1734.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1735.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1736.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1737.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1738.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1739.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1740.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1741.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1742.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1743.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1744.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1745.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1747.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1748.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1751.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1752.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1753.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1754.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1755.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1756.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1757.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1758.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1760.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1761.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1762.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1763.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1764.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1765.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1766.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1768.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1770.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1772.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1773.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1774.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1775.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1776.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1777.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1778.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1779.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1780.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1781.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1782.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1783.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1786.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1787.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1788.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1789.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1790.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1792.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1794.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1796.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1797.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1798.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1799.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1803.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1804.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1805.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1807.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1808.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1809.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1811.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1812.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1813.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1814.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1815.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1816.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1818.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1820.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1821.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1823.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1824.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1825.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1826.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1827.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1828.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1829.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1830.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1831.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1833.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1834.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1835.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1836.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1837.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1838.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1839.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1840.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1841.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1843.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1844.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1845.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1846.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1847.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1848.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1849.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1850.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1851.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1853.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1854.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1855.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1856.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1857.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1858.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1859.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1860.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1861.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1862.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1863.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1864.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1865.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1867.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1868.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1869.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1870.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1871.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1873.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1874.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1875.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1876.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1877.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1878.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1879.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1880.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1881.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1883.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1884.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1885.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1886.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1889.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1891.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1892.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1893.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1894.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1895.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1896.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1897.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1900.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1901.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1902.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1904.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1905.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1906.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1908.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1909.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1910.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1911.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1912.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1913.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1914.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1916.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1917.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1918.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1919.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1920.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1921.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1922.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1923.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1924.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1925.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1927.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1928.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1929.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1930.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1931.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1932.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1934.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1935.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1938.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1939.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1941.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1942.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1943.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1944.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1945.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1946.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1947.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1948.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1949.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1950.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1951.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1952.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1953.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1954.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1955.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1956.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1957.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1958.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1959.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1960.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1961.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1963.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1964.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1966.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1968.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1969.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1970.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1971.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1972.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1973.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1974.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1975.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1976.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1977.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1978.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1979.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1980.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1981.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1982.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1983.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1984.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1985.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1986.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1987.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1990.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1991.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1992.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1993.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1994.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1995.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1997.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1998.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/1999.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/2000.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/2001.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/2002.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/2003.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/2004.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/2005.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/2006.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/2007.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/2008.wav  \n",
            "  inflating: /content/TGT_AUDIO/test/2009.wav  \n",
            "   creating: /content/TGT_AUDIO/train/\n",
            "  inflating: /content/TGT_AUDIO/train/1.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/10.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1000.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1001.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1005.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1006.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1008.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/101.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1012.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1013.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1015.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1016.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1017.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1018.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1019.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/102.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1020.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1023.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1024.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1025.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1026.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1027.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1028.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1029.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1030.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1031.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1032.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1033.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1034.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1035.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1036.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1037.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1038.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1039.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/104.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1040.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1041.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1042.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1044.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1045.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1046.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1048.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1049.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1050.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1051.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1052.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1053.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1055.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1058.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/106.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1060.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1061.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1062.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1063.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1066.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1067.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1068.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1069.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1070.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1071.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1072.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1074.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1075.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1078.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1079.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/108.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1080.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1081.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1082.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1083.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1084.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1085.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1086.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1087.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1089.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/109.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1090.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1091.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1092.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1093.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1094.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1095.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1096.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1097.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1098.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1099.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/11.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/110.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1102.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1103.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1104.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1105.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1106.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1107.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1109.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/111.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1111.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1112.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1113.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1114.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1115.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1116.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1117.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1118.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1119.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1120.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1121.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1122.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1123.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1124.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1125.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1126.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1127.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1128.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1129.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/113.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1130.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1131.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1132.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1134.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1135.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1136.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1138.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/114.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1140.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1141.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1142.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1144.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1145.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1146.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1147.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1149.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/115.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1151.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1152.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1153.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1154.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1155.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1156.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1157.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1159.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/116.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1160.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1161.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1162.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1163.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1164.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1165.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1166.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1167.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1168.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1169.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/117.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1170.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1171.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1173.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1174.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1175.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1176.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1177.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/118.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1180.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1181.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1182.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1183.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1185.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1188.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/119.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1190.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1191.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1192.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1193.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1195.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1196.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1197.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1198.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1199.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/12.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/120.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1200.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1201.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1202.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1203.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1205.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1207.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1208.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1209.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/121.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1210.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1212.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1213.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1214.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1215.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1216.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1217.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1218.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1219.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/122.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1220.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1221.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1222.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1224.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1225.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1226.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1227.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1228.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1229.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1231.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1233.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1234.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1237.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1238.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1239.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/124.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1240.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1242.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1243.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1244.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1245.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1246.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1247.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1248.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/125.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1250.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1251.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1252.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1253.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1255.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1256.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1257.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1258.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1259.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1260.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1261.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1262.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1263.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1266.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1267.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1268.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1269.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1271.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1272.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1273.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1277.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1278.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1279.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/128.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1281.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1282.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1283.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1284.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1285.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1286.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1288.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1289.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/129.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1290.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1291.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1292.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1293.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1294.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1295.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1296.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1297.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1299.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/13.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/130.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1301.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1302.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1303.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1304.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1305.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1306.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1307.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/131.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1310.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1315.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1316.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1317.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1318.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1319.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/132.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1320.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1321.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1323.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1324.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1326.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1327.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1328.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1329.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/133.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1330.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1331.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1332.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1334.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1335.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1336.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1337.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/134.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1340.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1343.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1344.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1345.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1346.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1347.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1348.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1349.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1350.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1351.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1353.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1354.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1355.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1357.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1359.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/136.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1360.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1361.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1362.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1363.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1364.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1365.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1366.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1367.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1368.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/137.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1371.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1372.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1373.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1375.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1378.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/138.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1380.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1381.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1382.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1383.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1384.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1385.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1388.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1389.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/139.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1390.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1392.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1393.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1396.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1398.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1399.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/14.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/140.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1400.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1401.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1402.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1403.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1404.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1405.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1407.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1408.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/141.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1410.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1412.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1413.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1414.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1415.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1417.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1418.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/142.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1420.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1421.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1422.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1423.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1424.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1425.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1426.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1427.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1429.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/143.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1431.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1432.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1433.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1434.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1435.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1436.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1437.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1438.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1439.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/144.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1440.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1441.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1442.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1443.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1444.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1445.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1446.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1447.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1448.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1449.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/145.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1450.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1451.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1452.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1457.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1458.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1459.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/146.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1460.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1461.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1463.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1464.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1465.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1466.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1467.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1469.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/147.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1470.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1471.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1472.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1473.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1476.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1479.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/148.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1480.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1484.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1486.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1487.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1488.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1489.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/149.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1490.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1492.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1494.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1496.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1498.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/15.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/150.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1500.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1501.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1502.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1503.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1504.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1505.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1506.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1507.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1508.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/1509.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/151.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/152.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/153.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/154.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/155.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/156.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/157.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/158.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/159.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/160.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/161.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/162.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/163.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/164.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/165.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/166.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/167.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/168.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/169.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/17.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/171.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/172.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/173.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/174.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/177.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/178.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/179.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/180.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/181.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/182.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/183.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/184.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/186.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/187.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/188.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/189.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/19.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/190.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/191.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/192.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/193.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/194.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/195.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/196.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/197.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/198.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/199.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/2.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/20.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/200.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/201.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/202.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/203.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/204.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/205.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/206.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/207.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/208.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/209.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/21.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/210.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/211.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/212.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/213.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/214.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/215.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/216.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/217.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/218.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/219.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/22.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/220.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/221.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/223.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/224.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/225.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/226.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/227.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/228.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/229.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/23.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/230.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/231.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/232.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/233.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/234.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/235.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/236.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/237.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/238.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/239.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/24.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/240.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/241.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/242.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/243.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/244.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/245.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/246.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/248.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/249.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/250.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/251.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/252.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/253.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/255.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/256.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/257.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/258.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/259.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/26.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/260.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/261.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/262.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/263.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/264.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/265.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/266.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/267.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/268.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/269.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/27.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/270.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/271.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/272.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/273.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/274.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/275.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/276.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/277.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/278.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/279.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/28.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/280.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/281.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/283.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/284.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/285.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/286.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/287.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/288.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/289.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/290.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/291.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/292.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/293.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/294.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/295.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/296.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/297.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/298.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/299.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/3.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/30.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/300.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/301.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/302.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/304.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/305.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/307.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/308.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/309.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/31.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/310.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/311.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/312.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/313.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/314.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/315.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/317.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/319.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/32.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/320.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/321.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/322.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/323.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/324.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/325.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/326.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/327.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/328.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/329.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/33.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/330.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/331.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/332.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/334.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/335.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/336.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/337.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/338.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/339.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/34.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/340.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/342.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/344.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/345.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/347.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/348.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/349.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/35.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/350.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/351.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/353.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/354.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/356.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/357.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/359.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/36.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/362.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/363.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/364.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/367.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/369.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/37.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/370.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/371.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/372.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/373.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/374.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/375.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/376.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/377.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/378.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/379.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/38.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/381.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/382.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/383.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/384.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/385.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/388.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/389.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/39.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/390.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/391.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/393.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/396.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/398.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/399.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/4.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/40.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/400.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/401.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/402.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/403.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/405.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/406.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/408.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/409.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/411.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/412.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/413.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/416.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/417.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/418.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/419.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/42.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/420.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/421.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/422.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/423.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/424.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/425.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/426.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/427.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/428.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/429.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/431.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/432.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/433.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/434.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/435.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/436.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/437.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/438.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/44.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/440.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/441.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/442.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/443.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/444.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/445.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/446.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/447.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/448.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/449.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/45.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/450.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/451.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/452.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/454.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/455.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/456.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/458.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/46.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/462.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/463.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/464.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/465.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/466.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/467.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/468.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/469.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/47.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/470.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/471.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/473.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/475.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/476.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/477.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/478.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/48.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/480.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/482.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/483.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/484.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/489.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/49.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/490.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/492.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/493.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/495.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/497.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/498.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/499.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/5.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/50.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/500.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/502.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/504.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/505.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/506.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/507.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/508.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/509.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/51.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/511.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/514.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/515.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/516.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/517.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/518.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/519.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/52.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/520.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/521.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/522.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/524.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/525.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/526.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/527.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/528.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/529.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/53.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/530.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/531.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/532.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/533.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/535.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/536.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/537.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/538.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/539.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/54.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/540.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/542.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/544.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/545.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/547.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/548.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/55.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/550.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/551.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/555.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/556.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/557.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/559.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/56.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/560.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/561.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/562.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/565.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/566.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/568.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/57.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/570.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/571.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/572.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/573.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/574.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/575.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/576.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/577.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/578.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/58.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/581.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/582.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/583.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/585.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/586.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/587.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/588.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/589.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/59.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/590.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/591.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/592.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/594.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/595.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/596.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/597.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/598.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/6.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/600.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/602.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/603.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/605.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/606.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/607.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/608.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/609.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/61.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/610.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/611.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/612.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/613.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/614.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/617.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/618.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/619.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/62.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/620.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/622.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/623.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/624.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/625.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/627.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/628.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/629.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/63.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/630.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/631.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/632.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/633.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/634.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/635.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/636.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/638.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/639.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/64.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/640.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/641.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/642.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/643.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/646.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/647.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/649.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/65.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/650.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/651.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/652.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/653.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/654.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/655.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/658.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/659.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/66.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/660.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/661.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/663.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/664.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/665.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/667.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/668.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/669.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/67.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/670.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/671.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/672.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/674.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/675.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/676.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/677.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/679.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/680.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/681.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/682.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/683.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/684.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/686.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/688.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/69.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/690.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/691.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/692.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/693.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/694.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/695.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/696.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/697.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/698.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/699.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/7.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/70.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/700.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/701.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/702.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/703.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/704.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/705.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/706.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/707.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/708.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/709.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/71.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/711.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/712.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/714.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/716.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/717.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/718.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/720.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/721.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/722.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/723.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/724.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/726.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/727.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/728.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/73.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/730.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/731.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/732.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/733.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/734.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/735.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/736.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/738.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/739.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/741.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/742.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/743.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/744.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/745.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/747.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/748.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/749.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/75.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/750.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/751.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/752.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/755.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/756.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/757.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/758.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/759.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/760.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/761.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/763.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/764.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/765.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/766.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/767.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/768.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/769.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/77.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/770.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/771.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/772.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/773.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/774.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/775.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/776.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/777.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/778.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/78.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/781.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/783.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/785.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/786.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/787.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/788.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/789.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/79.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/790.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/791.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/792.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/794.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/795.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/796.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/798.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/8.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/80.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/800.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/802.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/803.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/804.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/805.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/806.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/809.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/81.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/810.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/812.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/814.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/815.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/816.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/817.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/818.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/819.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/821.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/822.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/823.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/824.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/825.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/826.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/827.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/828.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/829.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/83.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/830.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/831.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/832.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/833.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/834.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/835.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/837.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/838.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/839.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/84.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/840.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/842.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/843.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/845.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/847.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/849.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/85.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/850.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/851.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/852.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/853.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/854.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/855.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/856.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/857.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/858.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/859.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/86.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/861.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/862.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/863.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/864.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/865.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/866.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/869.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/87.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/870.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/871.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/872.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/873.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/874.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/875.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/876.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/877.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/878.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/879.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/88.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/881.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/883.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/884.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/885.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/886.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/887.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/888.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/89.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/890.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/891.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/892.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/894.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/895.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/896.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/897.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/898.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/9.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/90.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/900.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/901.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/902.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/903.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/905.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/906.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/908.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/909.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/91.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/911.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/913.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/914.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/915.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/916.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/918.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/919.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/92.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/920.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/921.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/923.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/925.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/926.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/927.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/929.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/931.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/932.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/933.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/935.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/936.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/937.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/938.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/939.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/94.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/940.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/941.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/942.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/943.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/944.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/947.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/948.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/95.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/950.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/952.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/953.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/954.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/955.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/956.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/957.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/958.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/959.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/96.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/960.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/961.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/962.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/963.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/965.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/968.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/969.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/970.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/971.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/972.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/973.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/974.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/976.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/977.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/978.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/979.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/98.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/980.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/981.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/982.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/983.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/984.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/985.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/986.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/987.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/99.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/990.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/991.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/992.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/993.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/994.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/995.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/996.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/997.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/998.wav  \n",
            "  inflating: /content/TGT_AUDIO/train/999.wav  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/SRC_AUDIO.zip -d /content\n",
        "!unzip /content/drive/MyDrive/TGT_AUDIO.zip -d /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EUIOJ4eZkhoi"
      },
      "outputs": [],
      "source": [
        "# tsv files\n",
        "!cd /content/fairseq && python examples/wav2vec/wav2vec_manifest.py /content/TGT_AUDIO/dev --dest /content/TGT_AUDIO/dev --ext wav --valid-percent 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SEV_MBPekofg"
      },
      "outputs": [],
      "source": [
        "!cd /content/fairseq && python examples/wav2vec/wav2vec_manifest.py /content/TGT_AUDIO/test --dest /content/TGT_AUDIO/test --ext wav --valid-percent 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NZlehtcgkwMp"
      },
      "outputs": [],
      "source": [
        "!cd /content/fairseq && python examples/wav2vec/wav2vec_manifest.py /content/TGT_AUDIO/train --dest /content/TGT_AUDIO/train --ext wav --valid-percent 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VKpynOSak8IO"
      },
      "outputs": [],
      "source": [
        "!cd /content/fairseq && python examples/wav2vec/wav2vec_manifest.py /content/SRC_AUDIO/dev --dest /content/SRC_AUDIO/dev --ext wav --valid-percent 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fh6-Z6Ymk-PJ"
      },
      "outputs": [],
      "source": [
        "!cd /content/fairseq && python examples/wav2vec/wav2vec_manifest.py /content/SRC_AUDIO/train --dest /content/SRC_AUDIO/train --ext wav --valid-percent 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "F11scViAlBEN"
      },
      "outputs": [],
      "source": [
        "!cd /content/fairseq && python examples/wav2vec/wav2vec_manifest.py /content/SRC_AUDIO/test --dest /content/SRC_AUDIO/test --ext wav --valid-percent 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5IFVQf6l6bd"
      },
      "source": [
        "renaming al the train.tsv is also done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFNh9IgYl9Hy",
        "outputId": "2aec8611-7953-483b-a56d-d1f853337adc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-18 20:05:35.648081: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-18 20:05:35.648136: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-18 20:05:35.794892: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-18 20:05:36.093122: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-18 20:05:38.587403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/content/fairseq/fairseq/tasks/multires_hubert_pretraining.py:154: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
            "  dictionaries = [ (Dictionary.load(f\"{label_dir}/dict.{label}.txt\") if label is not \"\" else None ) for label in self.cfg.labels]\n",
            "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg6\n",
            "DEBUG:torio._extension.utils:Failed to load FFmpeg6 extension.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 116, in _find_ffmpeg_extension\n",
            "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
            "    _load_lib(lib)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 94, in _load_lib\n",
            "    torch.ops.load_library(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 933, in load_library\n",
            "    ctypes.CDLL(path)\n",
            "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: libavutil.so.58: cannot open shared object file: No such file or directory\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg5\n",
            "DEBUG:torio._extension.utils:Failed to load FFmpeg5 extension.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 116, in _find_ffmpeg_extension\n",
            "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
            "    _load_lib(lib)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 94, in _load_lib\n",
            "    torch.ops.load_library(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 933, in load_library\n",
            "    ctypes.CDLL(path)\n",
            "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: libavutil.so.57: cannot open shared object file: No such file or directory\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg4\n",
            "DEBUG:torio._extension.utils:Successfully loaded FFmpeg4\n",
            "INFO:__main__:Namespace(feature_type='hubert', acoustic_model_path='/content/drive/MyDrive/hubert_base_ls960.pt', layer=6, kmeans_model_path='/content/drive/MyDrive/km_100.bin', features_path=None, manifest_path='/content/TGT_AUDIO/dev/dev.tsv', out_quantized_file_path='/content/TGT_AUDIO/dev.txt', extension='.wav', channel_id=None, hide_fname=False)\n",
            "INFO:__main__:Extracting hubert acoustic features...\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
            "INFO:fairseq.tasks.hubert_pretraining:current directory is /content/fairseq\n",
            "INFO:fairseq.tasks.hubert_pretraining:HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
            "INFO:fairseq.models.hubert.hubert:HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'conv_pos_batch_norm': False, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "100% 140/140 [00:08<00:00, 16.82it/s]\n",
            "INFO:__main__:Features extracted for 140 utterances.\n",
            "\n",
            "INFO:__main__:Dimensionality of representation = 768\n",
            "INFO:__main__:Loading K-means model from /content/drive/MyDrive/km_100.bin ...\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "Writing quantized predictions to /content/TGT_AUDIO/dev.txt\n"
          ]
        }
      ],
      "source": [
        "! cd /content/fairseq && python examples/textless_nlp/gslm/speech2unit/clustering/quantize_with_kmeans.py --feature_type hubert --kmeans_model_path /content/drive/MyDrive/km_100.bin --acoustic_model_path /content/drive/MyDrive/hubert_base_ls960.pt --layer 6 --manifest_path /content/TGT_AUDIO/dev/dev.tsv --out_quantized_file_path /content/TGT_AUDIO/dev.txt --extension \".wav\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxe2_GnXmSjx",
        "outputId": "3b4a6601-d93d-43c0-91ed-13e121d167ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-18 20:06:45.597011: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-18 20:06:45.597063: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-18 20:06:45.599149: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-18 20:06:45.610468: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-18 20:06:46.805174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg6\n",
            "DEBUG:torio._extension.utils:Failed to load FFmpeg6 extension.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 116, in _find_ffmpeg_extension\n",
            "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
            "    _load_lib(lib)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 94, in _load_lib\n",
            "    torch.ops.load_library(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 933, in load_library\n",
            "    ctypes.CDLL(path)\n",
            "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: libavutil.so.58: cannot open shared object file: No such file or directory\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg5\n",
            "DEBUG:torio._extension.utils:Failed to load FFmpeg5 extension.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 116, in _find_ffmpeg_extension\n",
            "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
            "    _load_lib(lib)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 94, in _load_lib\n",
            "    torch.ops.load_library(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 933, in load_library\n",
            "    ctypes.CDLL(path)\n",
            "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: libavutil.so.57: cannot open shared object file: No such file or directory\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg4\n",
            "DEBUG:torio._extension.utils:Successfully loaded FFmpeg4\n",
            "INFO:__main__:Namespace(feature_type='hubert', acoustic_model_path='/content/drive/MyDrive/hubert_base_ls960.pt', layer=6, kmeans_model_path='/content/drive/MyDrive/km_100.bin', features_path=None, manifest_path='/content/TGT_AUDIO/test/test.tsv', out_quantized_file_path='/content/TGT_AUDIO/test.txt', extension='.wav', channel_id=None, hide_fname=False)\n",
            "INFO:__main__:Extracting hubert acoustic features...\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
            "INFO:fairseq.tasks.hubert_pretraining:current directory is /content/fairseq\n",
            "INFO:fairseq.tasks.hubert_pretraining:HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
            "INFO:fairseq.models.hubert.hubert:HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'conv_pos_batch_norm': False, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "100% 292/292 [00:13<00:00, 21.36it/s]\n",
            "INFO:__main__:Features extracted for 292 utterances.\n",
            "\n",
            "INFO:__main__:Dimensionality of representation = 768\n",
            "INFO:__main__:Loading K-means model from /content/drive/MyDrive/km_100.bin ...\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "Writing quantized predictions to /content/TGT_AUDIO/test.txt\n"
          ]
        }
      ],
      "source": [
        "! cd /content/fairseq && python examples/textless_nlp/gslm/speech2unit/clustering/quantize_with_kmeans.py --feature_type hubert --kmeans_model_path /content/drive/MyDrive/km_100.bin --acoustic_model_path /content/drive/MyDrive/hubert_base_ls960.pt --layer 6 --manifest_path /content/TGT_AUDIO/test/test.tsv --out_quantized_file_path /content/TGT_AUDIO/test.txt --extension \".wav\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEJ72AP7mbkt",
        "outputId": "f5e6c004-752d-450b-e83e-d1b14a11cebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-18 20:07:23.588555: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-18 20:07:23.588608: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-18 20:07:23.590056: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-18 20:07:23.598415: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-18 20:07:24.837138: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg6\n",
            "DEBUG:torio._extension.utils:Failed to load FFmpeg6 extension.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 116, in _find_ffmpeg_extension\n",
            "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
            "    _load_lib(lib)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 94, in _load_lib\n",
            "    torch.ops.load_library(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 933, in load_library\n",
            "    ctypes.CDLL(path)\n",
            "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: libavutil.so.58: cannot open shared object file: No such file or directory\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg5\n",
            "DEBUG:torio._extension.utils:Failed to load FFmpeg5 extension.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 116, in _find_ffmpeg_extension\n",
            "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
            "    _load_lib(lib)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 94, in _load_lib\n",
            "    torch.ops.load_library(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 933, in load_library\n",
            "    ctypes.CDLL(path)\n",
            "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: libavutil.so.57: cannot open shared object file: No such file or directory\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg4\n",
            "DEBUG:torio._extension.utils:Successfully loaded FFmpeg4\n",
            "INFO:__main__:Namespace(feature_type='hubert', acoustic_model_path='/content/drive/MyDrive/hubert_base_ls960.pt', layer=6, kmeans_model_path='/content/drive/MyDrive/km_100.bin', features_path=None, manifest_path='/content/TGT_AUDIO/train/train.tsv', out_quantized_file_path='/content/TGT_AUDIO/train.txt', extension='.wav', channel_id=None, hide_fname=False)\n",
            "INFO:__main__:Extracting hubert acoustic features...\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
            "INFO:fairseq.tasks.hubert_pretraining:current directory is /content/fairseq\n",
            "INFO:fairseq.tasks.hubert_pretraining:HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
            "INFO:fairseq.models.hubert.hubert:HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'conv_pos_batch_norm': False, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "100% 1215/1215 [01:01<00:00, 19.76it/s]\n",
            "INFO:__main__:Features extracted for 1215 utterances.\n",
            "\n",
            "INFO:__main__:Dimensionality of representation = 768\n",
            "INFO:__main__:Loading K-means model from /content/drive/MyDrive/km_100.bin ...\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "Writing quantized predictions to /content/TGT_AUDIO/train.txt\n"
          ]
        }
      ],
      "source": [
        "! cd /content/fairseq && python examples/textless_nlp/gslm/speech2unit/clustering/quantize_with_kmeans.py --feature_type hubert --kmeans_model_path /content/drive/MyDrive/km_100.bin --acoustic_model_path /content/drive/MyDrive/hubert_base_ls960.pt --layer 6 --manifest_path /content/TGT_AUDIO/train/train.tsv --out_quantized_file_path /content/TGT_AUDIO/train.txt --extension \".wav\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#spectrogram starting\n",
        "! cd /content/fairseq && python examples/speech_to_speech/preprocessing/prep_s2spect_data.py --source-dir /content/SRC_AUDIO --target-dir /content/TGT_AUDIO --data-split train dev test --output-root /content/DATA_ROOT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7-B51wn0hvS",
        "outputId": "efbd6125-d750-4c7d-d7a6-13221d8c2cbe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-18 20:16:49.888026: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-18 20:16:49.888088: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-18 20:16:49.890281: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-18 20:16:49.901878: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-18 20:16:51.763708: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
            "Processing train...\n",
            "100% 1215/1215 [00:00<00:00, 8511.94it/s]\n",
            "Processed 1215 samples\n",
            "Processing dev...\n",
            "100% 140/140 [00:00<00:00, 10061.73it/s]\n",
            "Processed 140 samples\n",
            "Processing test...\n",
            "100% 292/292 [00:00<00:00, 10365.42it/s]\n",
            "Processed 292 samples\n",
            "Extracting Mel spectrogram features...\n",
            "100% 1647/1647 [02:43<00:00, 10.07it/s]\n",
            "ZIPing features...\n",
            "100% 1647/1647 [00:02<00:00, 708.74it/s]\n",
            "Fetching ZIP manifest...\n",
            "100% 1647/1647 [00:00<00:00, 3726.58it/s]\n",
            "Generating manifest...\n",
            "Processing train...\n",
            "100% 1215/1215 [00:00<00:00, 1171512.50it/s]\n",
            "Writing manifest to /content/DATA_ROOT/train.tsv...\n",
            "Processing dev...\n",
            "100% 140/140 [00:00<00:00, 855980.41it/s]\n",
            "Writing manifest to /content/DATA_ROOT/dev.tsv...\n",
            "Processing test...\n",
            "100% 292/292 [00:00<00:00, 903198.21it/s]\n",
            "Writing manifest to /content/DATA_ROOT/test.tsv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# things i did before making the above cell work"
      ],
      "metadata": {
        "id": "OG95f4Bi3vks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# basis = librosa.filters.mel(sample_rate, n_fft, n_mels, f_min, f_max)\n",
        "# TypeError: mel() takes 0 positional arguments but 5 were given\n",
        "\n",
        "# to overcome this error imma go to that file name and change a few things in the\n",
        "#/content/fairseq/fairseq/data/audio/audio_utils.py\n",
        "#in line: the change i did: 343 basis = librosa.filters.mel(sr=sample_rate,n_fft= n_fft,n_mels= n_mels, fmin=f_min, fmax=f_max\n",
        "# https://stackoverflow.com/questions/75796284/typeerror-mel-takes-0-positional-arguments-but-5-were-given\n"
      ],
      "metadata": {
        "id": "0oVQXl3k2WgS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing this, since i got an error in the above cell\n",
        "!sudo apt install libsox-dev"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbGxwp-a1RkA",
        "outputId": "82a39bd7-ea46-4b2b-c8f1-5c5af83aa43a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libao-common libao4 libid3tag0 libmad0 libopencore-amrnb0 libopencore-amrwb0\n",
            "  libsox-fmt-all libsox-fmt-alsa libsox-fmt-ao libsox-fmt-base libsox-fmt-mp3\n",
            "  libsox-fmt-oss libsox-fmt-pulse libsox3 libwavpack1\n",
            "Suggested packages:\n",
            "  libaudio2 libsndio6.1\n",
            "The following NEW packages will be installed:\n",
            "  libao-common libao4 libid3tag0 libmad0 libopencore-amrnb0 libopencore-amrwb0\n",
            "  libsox-dev libsox-fmt-all libsox-fmt-alsa libsox-fmt-ao libsox-fmt-base\n",
            "  libsox-fmt-mp3 libsox-fmt-oss libsox-fmt-pulse libsox3 libwavpack1\n",
            "0 upgraded, 16 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,053 kB of archives.\n",
            "After this operation, 4,061 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libao-common all 1.2.2+20180113-1.1ubuntu3 [6,568 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libao4 amd64 1.2.2+20180113-1.1ubuntu3 [35.2 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libid3tag0 amd64 0.15.1b-14 [31.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmad0 amd64 0.15.1b-10ubuntu1 [63.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrnb0 amd64 0.1.5-1 [94.8 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrwb0 amd64 0.1.5-1 [49.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox3 amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [240 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [11.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-ao amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [7,740 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwavpack1 amd64 5.4.0-1build2 [83.7 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-base amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [33.7 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-mp3 amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [17.3 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-oss amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [9,424 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-pulse amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [7,732 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-all amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [5,016 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-dev amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [356 kB]\n",
            "Fetched 1,053 kB in 0s (2,178 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 16.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libao-common.\n",
            "(Reading database ... 121752 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libao-common_1.2.2+20180113-1.1ubuntu3_all.deb ...\n",
            "Unpacking libao-common (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Selecting previously unselected package libao4:amd64.\n",
            "Preparing to unpack .../01-libao4_1.2.2+20180113-1.1ubuntu3_amd64.deb ...\n",
            "Unpacking libao4:amd64 (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Selecting previously unselected package libid3tag0:amd64.\n",
            "Preparing to unpack .../02-libid3tag0_0.15.1b-14_amd64.deb ...\n",
            "Unpacking libid3tag0:amd64 (0.15.1b-14) ...\n",
            "Selecting previously unselected package libmad0:amd64.\n",
            "Preparing to unpack .../03-libmad0_0.15.1b-10ubuntu1_amd64.deb ...\n",
            "Unpacking libmad0:amd64 (0.15.1b-10ubuntu1) ...\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "Preparing to unpack .../04-libopencore-amrnb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../05-libopencore-amrwb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../06-libsox3_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../07-libsox-fmt-alsa_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-ao:amd64.\n",
            "Preparing to unpack .../08-libsox-fmt-ao_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-ao:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libwavpack1:amd64.\n",
            "Preparing to unpack .../09-libwavpack1_5.4.0-1build2_amd64.deb ...\n",
            "Unpacking libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../10-libsox-fmt-base_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-mp3:amd64.\n",
            "Preparing to unpack .../11-libsox-fmt-mp3_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-mp3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-oss:amd64.\n",
            "Preparing to unpack .../12-libsox-fmt-oss_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-oss:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-pulse:amd64.\n",
            "Preparing to unpack .../13-libsox-fmt-pulse_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-pulse:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-all:amd64.\n",
            "Preparing to unpack .../14-libsox-fmt-all_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-all:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-dev:amd64.\n",
            "Preparing to unpack .../15-libsox-dev_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-dev:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-oss:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libao-common (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Setting up libid3tag0:amd64 (0.15.1b-14) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libao4:amd64 (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Setting up libmad0:amd64 (0.15.1b-10ubuntu1) ...\n",
            "Setting up libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-ao:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-mp3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-pulse:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-all:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-dev:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# writing into fodler\n",
        "!cp -r /content/DATA_ROOT /content/drive/MyDrive/attempt1_spectro"
      ],
      "metadata": {
        "id": "pKqHT5_S3HvR"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "!cd /content/fairseq && fairseq-train /content/DATA_ROOT  --config-yaml config.yaml --task speech_to_speech --n-frames-per-step 5 --criterion speech_to_spectrogram --arch s2spect_transformer_fisher --decoder-normalize-before --dropout 0.1 --attention-dropout 0.1 --relu-dropout 0.1 --train-subset train --valid-subset dev --save-dir /content/SAVE_DIR_TRAINING --eval-inference --best-checkpoint-metric mcd_loss --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-7 --warmup-updates 10000 --optimizer adam --adam-betas \"(0.9,0.98)\" --clip-norm 10.0 --weight-decay 1e-6 --max-update 400000 --max-tokens 80000 --max-tokens-valid 30000  --required-batch-size-multiple 1 --max-target-positions 3000 --update-freq 1 --seed 1 --fp16 --num-workers 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0GA3bSx3Hs1",
        "outputId": "6d9a812a-0c82-4840-ed2f-5ed58c446fd7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-18 20:57:03.814744: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-18 20:57:03.814806: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-18 20:57:03.816981: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-18 20:57:03.829158: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-18 20:57:05.535706: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-04-18 20:57:06 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2024-04-18 20:57:08 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-04-18 20:57:11 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 80000, 'batch_size': None, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'dev', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 30000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/SAVE_DIR_TRAINING', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'mcd_loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='speech_to_spectrogram', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', simul_type=None, scoring='bleu', task='speech_to_speech', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=80000, batch_size=None, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='dev', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid='30000', batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='s2spect_transformer_fisher', max_epoch=0, max_update=400000, stop_time_hours=0, clip_norm=10.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/content/SAVE_DIR_TRAINING', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='mcd_loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, conv_version='s2t_transformer', activation_fn='relu', data='/content/DATA_ROOT', config_yaml='config.yaml', multitask_config_yaml=None, max_source_positions=6000, max_target_positions=3000, target_is_code=False, target_code_size=None, n_frames_per_step=5, eval_inference=True, eval_args='{}', eos_prob_threshold=0.5, mcd_normalize_type='targ', vocoder='griffin_lim', spec_bwd_max_iter=8, infer_target_lang='', bce_pos_weight=1.0, use_guided_attention_loss=False, guided_attention_loss_sigma=0.4, ctc_weight=0.0, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=1e-06, use_old_adam=False, fp16_adam_stats=False, warmup_updates=10000, warmup_init_lr=1e-07, pad=1, eos=2, unk=3, decoder_normalize_before=True, dropout=0.1, attention_dropout=0.1, activation_dropout=0.1, no_seed_provided=False, encoder_embed_dim=256, encoder_ffn_embed_dim=2048, encoder_attention_heads=4, prenet_dim=32, encoder_freezing_updates=0, input_channels=1, conv_kernel_sizes='5,5', conv_channels=1024, conv_out_channels=256, encoder_layers=12, encoder_normalize_before=True, no_scale_embedding=False, speaker_embed_dim=256, output_frame_dim=80, prenet_dropout=0.5, prenet_layers=2, postnet_dropout=0.5, postnet_layers=5, postnet_conv_dim=512, postnet_conv_kernel_size=5, decoder_transformer_layers=6, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_attention_heads=4, _name='s2spect_transformer_fisher'), 'task': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='speech_to_spectrogram', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', simul_type=None, scoring='bleu', task='speech_to_speech', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=80000, batch_size=None, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='dev', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid='30000', batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='s2spect_transformer_fisher', max_epoch=0, max_update=400000, stop_time_hours=0, clip_norm=10.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/content/SAVE_DIR_TRAINING', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='mcd_loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, conv_version='s2t_transformer', activation_fn='relu', data='/content/DATA_ROOT', config_yaml='config.yaml', multitask_config_yaml=None, max_source_positions=6000, max_target_positions=3000, target_is_code=False, target_code_size=None, n_frames_per_step=5, eval_inference=True, eval_args='{}', eos_prob_threshold=0.5, mcd_normalize_type='targ', vocoder='griffin_lim', spec_bwd_max_iter=8, infer_target_lang='', bce_pos_weight=1.0, use_guided_attention_loss=False, guided_attention_loss_sigma=0.4, ctc_weight=0.0, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=1e-06, use_old_adam=False, fp16_adam_stats=False, warmup_updates=10000, warmup_init_lr=1e-07, pad=1, eos=2, unk=3, decoder_normalize_before=True, dropout=0.1, attention_dropout=0.1, activation_dropout=0.1, no_seed_provided=False, encoder_embed_dim=256, encoder_ffn_embed_dim=2048, encoder_attention_heads=4, prenet_dim=32, encoder_freezing_updates=0, input_channels=1, conv_kernel_sizes='5,5', conv_channels=1024, conv_out_channels=256, encoder_layers=12, encoder_normalize_before=True, no_scale_embedding=False, speaker_embed_dim=256, output_frame_dim=80, prenet_dropout=0.5, prenet_layers=2, postnet_dropout=0.5, postnet_layers=5, postnet_conv_dim=512, postnet_conv_kernel_size=5, decoder_transformer_layers=6, decoder_embed_dim=512, decoder_ffn_embed_dim=2048, decoder_attention_heads=4, _name='speech_to_speech'), 'criterion': {'_name': 'speech_to_spectrogram', 'bce_pos_weight': 1.0, 'use_guided_attention_loss': False, 'guided_attention_loss_sigma': 0.4, 'ctc_weight': 0.0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 1e-06, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 10000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/_vendor/packaging/requirements.py\", line 35, in __init__\n",
            "    parsed = parse_requirement(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/_vendor/packaging/_parser.py\", line 64, in parse_requirement\n",
            "    return _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/_vendor/packaging/_parser.py\", line 82, in _parse_requirement\n",
            "    url, specifier, marker = _parse_requirement_details(tokenizer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/_vendor/packaging/_parser.py\", line 120, in _parse_requirement_details\n",
            "    specifier = _parse_specifier(tokenizer)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/_vendor/packaging/_parser.py\", line 206, in _parse_specifier\n",
            "    with tokenizer.enclosing_tokens(\"LEFT_PARENTHESIS\", \"RIGHT_PARENTHESIS\"):\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 142, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/_vendor/packaging/_tokenizer.py\", line 183, in enclosing_tokens\n",
            "    self.raise_syntax_error(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/_vendor/packaging/_tokenizer.py\", line 163, in raise_syntax_error\n",
            "    raise ParserSyntaxError(\n",
            "pkg_resources.extern.packaging._tokenizer.ParserSyntaxError: Expected closing RIGHT_PARENTHESIS\n",
            "    PyYAML (>=5.1.*)\n",
            "           ~~~~~~^\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/fairseq-train\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-train')())\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 574, in cli_main\n",
            "    distributed_utils.call_main(cfg, main)\n",
            "  File \"/content/fairseq/fairseq/distributed/utils.py\", line 404, in call_main\n",
            "    main(cfg, **kwargs)\n",
            "  File \"/content/fairseq/fairseq_cli/train.py\", line 96, in main\n",
            "    model = task.build_model(cfg.model)\n",
            "  File \"/content/fairseq/fairseq/tasks/speech_to_speech.py\", line 353, in build_model\n",
            "    self.generator = self.build_generator(\n",
            "  File \"/content/fairseq/fairseq/tasks/speech_to_speech.py\", line 400, in build_generator\n",
            "    self.vocoder = get_vocoder(self.args, self.data_cfg)\n",
            "  File \"/content/fairseq/fairseq/models/text_to_speech/vocoder.py\", line 299, in get_vocoder\n",
            "    return GriffinLimVocoder.from_data_cfg(args, data_cfg)\n",
            "  File \"/content/fairseq/fairseq/models/text_to_speech/vocoder.py\", line 173, in from_data_cfg\n",
            "    return cls(\n",
            "  File \"/content/fairseq/fairseq/models/text_to_speech/vocoder.py\", line 136, in __init__\n",
            "    self.inv_mel_transform = PseudoInverseMelScale(\n",
            "  File \"/content/fairseq/fairseq/models/text_to_speech/vocoder.py\", line 34, in __init__\n",
            "    basis = get_mel_filters(sample_rate, (n_stft - 1) * 2, n_mels, f_min, f_max)\n",
            "  File \"/content/fairseq/fairseq/data/audio/audio_utils.py\", line 343, in get_mel_filters\n",
            "    basis = librosa.filters.mel(sr=sample_rate,n_fft= n_fft,n_mels= n_mels, fmin=f_min, fmax=f_max)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lazy_loader/__init__.py\", line 79, in __getattr__\n",
            "    return importlib.import_module(f\"{package_name}.{name}\")\n",
            "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/librosa/filters.py\", line 59, in <module>\n",
            "    from .core.convert import note_to_hz, hz_to_midi, midi_to_hz, hz_to_octs\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/librosa/core/convert.py\", line 7, in <module>\n",
            "    from . import notation\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/librosa/core/notation.py\", line 8, in <module>\n",
            "    from .intervals import INTERVALS\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/librosa/core/intervals.py\", line 8, in <module>\n",
            "    from pkg_resources import resource_filename\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 3327, in <module>\n",
            "    def _initialize_master_working_set():\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 3301, in _call_aside\n",
            "    f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 3339, in _initialize_master_working_set\n",
            "    working_set = WorkingSet._build_master()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 629, in _build_master\n",
            "    ws.require(__requires__)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 966, in require\n",
            "    needed = self.resolve(parse_requirements(requirements))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 832, in resolve\n",
            "    new_requirements = dist.requires(req.extras)[::-1]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 3120, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\", line 3173, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pkg_resources/_vendor/packaging/requirements.py\", line 37, in __init__\n",
            "    raise InvalidRequirement(str(e)) from e\n",
            "pkg_resources.extern.packaging.requirements.InvalidRequirement: Expected closing RIGHT_PARENTHESIS\n",
            "    PyYAML (>=5.1.*)\n",
            "           ~~~~~~^\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pipx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnzQR8qrAEJk",
        "outputId": "01b872a9-44e8-4ac3-9543-f31c3c906899"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pipx\n",
            "  Downloading pipx-1.5.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting argcomplete>=1.9.4 (from pipx)\n",
            "  Downloading argcomplete-3.3.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20 in /usr/local/lib/python3.10/dist-packages (from pipx) (24.0)\n",
            "Requirement already satisfied: platformdirs>=2.1 in /usr/local/lib/python3.10/dist-packages (from pipx) (4.2.0)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from pipx) (2.0.1)\n",
            "Collecting userpath!=1.9.0,>=1.6 (from pipx)\n",
            "  Downloading userpath-1.9.2-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from userpath!=1.9.0,>=1.6->pipx) (8.1.7)\n",
            "Installing collected packages: userpath, argcomplete, pipx\n",
            "Successfully installed argcomplete-3.3.0 pipx-1.5.0 userpath-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install packaging==20.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "5PjoFzeIAGlj",
        "outputId": "b22220a6-46e7-4243-96f4-a858a2fc3229"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting packaging==20.0\n",
            "  Downloading packaging-20.0-py2.py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging==20.0) (3.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from packaging==20.0) (1.16.0)\n",
            "Installing collected packages: packaging\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "huggingface-hub 0.20.3 requires packaging>=20.9, but you have packaging 20.0 which is incompatible.\n",
            "statsmodels 0.14.1 requires packaging>=21.3, but you have packaging 20.0 which is incompatible.\n",
            "xarray 2023.7.0 requires packaging>=21.3, but you have packaging 20.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed packaging-20.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              },
              "id": "3ad8dd32c8904208b9747262299988e0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mljXC3xJAU7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to handle the above error\n",
        "! pip install 'testpath<0.4'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XplnM3023HqJ",
        "outputId": "d580cb96-4d12-41d9-b104-3d9778b548d2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting testpath<0.4\n",
            "  Downloading testpath-0.3.1-py2.py3-none-any.whl (161 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.3/161.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: testpath\n",
            "Successfully installed testpath-0.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show pyyaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeCyEL193Hnn",
        "outputId": "dfbf8e12-d3c2-4253-8b02-85eb8168d5f3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: PyYAML\n",
            "Version: 6.0.1\n",
            "Summary: YAML parser and emitter for Python\n",
            "Home-page: https://pyyaml.org/\n",
            "Author: Kirill Simonov\n",
            "Author-email: xi@resolvent.net\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: \n",
            "Required-by: albumentations, astropy, bokeh, dask, distributed, fastai, flax, huggingface-hub, omegaconf, orbax-checkpoint, PyDrive, PyDrive2, transformers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyYAML==6.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-FmYUar71uB",
        "outputId": "b2c715be-cbe3-409f-8285-2855087c80a6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyYAML==6.0\n",
            "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyYAML\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "Successfully installed PyYAML-6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "this seems to be an issue with the pyyaml and omegaconf\n",
        "https://github.com/omry/omegaconf/issues/758"
      ],
      "metadata": {
        "id": "Kw1q2nss8ZpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ok the pyyaml thing doesnt work, let's try omegaconf updation\n",
        "! pip show omegaconf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nMaVMel3HlB",
        "outputId": "f23a54cc-7195-4bb7-a571-a021ef2950f8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: omegaconf\n",
            "Version: 2.2.0\n",
            "Summary: A flexible configuration library\n",
            "Home-page: https://github.com/omry/omegaconf\n",
            "Author: Omry Yadan\n",
            "Author-email: omry@yadan.net\n",
            "License: UNKNOWN\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: antlr4-python3-runtime, PyYAML\n",
            "Required-by: fairseq, hydra-core\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install omegaconf==2.3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 602
        },
        "id": "s3M09bi53HgT",
        "outputId": "5ed27359-68b1-4a4f-9f88-29c5a90a7dd4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting omegaconf==2.3.0\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from omegaconf==2.3.0)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.3.0) (6.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=36d10d1de83b2266def72ac1747665e8929261695906ffe8996f1dfe318cc0f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf\n",
            "  Attempting uninstall: antlr4-python3-runtime\n",
            "    Found existing installation: antlr4-python3-runtime 4.8\n",
            "    Uninstalling antlr4-python3-runtime-4.8:\n",
            "      Successfully uninstalled antlr4-python3-runtime-4.8\n",
            "  Attempting uninstall: omegaconf\n",
            "    Found existing installation: omegaconf 2.0.6\n",
            "    Uninstalling omegaconf-2.0.6:\n",
            "      Successfully uninstalled omegaconf-2.0.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fairseq 0.12.2 requires omegaconf<2.1, but you have omegaconf 2.3.0 which is incompatible.\n",
            "hydra-core 1.0.7 requires antlr4-python3-runtime==4.8, but you have antlr4-python3-runtime 4.9.3 which is incompatible.\n",
            "hydra-core 1.0.7 requires omegaconf<2.1,>=2.0.5, but you have omegaconf 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "e781efe393804b56afc7b371450f0465"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install omegaconf==2.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "nPws94W59aF9",
        "outputId": "960dc3f2-a2cc-4b25-c837-b8aa5e92498e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting omegaconf==2.2.0\n",
            "  Downloading omegaconf-2.2.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8 (from omegaconf==2.2.0)\n",
            "  Using cached antlr4_python3_runtime-4.8-py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.2.0) (6.0)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'omegaconf' candidate (version 2.2.0 at https://files.pythonhosted.org/packages/b5/f6/b463d1ddb249ae59a9b1c7c2ef423b60262973da70db781e06aee19d5f8b/omegaconf-2.2.0-py3-none-any.whl (from https://pypi.org/simple/omegaconf/) (requires-python:>=3.6))\n",
            "Reason for being yanked: Accidental release\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: antlr4-python3-runtime, omegaconf\n",
            "  Attempting uninstall: antlr4-python3-runtime\n",
            "    Found existing installation: antlr4-python3-runtime 4.9.3\n",
            "    Uninstalling antlr4-python3-runtime-4.9.3:\n",
            "      Successfully uninstalled antlr4-python3-runtime-4.9.3\n",
            "  Attempting uninstall: omegaconf\n",
            "    Found existing installation: omegaconf 2.3.0\n",
            "    Uninstalling omegaconf-2.3.0:\n",
            "      Successfully uninstalled omegaconf-2.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fairseq 0.12.2 requires omegaconf<2.1, but you have omegaconf 2.2.0 which is incompatible.\n",
            "hydra-core 1.0.7 requires omegaconf<2.1,>=2.0.5, but you have omegaconf 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.8 omegaconf-2.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "e2ec623bcb24486baefab271743b8c5b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install omegaconf==2.0.6 # doing this becoz only 2.0.6 works with fairseq lib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svxX6kg13Hdk",
        "outputId": "84393fc2-6506-46ac-d5d7-75080aaa5f44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting omegaconf==2.0.6\n",
            "  Using cached omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.0.6) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf==2.0.6) (4.11.0)\n",
            "Installing collected packages: omegaconf\n",
            "  Attempting uninstall: omegaconf\n",
            "    Found existing installation: omegaconf 2.1.0\n",
            "    Uninstalling omegaconf-2.1.0:\n",
            "      Successfully uninstalled omegaconf-2.1.0\n",
            "Successfully installed omegaconf-2.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OgT8jizg-Zgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GbKktPes-ZeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUmLTt0kmtOc"
      },
      "outputs": [],
      "source": [
        "# create a new folder DATA_ROOT, let's write into that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ2KZNBtmw-s",
        "outputId": "27e5c211-7b80-4b47-9982-28d77645b2c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-04-18 18:50:02.027546: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-18 18:50:02.027603: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-18 18:50:02.028965: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-18 18:50:02.036632: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-18 18:50:03.123780: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
            "Generating manifest...\n",
            "Processing train\n",
            "100% 1215/1215 [00:00<00:00, 4773.39it/s]\n",
            "Processed 1215 samples\n",
            "Writing manifest to /content/DATA_ROOT/train.tsv...\n",
            "Processing dev\n",
            "100% 140/140 [00:00<00:00, 5233.02it/s]\n",
            "Processed 140 samples\n",
            "Writing manifest to /content/DATA_ROOT/dev.tsv...\n",
            "Processing test\n",
            "100% 292/292 [00:00<00:00, 5470.02it/s]\n",
            "Processed 292 samples\n",
            "Writing manifest to /content/DATA_ROOT/test.tsv...\n"
          ]
        }
      ],
      "source": [
        "!cd /content/fairseq && python examples/speech_to_speech/preprocessing/prep_s2ut_data.py --source-dir /content/SRC_AUDIO --target-dir /content/TGT_AUDIO --data-split train dev test --output-root /content/DATA_ROOT --reduce-unit --vocoder-checkpoint /content/drive/MyDrive/g_00500000 --vocoder-cfg /content/drive/MyDrive/config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxEkB0zQnLNF"
      },
      "outputs": [],
      "source": [
        "! cp -r /content/DATA_ROOT /content/drive/MyDrive/attempt2_kmeans100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwnbT2xVnVdb",
        "outputId": "906f95ac-4c48-41e6-e861-b857cc2fbd3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-04-18 18:50:36.046480: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-18 18:50:36.046536: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-18 18:50:36.047943: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-18 18:50:36.055886: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-18 18:50:37.215680: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2024-04-18 18:50:38 | INFO | numexpr.utils | NumExpr defaulting to 8 threads.\n",
            "2024-04-18 18:50:39 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2024-04-18 18:50:41 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 20000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'dev', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 20000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 400000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/SAVE_DIR_TRAINING', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 20, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='speech_to_unit', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', simul_type=None, scoring='bleu', task='speech_to_speech', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=20000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='dev', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=20000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='s2ut_transformer_fisher', max_epoch=100, max_update=400000, stop_time_hours=0, clip_norm=10.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/content/SAVE_DIR_TRAINING', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=20, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, conv_version='s2t_transformer', activation_fn='relu', data='/content/DATA_ROOT', config_yaml='/content/DATA_ROOT/config.yaml', multitask_config_yaml=None, max_source_positions=6000, max_target_positions=3000, target_is_code=True, target_code_size=100, n_frames_per_step=1, eval_inference=False, eval_args='{}', eos_prob_threshold=0.5, mcd_normalize_type='targ', vocoder='code_hifigan', spec_bwd_max_iter=8, infer_target_lang='', label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, rdrop_alpha=0.0, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=10000, warmup_init_lr=1e-07, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.1, attention_dropout=0.1, activation_dropout=0.1, no_seed_provided=False, encoder_embed_dim=256, encoder_attention_heads=4, encoder_freezing_updates=0, input_channels=1, conv_kernel_sizes='5,5', conv_channels=1024, conv_out_channels=256, encoder_ffn_embed_dim=2048, encoder_layers=12, encoder_normalize_before=True, no_scale_embedding=False, speaker_embed_dim=256, decoder_embed_dim=256, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=True, decoder_learned_pos=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, decoder_layerdrop=0.0, decoder_output_dim=256, decoder_input_dim=256, quant_noise_pq=0, _name='s2ut_transformer_fisher'), 'task': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=True, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='speech_to_unit', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='inverse_sqrt', simul_type=None, scoring='bleu', task='speech_to_speech', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=20000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='dev', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=20000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='s2ut_transformer_fisher', max_epoch=100, max_update=400000, stop_time_hours=0, clip_norm=10.0, sentence_avg=False, update_freq=[1], lr=[0.0005], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, debug_param_names=False, save_dir='/content/SAVE_DIR_TRAINING', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=20, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, conv_version='s2t_transformer', activation_fn='relu', data='/content/DATA_ROOT', config_yaml='/content/DATA_ROOT/config.yaml', multitask_config_yaml=None, max_source_positions=6000, max_target_positions=3000, target_is_code=True, target_code_size=100, n_frames_per_step=1, eval_inference=False, eval_args='{}', eos_prob_threshold=0.5, mcd_normalize_type='targ', vocoder='code_hifigan', spec_bwd_max_iter=8, infer_target_lang='', label_smoothing=0.2, report_accuracy=False, ignore_prefix_size=0, rdrop_alpha=0.0, adam_betas='(0.9,0.98)', adam_eps=1e-08, weight_decay=0.0, use_old_adam=False, fp16_adam_stats=False, warmup_updates=10000, warmup_init_lr=1e-07, pad=1, eos=2, unk=3, share_decoder_input_output_embed=True, dropout=0.1, attention_dropout=0.1, activation_dropout=0.1, no_seed_provided=False, encoder_embed_dim=256, encoder_attention_heads=4, encoder_freezing_updates=0, input_channels=1, conv_kernel_sizes='5,5', conv_channels=1024, conv_out_channels=256, encoder_ffn_embed_dim=2048, encoder_layers=12, encoder_normalize_before=True, no_scale_embedding=False, speaker_embed_dim=256, decoder_embed_dim=256, decoder_ffn_embed_dim=2048, decoder_layers=6, decoder_attention_heads=8, decoder_normalize_before=True, decoder_learned_pos=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, no_token_positional_embeddings=False, adaptive_input=False, decoder_layerdrop=0.0, decoder_output_dim=256, decoder_input_dim=256, quant_noise_pq=0, _name='speech_to_speech'), 'criterion': {'_name': 'speech_to_unit', 'label_smoothing': 0.2, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False, 'rdrop_alpha': 0.0}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 10000, 'warmup_init_lr': 1e-07, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2024-04-18 18:50:41 | INFO | fairseq.tasks.speech_to_speech | dictionary size: 104\n",
            "2024-04-18 18:50:42 | INFO | fairseq_cli.train | S2UTTransformerModel(\n",
            "  (encoder): S2STransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (subsample): Conv1dSubsampler(\n",
            "      (conv_layers): ModuleList(\n",
            "        (0): Conv1d(80, 1024, kernel_size=(5,), stride=(2,), padding=(2,))\n",
            "        (1): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))\n",
            "      )\n",
            "    )\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-11): 12 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerUnitDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): StackedEmbedding(104, 256, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=256, out_features=104, bias=False)\n",
            "  )\n",
            ")\n",
            "2024-04-18 18:50:42 | INFO | fairseq_cli.train | task: SpeechToSpeechTask\n",
            "2024-04-18 18:50:42 | INFO | fairseq_cli.train | model: S2UTTransformerModel\n",
            "2024-04-18 18:50:42 | INFO | fairseq_cli.train | criterion: SpeechToUnitMultitaskTaskCriterion\n",
            "2024-04-18 18:50:42 | INFO | fairseq_cli.train | num. shared model params: 27,002,880 (num. trained: 27,002,880)\n",
            "2024-04-18 18:50:42 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2024-04-18 18:50:42 | WARNING | fairseq.data.audio.data_cfg | Auto converting transforms into feature_transforms, but transforms will be deprecated in the future. Please update this in the config.\n",
            "2024-04-18 18:50:42 | INFO | fairseq.data.audio.speech_to_text_dataset | 'dev' has 0.00% OOV\n",
            "2024-04-18 18:50:42 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToSpeechDataset(split=\"dev\", n_samples=140, prepend_tgt_lang_tag=False, n_frames_per_step=1, shuffle=False, feature_transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "), waveform_transforms=None, dataset_transforms=CompositeAudioDatasetTransform(\n",
            "))\n",
            "2024-04-18 18:50:42 | INFO | fairseq.data.audio.speech_to_speech_dataset | SpeechToSpeechDataset(split=\"dev\", n_samples=140, prepend_tgt_lang_tag=False, n_frames_per_step=1, shuffle=False, feature_transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "), waveform_transforms=None, dataset_transforms=CompositeAudioDatasetTransform(\n",
            "))\n",
            "2024-04-18 18:50:42 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight\n",
            "2024-04-18 18:50:42 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-04-18 18:50:42 | INFO | fairseq.utils | rank   0: capabilities =  7.0  ; total memory = 15.773 GB ; name = Tesla V100-SXM2-16GB                    \n",
            "2024-04-18 18:50:42 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2024-04-18 18:50:42 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2024-04-18 18:50:42 | INFO | fairseq_cli.train | max tokens per device = 20000 and max sentences per device = None\n",
            "2024-04-18 18:50:42 | INFO | fairseq.trainer | Preparing to load checkpoint /content/SAVE_DIR_TRAINING/checkpoint_last.pt\n",
            "2024-04-18 18:50:42 | INFO | fairseq.trainer | No existing checkpoint found /content/SAVE_DIR_TRAINING/checkpoint_last.pt\n",
            "2024-04-18 18:50:42 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2024-04-18 18:50:42 | WARNING | fairseq.data.audio.data_cfg | Auto converting transforms into feature_transforms, but transforms will be deprecated in the future. Please update this in the config.\n",
            "2024-04-18 18:50:42 | INFO | fairseq.data.audio.speech_to_text_dataset | 'train' has 0.00% OOV\n",
            "2024-04-18 18:50:42 | INFO | fairseq.data.audio.speech_to_text_dataset | SpeechToSpeechDataset(split=\"train\", n_samples=1_215, prepend_tgt_lang_tag=False, n_frames_per_step=1, shuffle=False, feature_transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "    SpecAugmentTransform(time_warp_w=0, freq_mask_n=1, freq_mask_f=27, time_mask_n=1, time_mask_t=100, time_mask_p=1.0)\n",
            "), waveform_transforms=None, dataset_transforms=CompositeAudioDatasetTransform(\n",
            "))\n",
            "2024-04-18 18:50:42 | INFO | fairseq.data.audio.speech_to_speech_dataset | SpeechToSpeechDataset(split=\"train\", n_samples=1_215, prepend_tgt_lang_tag=False, n_frames_per_step=1, shuffle=False, feature_transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "    SpecAugmentTransform(time_warp_w=0, freq_mask_n=1, freq_mask_f=27, time_mask_n=1, time_mask_t=100, time_mask_p=1.0)\n",
            "), waveform_transforms=None, dataset_transforms=CompositeAudioDatasetTransform(\n",
            "))\n",
            "2024-04-18 18:50:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:50:42 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2024-04-18 18:50:42 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2024-04-18 18:50:42 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "2024-04-18 18:50:44 | INFO | fairseq_cli.train | begin dry-run validation on \"dev\" subset\n",
            "2024-04-18 18:50:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:50:44 | INFO | fairseq.tasks.fairseq_task | reuse_dataloader = True\n",
            "2024-04-18 18:50:44 | INFO | fairseq.tasks.fairseq_task | rebuild_batches = False\n",
            "2024-04-18 18:50:44 | INFO | fairseq.tasks.fairseq_task | creating new batches for epoch 1\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "2024-04-18 18:50:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 001:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:50:45 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2024-04-18 18:50:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n",
            "2024-04-18 18:50:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
            "epoch 001:   1% 1/111 [00:02<03:42,  2.02s/it]2024-04-18 18:50:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "epoch 001:   3% 3/111 [00:02<01:12,  1.49it/s]2024-04-18 18:50:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
            "epoch 001:  99% 110/111 [00:15<00:00,  8.13it/s, loss=8.367, nll_loss=8.354, ppl=327.22, wps=29929.2, ups=7.94, wpb=3763.4, bsz=11.2, num_updates=100, lr=5.099e-06, gnorm=7.247, clip=28, loss_scale=16, train_wall=12, gb_free=13.5, wall=18]2024-04-18 18:51:01 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:51:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 001 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  4.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  5.72it/s]\u001b[A\n",
            "epoch 001 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.71it/s]\u001b[A\n",
            "epoch 001 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.76it/s]\u001b[A\n",
            "epoch 001 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.10it/s]\u001b[A\n",
            "epoch 001 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.83it/s]\u001b[A\n",
            "epoch 001 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.97it/s]\u001b[A\n",
            "epoch 001 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.47it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:51:03 | INFO | dev | epoch 001 | valid on 'dev' subset | loss 7.014 | nll_loss 6.938 | ppl 122.58 | wps 27567 | wpb 3571.7 | bsz 11.1 | num_updates 108\n",
            "2024-04-18 18:51:03 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2024-04-18 18:51:03 | INFO | train | epoch 001 | loss 8.284 | nll_loss 8.267 | ppl 308.04 | wps 26256.6 | ups 7.03 | wpb 3729.3 | bsz 11 | num_updates 108 | lr 5.49892e-06 | gnorm 6.823 | clip 25.9 | loss_scale 16 | train_wall 13 | gb_free 13.7 | wall 21\n",
            "2024-04-18 18:51:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:51:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 002:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:51:03 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2024-04-18 18:51:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  99% 110/111 [00:15<00:00,  7.35it/s, loss=6.792, nll_loss=6.692, ppl=103.4, wps=23738.6, ups=6.37, wpb=3729.4, bsz=10.9, num_updates=200, lr=1.0098e-05, gnorm=0.882, clip=0, loss_scale=16, train_wall=9, gb_free=13.3, wall=34]2024-04-18 18:51:19 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:51:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 002 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.61it/s]\u001b[A\n",
            "epoch 002 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.01it/s]\u001b[A\n",
            "epoch 002 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.48it/s]\u001b[A\n",
            "epoch 002 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.46it/s]\u001b[A\n",
            "epoch 002 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.73it/s]\u001b[A\n",
            "epoch 002 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.38it/s]\u001b[A\n",
            "epoch 002 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.21it/s]\u001b[A\n",
            "epoch 002 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.19it/s]\u001b[A\n",
            "epoch 002 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.56it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:51:20 | INFO | dev | epoch 002 | valid on 'dev' subset | loss 6.512 | nll_loss 6.381 | ppl 83.32 | wps 27717.2 | wpb 3571.7 | bsz 11.1 | num_updates 219\n",
            "2024-04-18 18:51:20 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2024-04-18 18:51:20 | INFO | train | epoch 002 | loss 6.735 | nll_loss 6.629 | ppl 98.98 | wps 23874.6 | ups 6.42 | wpb 3721.2 | bsz 10.9 | num_updates 219 | lr 1.10478e-05 | gnorm 0.768 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.7 | wall 38\n",
            "2024-04-18 18:51:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:51:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 003:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:51:21 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2024-04-18 18:51:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  99% 110/111 [00:15<00:00,  8.15it/s, loss=6.476, nll_loss=6.333, ppl=80.63, wps=23503.5, ups=6.3, wpb=3728.4, bsz=10.7, num_updates=300, lr=1.5097e-05, gnorm=0.415, clip=0, loss_scale=16, train_wall=9, gb_free=12.8, wall=50]2024-04-18 18:51:36 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:51:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 003 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.83it/s]\u001b[A\n",
            "epoch 003 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.39it/s]\u001b[A\n",
            "epoch 003 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.11it/s]\u001b[A\n",
            "epoch 003 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.66it/s]\u001b[A\n",
            "epoch 003 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.23it/s]\u001b[A\n",
            "epoch 003 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.58it/s]\u001b[A\n",
            "epoch 003 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.99it/s]\u001b[A\n",
            "epoch 003 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.10it/s]\u001b[A\n",
            "epoch 003 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.66it/s]\u001b[A\n",
            "epoch 003 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.62it/s]\u001b[A\n",
            "epoch 003 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.86it/s]\u001b[A\n",
            "epoch 003 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.58it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:51:38 | INFO | dev | epoch 003 | valid on 'dev' subset | loss 6.211 | nll_loss 5.985 | ppl 63.32 | wps 27405.5 | wpb 3571.7 | bsz 11.1 | num_updates 330\n",
            "2024-04-18 18:51:38 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2024-04-18 18:51:38 | INFO | train | epoch 003 | loss 6.422 | nll_loss 6.264 | ppl 76.84 | wps 24071.5 | ups 6.47 | wpb 3721.2 | bsz 10.9 | num_updates 330 | lr 1.65967e-05 | gnorm 0.404 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.6 | wall 56\n",
            "2024-04-18 18:51:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:51:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 004:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:51:38 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2024-04-18 18:51:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  99% 110/111 [00:15<00:00,  7.25it/s, loss=5.996, nll_loss=5.671, ppl=50.93, wps=23752.7, ups=6.57, wpb=3613.7, bsz=10.7, num_updates=400, lr=2.0096e-05, gnorm=0.537, clip=0, loss_scale=16, train_wall=9, gb_free=14, wall=65]2024-04-18 18:51:53 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:51:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 004 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.76it/s]\u001b[A\n",
            "epoch 004 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.31it/s]\u001b[A\n",
            "epoch 004 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.06it/s]\u001b[A\n",
            "epoch 004 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.50it/s]\u001b[A\n",
            "epoch 004 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.09it/s]\u001b[A\n",
            "epoch 004 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.60it/s]\u001b[A\n",
            "epoch 004 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  6.99it/s]\u001b[A\n",
            "epoch 004 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.65it/s]\u001b[A\n",
            "epoch 004 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.45it/s]\u001b[A\n",
            "epoch 004 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.59it/s]\u001b[A\n",
            "epoch 004 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  6.95it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:51:55 | INFO | dev | epoch 004 | valid on 'dev' subset | loss 4.949 | nll_loss 4.143 | ppl 17.67 | wps 26097.8 | wpb 3571.7 | bsz 11.1 | num_updates 441\n",
            "2024-04-18 18:51:55 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2024-04-18 18:51:55 | INFO | train | epoch 004 | loss 5.597 | nll_loss 5.099 | ppl 34.27 | wps 24174.5 | ups 6.5 | wpb 3721.2 | bsz 10.9 | num_updates 441 | lr 2.21456e-05 | gnorm 0.617 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.5 | wall 73\n",
            "2024-04-18 18:51:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:51:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 005:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:51:55 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2024-04-18 18:51:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  98% 109/111 [00:14<00:00,  6.42it/s, loss=5.009, nll_loss=4.248, ppl=19, wps=24168.5, ups=6.56, wpb=3683.4, bsz=10.7, num_updates=500, lr=2.5095e-05, gnorm=0.632, clip=0, loss_scale=16, train_wall=9, gb_free=13.6, wall=80]2024-04-18 18:52:10 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:52:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 005 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.78it/s]\u001b[A\n",
            "epoch 005 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.38it/s]\u001b[A\n",
            "epoch 005 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.18it/s]\u001b[A\n",
            "epoch 005 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.67it/s]\u001b[A\n",
            "epoch 005 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.27it/s]\u001b[A\n",
            "epoch 005 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.67it/s]\u001b[A\n",
            "epoch 005 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.74it/s]\u001b[A\n",
            "epoch 005 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.23it/s]\u001b[A\n",
            "epoch 005 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.05it/s]\u001b[A\n",
            "epoch 005 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.15it/s]\u001b[A\n",
            "epoch 005 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:52:11 | INFO | dev | epoch 005 | valid on 'dev' subset | loss 4.659 | nll_loss 3.683 | ppl 12.85 | wps 27875.9 | wpb 3571.7 | bsz 11.1 | num_updates 552\n",
            "2024-04-18 18:52:11 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2024-04-18 18:52:11 | INFO | train | epoch 005 | loss 4.804 | nll_loss 3.951 | ppl 15.47 | wps 24603.9 | ups 6.61 | wpb 3721.2 | bsz 10.9 | num_updates 552 | lr 2.76945e-05 | gnorm 0.592 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13 | wall 89\n",
            "2024-04-18 18:52:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:52:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 006:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:52:12 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2024-04-18 18:52:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  98% 109/111 [00:14<00:00,  7.67it/s, loss=4.688, nll_loss=3.784, ppl=13.78, wps=24327.2, ups=6.39, wpb=3807.1, bsz=11.5, num_updates=600, lr=3.0094e-05, gnorm=0.577, clip=0, loss_scale=16, train_wall=9, gb_free=12.8, wall=96]2024-04-18 18:52:27 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:52:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 006 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.63it/s]\u001b[A\n",
            "epoch 006 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.22it/s]\u001b[A\n",
            "epoch 006 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.07it/s]\u001b[A\n",
            "epoch 006 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.68it/s]\u001b[A\n",
            "epoch 006 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.24it/s]\u001b[A\n",
            "epoch 006 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.72it/s]\u001b[A\n",
            "epoch 006 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.90it/s]\u001b[A\n",
            "epoch 006 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.39it/s]\u001b[A\n",
            "epoch 006 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.22it/s]\u001b[A\n",
            "epoch 006 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.30it/s]\u001b[A\n",
            "epoch 006 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.74it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:52:28 | INFO | dev | epoch 006 | valid on 'dev' subset | loss 4.59 | nll_loss 3.592 | ppl 12.06 | wps 28211.5 | wpb 3571.7 | bsz 11.1 | num_updates 663\n",
            "2024-04-18 18:52:28 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2024-04-18 18:52:28 | INFO | train | epoch 006 | loss 4.644 | nll_loss 3.724 | ppl 13.22 | wps 24395.1 | ups 6.56 | wpb 3721.2 | bsz 10.9 | num_updates 663 | lr 3.32434e-05 | gnorm 0.576 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.7 | wall 106\n",
            "2024-04-18 18:52:28 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:52:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 007:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:52:28 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2024-04-18 18:52:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007:  99% 110/111 [00:14<00:00,  5.89it/s, loss=4.623, nll_loss=3.696, ppl=12.96, wps=24180.4, ups=6.36, wpb=3801.8, bsz=11.3, num_updates=700, lr=3.5093e-05, gnorm=0.565, clip=0, loss_scale=16, train_wall=9, gb_free=13.4, wall=112]2024-04-18 18:52:43 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:52:43 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 007 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  6.01it/s]\u001b[A\n",
            "epoch 007 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.47it/s]\u001b[A\n",
            "epoch 007 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.31it/s]\u001b[A\n",
            "epoch 007 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.68it/s]\u001b[A\n",
            "epoch 007 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.30it/s]\u001b[A\n",
            "epoch 007 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.45it/s]\u001b[A\n",
            "epoch 007 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.94it/s]\u001b[A\n",
            "epoch 007 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.46it/s]\u001b[A\n",
            "epoch 007 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.30it/s]\u001b[A\n",
            "epoch 007 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.33it/s]\u001b[A\n",
            "epoch 007 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:52:45 | INFO | dev | epoch 007 | valid on 'dev' subset | loss 4.549 | nll_loss 3.538 | ppl 11.61 | wps 28541.6 | wpb 3571.7 | bsz 11.1 | num_updates 774\n",
            "2024-04-18 18:52:45 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2024-04-18 18:52:45 | INFO | train | epoch 007 | loss 4.583 | nll_loss 3.641 | ppl 12.47 | wps 25100.6 | ups 6.75 | wpb 3721.2 | bsz 10.9 | num_updates 774 | lr 3.87923e-05 | gnorm 0.559 | clip 0 | loss_scale 16 | train_wall 9 | gb_free 14 | wall 123\n",
            "2024-04-18 18:52:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:52:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 008:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:52:45 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2024-04-18 18:52:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:  99% 110/111 [00:14<00:00,  9.04it/s, loss=4.57, nll_loss=3.624, ppl=12.33, wps=24704.1, ups=6.91, wpb=3577.6, bsz=10.2, num_updates=800, lr=4.0092e-05, gnorm=0.564, clip=0, loss_scale=16, train_wall=8, gb_free=13.4, wall=126]2024-04-18 18:53:00 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:53:00 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 008 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.55it/s]\u001b[A\n",
            "epoch 008 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.39it/s]\u001b[A\n",
            "epoch 008 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.12it/s]\u001b[A\n",
            "epoch 008 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.75it/s]\u001b[A\n",
            "epoch 008 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  9.01it/s]\u001b[A\n",
            "epoch 008 | valid on 'dev' subset:  67% 8/12 [00:00<00:00,  8.55it/s]\u001b[A\n",
            "epoch 008 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  8.03it/s]\u001b[A\n",
            "epoch 008 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.80it/s]\u001b[A\n",
            "epoch 008 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.74it/s]\u001b[A\n",
            "epoch 008 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  8.15it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:53:01 | INFO | dev | epoch 008 | valid on 'dev' subset | loss 4.522 | nll_loss 3.533 | ppl 11.58 | wps 29830.7 | wpb 3571.7 | bsz 11.1 | num_updates 885\n",
            "2024-04-18 18:53:01 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2024-04-18 18:53:01 | INFO | train | epoch 008 | loss 4.545 | nll_loss 3.593 | ppl 12.07 | wps 25331.9 | ups 6.81 | wpb 3721.2 | bsz 10.9 | num_updates 885 | lr 4.43412e-05 | gnorm 0.547 | clip 0 | loss_scale 16 | train_wall 9 | gb_free 12.1 | wall 139\n",
            "2024-04-18 18:53:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:53:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 009:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:53:01 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2024-04-18 18:53:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009:  99% 110/111 [00:14<00:00,  7.55it/s, loss=4.542, nll_loss=3.588, ppl=12.03, wps=25352.5, ups=6.67, wpb=3803.8, bsz=11.1, num_updates=900, lr=4.5091e-05, gnorm=0.54, clip=0, loss_scale=16, train_wall=9, gb_free=11.5, wall=141]2024-04-18 18:53:16 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:53:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 009 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.93it/s]\u001b[A\n",
            "epoch 009 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.49it/s]\u001b[A\n",
            "epoch 009 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.38it/s]\u001b[A\n",
            "epoch 009 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.82it/s]\u001b[A\n",
            "epoch 009 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.33it/s]\u001b[A\n",
            "epoch 009 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.70it/s]\u001b[A\n",
            "epoch 009 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  8.13it/s]\u001b[A\n",
            "epoch 009 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.66it/s]\u001b[A\n",
            "epoch 009 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.35it/s]\u001b[A\n",
            "epoch 009 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.34it/s]\u001b[A\n",
            "epoch 009 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.76it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:53:18 | INFO | dev | epoch 009 | valid on 'dev' subset | loss 4.509 | nll_loss 3.485 | ppl 11.2 | wps 28617.4 | wpb 3571.7 | bsz 11.1 | num_updates 996\n",
            "2024-04-18 18:53:18 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2024-04-18 18:53:18 | INFO | train | epoch 009 | loss 4.516 | nll_loss 3.554 | ppl 11.74 | wps 25013.6 | ups 6.72 | wpb 3721.2 | bsz 10.9 | num_updates 996 | lr 4.989e-05 | gnorm 0.528 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.9 | wall 156\n",
            "2024-04-18 18:53:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:53:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 010:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:53:18 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2024-04-18 18:53:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010:  98% 109/111 [00:14<00:00,  6.28it/s, loss=4.495, nll_loss=3.528, ppl=11.54, wps=27795.6, ups=7.56, wpb=3675.6, bsz=10.8, num_updates=1100, lr=5.5089e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=9, gb_free=12.5, wall=170]2024-04-18 18:53:33 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:53:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 010 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.96it/s]\u001b[A\n",
            "epoch 010 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.43it/s]\u001b[A\n",
            "epoch 010 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.33it/s]\u001b[A\n",
            "epoch 010 | valid on 'dev' subset:  33% 4/12 [00:00<00:00,  8.03it/s]\u001b[A\n",
            "epoch 010 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  9.16it/s]\u001b[A\n",
            "epoch 010 | valid on 'dev' subset:  67% 8/12 [00:00<00:00,  8.38it/s]\u001b[A\n",
            "epoch 010 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.79it/s]\u001b[A\n",
            "epoch 010 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.49it/s]\u001b[A\n",
            "epoch 010 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.47it/s]\u001b[A\n",
            "epoch 010 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.95it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:53:34 | INFO | dev | epoch 010 | valid on 'dev' subset | loss 4.488 | nll_loss 3.475 | ppl 11.12 | wps 29144.2 | wpb 3571.7 | bsz 11.1 | num_updates 1107\n",
            "2024-04-18 18:53:34 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2024-04-18 18:53:34 | INFO | train | epoch 010 | loss 4.492 | nll_loss 3.524 | ppl 11.5 | wps 24825.8 | ups 6.67 | wpb 3721.2 | bsz 10.9 | num_updates 1107 | lr 5.54389e-05 | gnorm 0.525 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13 | wall 172\n",
            "2024-04-18 18:53:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:53:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 011:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:53:34 | INFO | fairseq.trainer | begin training epoch 11\n",
            "2024-04-18 18:53:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 011:  99% 110/111 [00:15<00:00,  7.56it/s, loss=4.471, nll_loss=3.495, ppl=11.27, wps=24440, ups=6.48, wpb=3773.1, bsz=11.2, num_updates=1200, lr=6.0088e-05, gnorm=0.506, clip=0, loss_scale=16, train_wall=9, gb_free=13.5, wall=185]2024-04-18 18:53:50 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:53:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 011 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  5.48it/s]\u001b[A\n",
            "epoch 011 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.32it/s]\u001b[A\n",
            "epoch 011 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.25it/s]\u001b[A\n",
            "epoch 011 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.86it/s]\u001b[A\n",
            "epoch 011 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.42it/s]\u001b[A\n",
            "epoch 011 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.51it/s]\u001b[A\n",
            "epoch 011 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.96it/s]\u001b[A\n",
            "epoch 011 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.54it/s]\u001b[A\n",
            "epoch 011 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.43it/s]\u001b[A\n",
            "epoch 011 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.64it/s]\u001b[A\n",
            "epoch 011 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  8.19it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:53:51 | INFO | dev | epoch 011 | valid on 'dev' subset | loss 4.479 | nll_loss 3.447 | ppl 10.9 | wps 29596.6 | wpb 3571.7 | bsz 11.1 | num_updates 1218\n",
            "2024-04-18 18:53:51 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2024-04-18 18:53:51 | INFO | train | epoch 011 | loss 4.472 | nll_loss 3.497 | ppl 11.29 | wps 24528.5 | ups 6.59 | wpb 3721.2 | bsz 10.9 | num_updates 1218 | lr 6.09878e-05 | gnorm 0.504 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.8 | wall 189\n",
            "2024-04-18 18:53:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:53:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 012:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:53:51 | INFO | fairseq.trainer | begin training epoch 12\n",
            "2024-04-18 18:53:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 012:  99% 110/111 [00:15<00:00,  8.26it/s, loss=4.461, nll_loss=3.485, ppl=11.2, wps=23754.1, ups=6.45, wpb=3681.4, bsz=10.8, num_updates=1300, lr=6.5087e-05, gnorm=0.5, clip=0, loss_scale=16, train_wall=9, gb_free=13.6, wall=201]2024-04-18 18:54:07 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:54:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 012 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  4.71it/s]\u001b[A\n",
            "epoch 012 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  5.41it/s]\u001b[A\n",
            "epoch 012 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.44it/s]\u001b[A\n",
            "epoch 012 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.10it/s]\u001b[A\n",
            "epoch 012 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.66it/s]\u001b[A\n",
            "epoch 012 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.25it/s]\u001b[A\n",
            "epoch 012 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.63it/s]\u001b[A\n",
            "epoch 012 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.23it/s]\u001b[A\n",
            "epoch 012 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.85it/s]\u001b[A\n",
            "epoch 012 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.92it/s]\u001b[A\n",
            "epoch 012 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.54it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:54:08 | INFO | dev | epoch 012 | valid on 'dev' subset | loss 4.475 | nll_loss 3.461 | ppl 11.01 | wps 27147.6 | wpb 3571.7 | bsz 11.1 | num_updates 1329\n",
            "2024-04-18 18:54:08 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2024-04-18 18:54:08 | INFO | train | epoch 012 | loss 4.457 | nll_loss 3.478 | ppl 11.14 | wps 24089.2 | ups 6.47 | wpb 3721.2 | bsz 10.9 | num_updates 1329 | lr 6.65367e-05 | gnorm 0.497 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.7 | wall 206\n",
            "2024-04-18 18:54:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:54:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 013:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:54:08 | INFO | fairseq.trainer | begin training epoch 13\n",
            "2024-04-18 18:54:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 013:  99% 110/111 [00:14<00:00,  7.43it/s, loss=4.45, nll_loss=3.469, ppl=11.08, wps=24797.3, ups=6.76, wpb=3667.3, bsz=10.7, num_updates=1400, lr=7.0086e-05, gnorm=0.478, clip=0, loss_scale=16, train_wall=8, gb_free=12.7, wall=215]2024-04-18 18:54:23 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:54:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 013 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.93it/s]\u001b[A\n",
            "epoch 013 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.51it/s]\u001b[A\n",
            "epoch 013 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.38it/s]\u001b[A\n",
            "epoch 013 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.89it/s]\u001b[A\n",
            "epoch 013 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.47it/s]\u001b[A\n",
            "epoch 013 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.44it/s]\u001b[A\n",
            "epoch 013 | valid on 'dev' subset:  67% 8/12 [00:00<00:00,  8.18it/s]\u001b[A\n",
            "epoch 013 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.70it/s]\u001b[A\n",
            "epoch 013 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.52it/s]\u001b[A\n",
            "epoch 013 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.60it/s]\u001b[A\n",
            "epoch 013 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  8.10it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:54:25 | INFO | dev | epoch 013 | valid on 'dev' subset | loss 4.47 | nll_loss 3.442 | ppl 10.86 | wps 29545.1 | wpb 3571.7 | bsz 11.1 | num_updates 1440\n",
            "2024-04-18 18:54:25 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2024-04-18 18:54:25 | INFO | train | epoch 013 | loss 4.442 | nll_loss 3.459 | ppl 11 | wps 25299.8 | ups 6.8 | wpb 3721.2 | bsz 10.9 | num_updates 1440 | lr 7.20856e-05 | gnorm 0.468 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.6 | wall 223\n",
            "2024-04-18 18:54:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:54:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 014:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:54:25 | INFO | fairseq.trainer | begin training epoch 14\n",
            "2024-04-18 18:54:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 014:  99% 110/111 [00:14<00:00,  7.69it/s, loss=4.419, nll_loss=3.427, ppl=10.76, wps=24989.4, ups=6.48, wpb=3857.7, bsz=11.7, num_updates=1500, lr=7.5085e-05, gnorm=0.463, clip=0, loss_scale=16, train_wall=9, gb_free=13.1, wall=231]2024-04-18 18:54:39 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:54:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 014 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.55it/s]\u001b[A\n",
            "epoch 014 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.36it/s]\u001b[A\n",
            "epoch 014 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.28it/s]\u001b[A\n",
            "epoch 014 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.89it/s]\u001b[A\n",
            "epoch 014 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.97it/s]\u001b[A\n",
            "epoch 014 | valid on 'dev' subset:  67% 8/12 [00:00<00:00,  8.49it/s]\u001b[A\n",
            "epoch 014 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.80it/s]\u001b[A\n",
            "epoch 014 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.36it/s]\u001b[A\n",
            "epoch 014 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.43it/s]\u001b[A\n",
            "epoch 014 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.95it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:54:41 | INFO | dev | epoch 014 | valid on 'dev' subset | loss 4.451 | nll_loss 3.425 | ppl 10.74 | wps 29194.1 | wpb 3571.7 | bsz 11.1 | num_updates 1551\n",
            "2024-04-18 18:54:41 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2024-04-18 18:54:41 | INFO | train | epoch 014 | loss 4.429 | nll_loss 3.443 | ppl 10.87 | wps 25096 | ups 6.74 | wpb 3721.2 | bsz 10.9 | num_updates 1551 | lr 7.76345e-05 | gnorm 0.497 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 14.2 | wall 239\n",
            "2024-04-18 18:54:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:54:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 015:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:54:41 | INFO | fairseq.trainer | begin training epoch 15\n",
            "2024-04-18 18:54:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 015:  99% 110/111 [00:14<00:00,  9.00it/s, loss=4.441, nll_loss=3.46, ppl=11, wps=24702.4, ups=6.76, wpb=3656.2, bsz=10.3, num_updates=1600, lr=8.0084e-05, gnorm=0.496, clip=0, loss_scale=16, train_wall=9, gb_free=13.8, wall=246]2024-04-18 18:54:56 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:54:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 015 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  6.01it/s]\u001b[A\n",
            "epoch 015 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.54it/s]\u001b[A\n",
            "epoch 015 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.19it/s]\u001b[A\n",
            "epoch 015 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.74it/s]\u001b[A\n",
            "epoch 015 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.16it/s]\u001b[A\n",
            "epoch 015 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.56it/s]\u001b[A\n",
            "epoch 015 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.90it/s]\u001b[A\n",
            "epoch 015 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.51it/s]\u001b[A\n",
            "epoch 015 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.18it/s]\u001b[A\n",
            "epoch 015 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.25it/s]\u001b[A\n",
            "epoch 015 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.76it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:54:57 | INFO | dev | epoch 015 | valid on 'dev' subset | loss 4.448 | nll_loss 3.42 | ppl 10.7 | wps 28163.5 | wpb 3571.7 | bsz 11.1 | num_updates 1662\n",
            "2024-04-18 18:54:57 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2024-04-18 18:54:57 | INFO | train | epoch 015 | loss 4.418 | nll_loss 3.428 | ppl 10.76 | wps 25198.3 | ups 6.77 | wpb 3721.2 | bsz 10.9 | num_updates 1662 | lr 8.31834e-05 | gnorm 0.462 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.4 | wall 255\n",
            "2024-04-18 18:54:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:54:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 016:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:54:58 | INFO | fairseq.trainer | begin training epoch 16\n",
            "2024-04-18 18:54:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 016:  99% 110/111 [00:14<00:00,  7.20it/s, loss=4.409, nll_loss=3.415, ppl=10.67, wps=25064.9, ups=6.71, wpb=3737.9, bsz=11.1, num_updates=1700, lr=8.5083e-05, gnorm=0.452, clip=0, loss_scale=16, train_wall=8, gb_free=12.4, wall=261]2024-04-18 18:55:12 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:55:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 016 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  6.13it/s]\u001b[A\n",
            "epoch 016 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.68it/s]\u001b[A\n",
            "epoch 016 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.54it/s]\u001b[A\n",
            "epoch 016 | valid on 'dev' subset:  33% 4/12 [00:00<00:00,  8.02it/s]\u001b[A\n",
            "epoch 016 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.53it/s]\u001b[A\n",
            "epoch 016 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.61it/s]\u001b[A\n",
            "epoch 016 | valid on 'dev' subset:  67% 8/12 [00:00<00:00,  8.24it/s]\u001b[A\n",
            "epoch 016 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.80it/s]\u001b[A\n",
            "epoch 016 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.53it/s]\u001b[A\n",
            "epoch 016 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.59it/s]\u001b[A\n",
            "epoch 016 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  8.09it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:55:14 | INFO | dev | epoch 016 | valid on 'dev' subset | loss 4.448 | nll_loss 3.422 | ppl 10.72 | wps 29690.9 | wpb 3571.7 | bsz 11.1 | num_updates 1773\n",
            "2024-04-18 18:55:14 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2024-04-18 18:55:14 | INFO | train | epoch 016 | loss 4.408 | nll_loss 3.415 | ppl 10.66 | wps 25226.2 | ups 6.78 | wpb 3721.2 | bsz 10.9 | num_updates 1773 | lr 8.87323e-05 | gnorm 0.448 | clip 0 | loss_scale 16 | train_wall 9 | gb_free 13.3 | wall 272\n",
            "2024-04-18 18:55:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:55:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 017:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:55:14 | INFO | fairseq.trainer | begin training epoch 17\n",
            "2024-04-18 18:55:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 017:  99% 110/111 [00:14<00:00,  8.74it/s, loss=4.404, nll_loss=3.41, ppl=10.63, wps=24864.1, ups=6.57, wpb=3786.9, bsz=11, num_updates=1800, lr=9.0082e-05, gnorm=0.472, clip=0, loss_scale=16, train_wall=9, gb_free=13.2, wall=276]2024-04-18 18:55:29 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:55:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 017 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  6.13it/s]\u001b[A\n",
            "epoch 017 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.64it/s]\u001b[A\n",
            "epoch 017 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.45it/s]\u001b[A\n",
            "epoch 017 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.88it/s]\u001b[A\n",
            "epoch 017 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.27it/s]\u001b[A\n",
            "epoch 017 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.77it/s]\u001b[A\n",
            "epoch 017 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  8.11it/s]\u001b[A\n",
            "epoch 017 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.67it/s]\u001b[A\n",
            "epoch 017 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.33it/s]\u001b[A\n",
            "epoch 017 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.39it/s]\u001b[A\n",
            "epoch 017 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.77it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:55:30 | INFO | dev | epoch 017 | valid on 'dev' subset | loss 4.442 | nll_loss 3.409 | ppl 10.62 | wps 28593.8 | wpb 3571.7 | bsz 11.1 | num_updates 1884\n",
            "2024-04-18 18:55:30 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2024-04-18 18:55:30 | INFO | train | epoch 017 | loss 4.398 | nll_loss 3.402 | ppl 10.57 | wps 25251.2 | ups 6.79 | wpb 3721.2 | bsz 10.9 | num_updates 1884 | lr 9.42812e-05 | gnorm 0.476 | clip 0 | loss_scale 16 | train_wall 9 | gb_free 13.6 | wall 288\n",
            "2024-04-18 18:55:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:55:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 018:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:55:30 | INFO | fairseq.trainer | begin training epoch 18\n",
            "2024-04-18 18:55:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 018:  99% 110/111 [00:14<00:00,  8.09it/s, loss=4.395, nll_loss=3.399, ppl=10.55, wps=24853.9, ups=6.78, wpb=3665.1, bsz=10.9, num_updates=1900, lr=9.5081e-05, gnorm=0.459, clip=0, loss_scale=16, train_wall=9, gb_free=13.4, wall=291]2024-04-18 18:55:45 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:55:45 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 018 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.99it/s]\u001b[A\n",
            "epoch 018 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.46it/s]\u001b[A\n",
            "epoch 018 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.17it/s]\u001b[A\n",
            "epoch 018 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.64it/s]\u001b[A\n",
            "epoch 018 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.15it/s]\u001b[A\n",
            "epoch 018 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.67it/s]\u001b[A\n",
            "epoch 018 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.85it/s]\u001b[A\n",
            "epoch 018 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.42it/s]\u001b[A\n",
            "epoch 018 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.24it/s]\u001b[A\n",
            "epoch 018 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.28it/s]\u001b[A\n",
            "epoch 018 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.77it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:55:47 | INFO | dev | epoch 018 | valid on 'dev' subset | loss 4.437 | nll_loss 3.407 | ppl 10.61 | wps 28148.1 | wpb 3571.7 | bsz 11.1 | num_updates 1995\n",
            "2024-04-18 18:55:47 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2024-04-18 18:55:47 | INFO | train | epoch 018 | loss 4.39 | nll_loss 3.392 | ppl 10.5 | wps 25293.5 | ups 6.8 | wpb 3721.2 | bsz 10.9 | num_updates 1995 | lr 9.98301e-05 | gnorm 0.443 | clip 0 | loss_scale 16 | train_wall 9 | gb_free 12.5 | wall 304\n",
            "2024-04-18 18:55:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:55:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 019:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:55:47 | INFO | fairseq.trainer | begin training epoch 19\n",
            "2024-04-18 18:55:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 019:  99% 110/111 [00:14<00:00,  7.14it/s, loss=4.384, nll_loss=3.384, ppl=10.44, wps=28336.8, ups=7.65, wpb=3703.2, bsz=10.8, num_updates=2100, lr=0.000105079, gnorm=0.441, clip=0, loss_scale=16, train_wall=9, gb_free=12.9, wall=318]2024-04-18 18:56:01 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:56:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 019 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.72it/s]\u001b[A\n",
            "epoch 019 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.30it/s]\u001b[A\n",
            "epoch 019 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.20it/s]\u001b[A\n",
            "epoch 019 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.79it/s]\u001b[A\n",
            "epoch 019 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.37it/s]\u001b[A\n",
            "epoch 019 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.47it/s]\u001b[A\n",
            "epoch 019 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  8.06it/s]\u001b[A\n",
            "epoch 019 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.59it/s]\u001b[A\n",
            "epoch 019 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.40it/s]\u001b[A\n",
            "epoch 019 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.48it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:56:03 | INFO | dev | epoch 019 | valid on 'dev' subset | loss 4.433 | nll_loss 3.407 | ppl 10.61 | wps 29325.8 | wpb 3571.7 | bsz 11.1 | num_updates 2106\n",
            "2024-04-18 18:56:03 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2024-04-18 18:56:03 | INFO | train | epoch 019 | loss 4.382 | nll_loss 3.381 | ppl 10.42 | wps 25301.5 | ups 6.8 | wpb 3721.2 | bsz 10.9 | num_updates 2106 | lr 0.000105379 | gnorm 0.444 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.5 | wall 321\n",
            "2024-04-18 18:56:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:56:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 020:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:56:03 | INFO | fairseq.trainer | begin training epoch 20\n",
            "2024-04-18 18:56:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 020:  99% 110/111 [00:14<00:00,  7.45it/s, loss=4.372, nll_loss=3.367, ppl=10.32, wps=24891.7, ups=6.59, wpb=3779.5, bsz=11.2, num_updates=2200, lr=0.000110078, gnorm=0.49, clip=0, loss_scale=16, train_wall=9, gb_free=12.5, wall=334]2024-04-18 18:56:18 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:56:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 020 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.87it/s]\u001b[A\n",
            "epoch 020 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.48it/s]\u001b[A\n",
            "epoch 020 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.45it/s]\u001b[A\n",
            "epoch 020 | valid on 'dev' subset:  33% 4/12 [00:00<00:00,  8.01it/s]\u001b[A\n",
            "epoch 020 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.47it/s]\u001b[A\n",
            "epoch 020 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.37it/s]\u001b[A\n",
            "epoch 020 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.82it/s]\u001b[A\n",
            "epoch 020 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.38it/s]\u001b[A\n",
            "epoch 020 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.28it/s]\u001b[A\n",
            "epoch 020 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.36it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:56:19 | INFO | dev | epoch 020 | valid on 'dev' subset | loss 4.429 | nll_loss 3.402 | ppl 10.57 | wps 29008.9 | wpb 3571.7 | bsz 11.1 | num_updates 2217\n",
            "2024-04-18 18:56:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 2217 updates\n",
            "2024-04-18 18:56:19 | INFO | fairseq.trainer | Saving checkpoint to /content/SAVE_DIR_TRAINING/checkpoint20.pt\n",
            "2024-04-18 18:56:20 | INFO | fairseq.trainer | Finished saving checkpoint to /content/SAVE_DIR_TRAINING/checkpoint20.pt\n",
            "2024-04-18 18:56:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/SAVE_DIR_TRAINING/checkpoint20.pt (epoch 20 @ 2217 updates, score 4.429) (writing took 1.3552334680000513 seconds)\n",
            "2024-04-18 18:56:21 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2024-04-18 18:56:21 | INFO | train | epoch 020 | loss 4.373 | nll_loss 3.369 | ppl 10.33 | wps 23186.5 | ups 6.23 | wpb 3721.2 | bsz 10.9 | num_updates 2217 | lr 0.000110928 | gnorm 0.491 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 14 | wall 339\n",
            "2024-04-18 18:56:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:56:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 021:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:56:21 | INFO | fairseq.trainer | begin training epoch 21\n",
            "2024-04-18 18:56:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 021:  99% 110/111 [00:14<00:00,  8.37it/s, loss=4.372, nll_loss=3.368, ppl=10.32, wps=22273.5, ups=6.2, wpb=3589.7, bsz=10.4, num_updates=2300, lr=0.000115077, gnorm=0.455, clip=0, loss_scale=16, train_wall=8, gb_free=12.7, wall=350]2024-04-18 18:56:36 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:56:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 021 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  6.06it/s]\u001b[A\n",
            "epoch 021 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.55it/s]\u001b[A\n",
            "epoch 021 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.49it/s]\u001b[A\n",
            "epoch 021 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.99it/s]\u001b[A\n",
            "epoch 021 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  9.20it/s]\u001b[A\n",
            "epoch 021 | valid on 'dev' subset:  67% 8/12 [00:00<00:00,  8.40it/s]\u001b[A\n",
            "epoch 021 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.97it/s]\u001b[A\n",
            "epoch 021 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.75it/s]\u001b[A\n",
            "epoch 021 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.80it/s]\u001b[A\n",
            "epoch 021 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  8.28it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:56:37 | INFO | dev | epoch 021 | valid on 'dev' subset | loss 4.429 | nll_loss 3.389 | ppl 10.48 | wps 29930.9 | wpb 3571.7 | bsz 11.1 | num_updates 2328 | best_loss 4.429\n",
            "2024-04-18 18:56:37 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2024-04-18 18:56:37 | INFO | train | epoch 021 | loss 4.367 | nll_loss 3.361 | ppl 10.27 | wps 24992.4 | ups 6.72 | wpb 3721.2 | bsz 10.9 | num_updates 2328 | lr 0.000116477 | gnorm 0.43 | clip 0 | loss_scale 16 | train_wall 9 | gb_free 12.3 | wall 355\n",
            "2024-04-18 18:56:37 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:56:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 022:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:56:37 | INFO | fairseq.trainer | begin training epoch 22\n",
            "2024-04-18 18:56:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 022:  99% 110/111 [00:14<00:00,  8.48it/s, loss=4.356, nll_loss=3.346, ppl=10.17, wps=25108.4, ups=6.47, wpb=3878.6, bsz=11.6, num_updates=2400, lr=0.000120076, gnorm=0.426, clip=0, loss_scale=16, train_wall=9, gb_free=13.9, wall=365]2024-04-18 18:56:52 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:56:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 022 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.61it/s]\u001b[A\n",
            "epoch 022 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.35it/s]\u001b[A\n",
            "epoch 022 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.18it/s]\u001b[A\n",
            "epoch 022 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.61it/s]\u001b[A\n",
            "epoch 022 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.96it/s]\u001b[A\n",
            "epoch 022 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.45it/s]\u001b[A\n",
            "epoch 022 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.84it/s]\u001b[A\n",
            "epoch 022 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.44it/s]\u001b[A\n",
            "epoch 022 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.26it/s]\u001b[A\n",
            "epoch 022 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.21it/s]\u001b[A\n",
            "epoch 022 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.60it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:56:54 | INFO | dev | epoch 022 | valid on 'dev' subset | loss 4.422 | nll_loss 3.388 | ppl 10.47 | wps 27938.6 | wpb 3571.7 | bsz 11.1 | num_updates 2439 | best_loss 4.422\n",
            "2024-04-18 18:56:54 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2024-04-18 18:56:54 | INFO | train | epoch 022 | loss 4.358 | nll_loss 3.349 | ppl 10.19 | wps 25076.7 | ups 6.74 | wpb 3721.2 | bsz 10.9 | num_updates 2439 | lr 0.000122026 | gnorm 0.455 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.6 | wall 372\n",
            "2024-04-18 18:56:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:56:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 023:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:56:54 | INFO | fairseq.trainer | begin training epoch 23\n",
            "2024-04-18 18:56:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 023:  99% 110/111 [00:14<00:00,  6.90it/s, loss=4.352, nll_loss=3.34, ppl=10.13, wps=24981.8, ups=6.91, wpb=3616.7, bsz=10.6, num_updates=2500, lr=0.000125075, gnorm=0.446, clip=0, loss_scale=16, train_wall=8, gb_free=13.9, wall=380]2024-04-18 18:57:08 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:57:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 023 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.56it/s]\u001b[A\n",
            "epoch 023 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  5.93it/s]\u001b[A\n",
            "epoch 023 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.92it/s]\u001b[A\n",
            "epoch 023 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.21it/s]\u001b[A\n",
            "epoch 023 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.53it/s]\u001b[A\n",
            "epoch 023 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.79it/s]\u001b[A\n",
            "epoch 023 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.39it/s]\u001b[A\n",
            "epoch 023 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.14it/s]\u001b[A\n",
            "epoch 023 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.13it/s]\u001b[A\n",
            "epoch 023 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.53it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:57:10 | INFO | dev | epoch 023 | valid on 'dev' subset | loss 4.413 | nll_loss 3.375 | ppl 10.38 | wps 27479.6 | wpb 3571.7 | bsz 11.1 | num_updates 2550 | best_loss 4.413\n",
            "2024-04-18 18:57:10 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2024-04-18 18:57:10 | INFO | train | epoch 023 | loss 4.349 | nll_loss 3.337 | ppl 10.11 | wps 25067 | ups 6.74 | wpb 3721.2 | bsz 10.9 | num_updates 2550 | lr 0.000127575 | gnorm 0.415 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.5 | wall 388\n",
            "2024-04-18 18:57:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:57:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 024:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:57:10 | INFO | fairseq.trainer | begin training epoch 24\n",
            "2024-04-18 18:57:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 024:  98% 109/111 [00:14<00:00,  7.73it/s, loss=4.342, nll_loss=3.327, ppl=10.04, wps=24520.2, ups=6.55, wpb=3746, bsz=11.2, num_updates=2600, lr=0.000130074, gnorm=0.417, clip=0, loss_scale=16, train_wall=9, gb_free=13.8, wall=395]2024-04-18 18:57:25 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:57:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 024 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  5.15it/s]\u001b[A\n",
            "epoch 024 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.07it/s]\u001b[A\n",
            "epoch 024 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.99it/s]\u001b[A\n",
            "epoch 024 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.36it/s]\u001b[A\n",
            "epoch 024 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.84it/s]\u001b[A\n",
            "epoch 024 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.45it/s]\u001b[A\n",
            "epoch 024 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.91it/s]\u001b[A\n",
            "epoch 024 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.43it/s]\u001b[A\n",
            "epoch 024 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.10it/s]\u001b[A\n",
            "epoch 024 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.23it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:57:27 | INFO | dev | epoch 024 | valid on 'dev' subset | loss 4.416 | nll_loss 3.39 | ppl 10.48 | wps 28234.1 | wpb 3571.7 | bsz 11.1 | num_updates 2661 | best_loss 4.416\n",
            "2024-04-18 18:57:27 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2024-04-18 18:57:27 | INFO | train | epoch 024 | loss 4.341 | nll_loss 3.327 | ppl 10.04 | wps 24599.5 | ups 6.61 | wpb 3721.2 | bsz 10.9 | num_updates 2661 | lr 0.000133123 | gnorm 0.426 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.3 | wall 405\n",
            "2024-04-18 18:57:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:57:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 025:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:57:27 | INFO | fairseq.trainer | begin training epoch 25\n",
            "2024-04-18 18:57:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 025:  98% 109/111 [00:14<00:00,  6.72it/s, loss=4.346, nll_loss=3.333, ppl=10.08, wps=24299, ups=6.43, wpb=3779.4, bsz=10.8, num_updates=2700, lr=0.000135073, gnorm=0.415, clip=0, loss_scale=16, train_wall=9, gb_free=13.4, wall=410]2024-04-18 18:57:42 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:57:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 025 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.72it/s]\u001b[A\n",
            "epoch 025 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.21it/s]\u001b[A\n",
            "epoch 025 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.86it/s]\u001b[A\n",
            "epoch 025 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.43it/s]\u001b[A\n",
            "epoch 025 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.06it/s]\u001b[A\n",
            "epoch 025 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.62it/s]\u001b[A\n",
            "epoch 025 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  8.08it/s]\u001b[A\n",
            "epoch 025 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.60it/s]\u001b[A\n",
            "epoch 025 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.21it/s]\u001b[A\n",
            "epoch 025 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.27it/s]\u001b[A\n",
            "epoch 025 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.81it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:57:44 | INFO | dev | epoch 025 | valid on 'dev' subset | loss 4.41 | nll_loss 3.374 | ppl 10.37 | wps 28200 | wpb 3571.7 | bsz 11.1 | num_updates 2772 | best_loss 4.41\n",
            "2024-04-18 18:57:44 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2024-04-18 18:57:44 | INFO | train | epoch 025 | loss 4.333 | nll_loss 3.315 | ppl 9.95 | wps 24520.7 | ups 6.59 | wpb 3721.2 | bsz 10.9 | num_updates 2772 | lr 0.000138672 | gnorm 0.426 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.4 | wall 422\n",
            "2024-04-18 18:57:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:57:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 026:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:57:44 | INFO | fairseq.trainer | begin training epoch 26\n",
            "2024-04-18 18:57:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 026:  99% 110/111 [00:15<00:00,  8.23it/s, loss=4.329, nll_loss=3.309, ppl=9.91, wps=23970.2, ups=6.53, wpb=3671.6, bsz=10.8, num_updates=2800, lr=0.000140072, gnorm=0.431, clip=0, loss_scale=16, train_wall=9, gb_free=13.7, wall=426]2024-04-18 18:57:59 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:57:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 026 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 026 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.53it/s]\u001b[A\n",
            "epoch 026 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.18it/s]\u001b[A\n",
            "epoch 026 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.98it/s]\u001b[A\n",
            "epoch 026 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.49it/s]\u001b[A\n",
            "epoch 026 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.03it/s]\u001b[A\n",
            "epoch 026 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.44it/s]\u001b[A\n",
            "epoch 026 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.85it/s]\u001b[A\n",
            "epoch 026 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.26it/s]\u001b[A\n",
            "epoch 026 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.05it/s]\u001b[A\n",
            "epoch 026 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.12it/s]\u001b[A\n",
            "epoch 026 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.62it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:58:01 | INFO | dev | epoch 026 | valid on 'dev' subset | loss 4.404 | nll_loss 3.368 | ppl 10.32 | wps 27709.7 | wpb 3571.7 | bsz 11.1 | num_updates 2883 | best_loss 4.404\n",
            "2024-04-18 18:58:01 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
            "2024-04-18 18:58:01 | INFO | train | epoch 026 | loss 4.324 | nll_loss 3.304 | ppl 9.87 | wps 23888.1 | ups 6.42 | wpb 3721.2 | bsz 10.9 | num_updates 2883 | lr 0.000144221 | gnorm 0.418 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.7 | wall 439\n",
            "2024-04-18 18:58:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:58:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 027:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:58:01 | INFO | fairseq.trainer | begin training epoch 27\n",
            "2024-04-18 18:58:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 027:  99% 110/111 [00:15<00:00,  6.99it/s, loss=4.319, nll_loss=3.297, ppl=9.83, wps=23607.9, ups=6.3, wpb=3744.6, bsz=11, num_updates=2900, lr=0.000145071, gnorm=0.419, clip=0, loss_scale=16, train_wall=9, gb_free=12.5, wall=442]2024-04-18 18:58:17 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:58:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 027 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 027 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.72it/s]\u001b[A\n",
            "epoch 027 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.31it/s]\u001b[A\n",
            "epoch 027 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.16it/s]\u001b[A\n",
            "epoch 027 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.77it/s]\u001b[A\n",
            "epoch 027 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.24it/s]\u001b[A\n",
            "epoch 027 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.59it/s]\u001b[A\n",
            "epoch 027 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.73it/s]\u001b[A\n",
            "epoch 027 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.26it/s]\u001b[A\n",
            "epoch 027 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.15it/s]\u001b[A\n",
            "epoch 027 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.22it/s]\u001b[A\n",
            "epoch 027 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.70it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:58:18 | INFO | dev | epoch 027 | valid on 'dev' subset | loss 4.411 | nll_loss 3.365 | ppl 10.3 | wps 27974.6 | wpb 3571.7 | bsz 11.1 | num_updates 2994 | best_loss 4.411\n",
            "2024-04-18 18:58:18 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
            "2024-04-18 18:58:18 | INFO | train | epoch 027 | loss 4.317 | nll_loss 3.293 | ppl 9.8 | wps 24103 | ups 6.48 | wpb 3721.2 | bsz 10.9 | num_updates 2994 | lr 0.00014977 | gnorm 0.431 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 11.5 | wall 456\n",
            "2024-04-18 18:58:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:58:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 028:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:58:18 | INFO | fairseq.trainer | begin training epoch 28\n",
            "2024-04-18 18:58:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 028:  99% 110/111 [00:14<00:00,  6.90it/s, loss=4.307, nll_loss=3.28, ppl=9.71, wps=27409.6, ups=7.46, wpb=3672.3, bsz=10.7, num_updates=3100, lr=0.000155069, gnorm=0.414, clip=0, loss_scale=16, train_wall=9, gb_free=11.8, wall=471]2024-04-18 18:58:33 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:58:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 028 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 028 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.82it/s]\u001b[A\n",
            "epoch 028 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.19it/s]\u001b[A\n",
            "epoch 028 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.97it/s]\u001b[A\n",
            "epoch 028 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.37it/s]\u001b[A\n",
            "epoch 028 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.86it/s]\u001b[A\n",
            "epoch 028 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.31it/s]\u001b[A\n",
            "epoch 028 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.53it/s]\u001b[A\n",
            "epoch 028 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.91it/s]\u001b[A\n",
            "epoch 028 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.58it/s]\u001b[A\n",
            "epoch 028 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.58it/s]\u001b[A\n",
            "epoch 028 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  6.84it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:58:35 | INFO | dev | epoch 028 | valid on 'dev' subset | loss 4.393 | nll_loss 3.354 | ppl 10.23 | wps 25872.2 | wpb 3571.7 | bsz 11.1 | num_updates 3105 | best_loss 4.393\n",
            "2024-04-18 18:58:35 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
            "2024-04-18 18:58:35 | INFO | train | epoch 028 | loss 4.307 | nll_loss 3.28 | ppl 9.71 | wps 24346.2 | ups 6.54 | wpb 3721.2 | bsz 10.9 | num_updates 3105 | lr 0.000155319 | gnorm 0.412 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.9 | wall 473\n",
            "2024-04-18 18:58:35 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:58:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 029:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:58:35 | INFO | fairseq.trainer | begin training epoch 29\n",
            "2024-04-18 18:58:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 029:  99% 110/111 [00:14<00:00,  7.46it/s, loss=4.298, nll_loss=3.267, ppl=9.63, wps=24325.7, ups=6.45, wpb=3771, bsz=11.1, num_updates=3200, lr=0.000160068, gnorm=0.571, clip=1, loss_scale=16, train_wall=9, gb_free=14, wall=486]2024-04-18 18:58:50 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:58:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 029 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 029 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.79it/s]\u001b[A\n",
            "epoch 029 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.31it/s]\u001b[A\n",
            "epoch 029 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.14it/s]\u001b[A\n",
            "epoch 029 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.77it/s]\u001b[A\n",
            "epoch 029 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.22it/s]\u001b[A\n",
            "epoch 029 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.23it/s]\u001b[A\n",
            "epoch 029 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.91it/s]\u001b[A\n",
            "epoch 029 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.31it/s]\u001b[A\n",
            "epoch 029 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.20it/s]\u001b[A\n",
            "epoch 029 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.39it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:58:52 | INFO | dev | epoch 029 | valid on 'dev' subset | loss 4.397 | nll_loss 3.351 | ppl 10.2 | wps 28792.3 | wpb 3571.7 | bsz 11.1 | num_updates 3216 | best_loss 4.397\n",
            "2024-04-18 18:58:52 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
            "2024-04-18 18:58:52 | INFO | train | epoch 029 | loss 4.298 | nll_loss 3.267 | ppl 9.62 | wps 24727.1 | ups 6.64 | wpb 3721.2 | bsz 10.9 | num_updates 3216 | lr 0.000160868 | gnorm 0.559 | clip 0.9 | loss_scale 16 | train_wall 10 | gb_free 13.2 | wall 490\n",
            "2024-04-18 18:58:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:58:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 030:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:58:52 | INFO | fairseq.trainer | begin training epoch 30\n",
            "2024-04-18 18:58:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 030:  99% 110/111 [00:14<00:00,  7.10it/s, loss=4.292, nll_loss=3.259, ppl=9.57, wps=24391.5, ups=6.66, wpb=3662.8, bsz=10.6, num_updates=3300, lr=0.000165067, gnorm=0.433, clip=0, loss_scale=16, train_wall=9, gb_free=13.7, wall=501]2024-04-18 18:59:07 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:59:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 030 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 030 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.97it/s]\u001b[A\n",
            "epoch 030 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.45it/s]\u001b[A\n",
            "epoch 030 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.30it/s]\u001b[A\n",
            "epoch 030 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.70it/s]\u001b[A\n",
            "epoch 030 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.32it/s]\u001b[A\n",
            "epoch 030 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.36it/s]\u001b[A\n",
            "epoch 030 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  8.07it/s]\u001b[A\n",
            "epoch 030 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.57it/s]\u001b[A\n",
            "epoch 030 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.47it/s]\u001b[A\n",
            "epoch 030 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.54it/s]\u001b[A\n",
            "epoch 030 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  8.04it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:59:09 | INFO | dev | epoch 030 | valid on 'dev' subset | loss 4.383 | nll_loss 3.337 | ppl 10.1 | wps 29177.9 | wpb 3571.7 | bsz 11.1 | num_updates 3327 | best_loss 4.383\n",
            "2024-04-18 18:59:09 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
            "2024-04-18 18:59:09 | INFO | train | epoch 030 | loss 4.287 | nll_loss 3.252 | ppl 9.53 | wps 24803.6 | ups 6.67 | wpb 3721.2 | bsz 10.9 | num_updates 3327 | lr 0.000166417 | gnorm 0.436 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.7 | wall 507\n",
            "2024-04-18 18:59:09 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:59:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 031:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:59:09 | INFO | fairseq.trainer | begin training epoch 31\n",
            "2024-04-18 18:59:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 031:  99% 110/111 [00:14<00:00,  7.37it/s, loss=4.278, nll_loss=3.24, ppl=9.45, wps=24713, ups=6.61, wpb=3736.3, bsz=11.2, num_updates=3400, lr=0.000170066, gnorm=0.429, clip=0, loss_scale=16, train_wall=9, gb_free=13.3, wall=516]2024-04-18 18:59:24 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:59:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 031 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 031 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  5.42it/s]\u001b[A\n",
            "epoch 031 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  5.77it/s]\u001b[A\n",
            "epoch 031 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.61it/s]\u001b[A\n",
            "epoch 031 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.32it/s]\u001b[A\n",
            "epoch 031 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.79it/s]\u001b[A\n",
            "epoch 031 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.38it/s]\u001b[A\n",
            "epoch 031 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.84it/s]\u001b[A\n",
            "epoch 031 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.39it/s]\u001b[A\n",
            "epoch 031 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.00it/s]\u001b[A\n",
            "epoch 031 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.93it/s]\u001b[A\n",
            "epoch 031 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.13it/s]\u001b[A\n",
            "epoch 031 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.75it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:59:25 | INFO | dev | epoch 031 | valid on 'dev' subset | loss 4.38 | nll_loss 3.322 | ppl 10 | wps 27656.6 | wpb 3571.7 | bsz 11.1 | num_updates 3438 | best_loss 4.38\n",
            "2024-04-18 18:59:25 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
            "2024-04-18 18:59:25 | INFO | train | epoch 031 | loss 4.276 | nll_loss 3.237 | ppl 9.43 | wps 24688.5 | ups 6.63 | wpb 3721.2 | bsz 10.9 | num_updates 3438 | lr 0.000171966 | gnorm 0.434 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.2 | wall 523\n",
            "2024-04-18 18:59:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:59:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 032:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:59:25 | INFO | fairseq.trainer | begin training epoch 32\n",
            "2024-04-18 18:59:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 032:  99% 110/111 [00:14<00:00,  7.93it/s, loss=4.274, nll_loss=3.234, ppl=9.41, wps=24158.7, ups=6.44, wpb=3752.4, bsz=11.2, num_updates=3500, lr=0.000175065, gnorm=0.436, clip=0, loss_scale=16, train_wall=9, gb_free=13.2, wall=532]2024-04-18 18:59:40 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:59:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 032 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 032 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  6.13it/s]\u001b[A\n",
            "epoch 032 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.41it/s]\u001b[A\n",
            "epoch 032 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.00it/s]\u001b[A\n",
            "epoch 032 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.46it/s]\u001b[A\n",
            "epoch 032 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.94it/s]\u001b[A\n",
            "epoch 032 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.33it/s]\u001b[A\n",
            "epoch 032 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.73it/s]\u001b[A\n",
            "epoch 032 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.15it/s]\u001b[A\n",
            "epoch 032 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.91it/s]\u001b[A\n",
            "epoch 032 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.97it/s]\u001b[A\n",
            "epoch 032 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.47it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:59:42 | INFO | dev | epoch 032 | valid on 'dev' subset | loss 4.378 | nll_loss 3.32 | ppl 9.99 | wps 27090.1 | wpb 3571.7 | bsz 11.1 | num_updates 3549 | best_loss 4.378\n",
            "2024-04-18 18:59:42 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
            "2024-04-18 18:59:42 | INFO | train | epoch 032 | loss 4.267 | nll_loss 3.224 | ppl 9.35 | wps 24726.6 | ups 6.64 | wpb 3721.2 | bsz 10.9 | num_updates 3549 | lr 0.000177515 | gnorm 0.438 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.1 | wall 540\n",
            "2024-04-18 18:59:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:59:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 033:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:59:42 | INFO | fairseq.trainer | begin training epoch 33\n",
            "2024-04-18 18:59:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 033:  99% 110/111 [00:15<00:00,  6.34it/s, loss=4.253, nll_loss=3.206, ppl=9.23, wps=23912.5, ups=6.48, wpb=3689.2, bsz=10.6, num_updates=3600, lr=0.000180064, gnorm=0.449, clip=0, loss_scale=16, train_wall=9, gb_free=14, wall=547]2024-04-18 18:59:57 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 18:59:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 033 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 033 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.80it/s]\u001b[A\n",
            "epoch 033 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.23it/s]\u001b[A\n",
            "epoch 033 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.92it/s]\u001b[A\n",
            "epoch 033 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.44it/s]\u001b[A\n",
            "epoch 033 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.93it/s]\u001b[A\n",
            "epoch 033 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.28it/s]\u001b[A\n",
            "epoch 033 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.47it/s]\u001b[A\n",
            "epoch 033 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.17it/s]\u001b[A\n",
            "epoch 033 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.69it/s]\u001b[A\n",
            "epoch 033 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.67it/s]\u001b[A\n",
            "epoch 033 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.86it/s]\u001b[A\n",
            "epoch 033 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.35it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 18:59:59 | INFO | dev | epoch 033 | valid on 'dev' subset | loss 4.367 | nll_loss 3.32 | ppl 9.99 | wps 26823.1 | wpb 3571.7 | bsz 11.1 | num_updates 3660 | best_loss 4.367\n",
            "2024-04-18 18:59:59 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
            "2024-04-18 18:59:59 | INFO | train | epoch 033 | loss 4.256 | nll_loss 3.21 | ppl 9.25 | wps 23970.6 | ups 6.44 | wpb 3721.2 | bsz 10.9 | num_updates 3660 | lr 0.000183063 | gnorm 0.436 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.1 | wall 557\n",
            "2024-04-18 18:59:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 18:59:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 034:   0% 0/111 [00:00<?, ?it/s]2024-04-18 18:59:59 | INFO | fairseq.trainer | begin training epoch 34\n",
            "2024-04-18 18:59:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 034:  99% 110/111 [00:15<00:00,  8.05it/s, loss=4.258, nll_loss=3.211, ppl=9.26, wps=24206.2, ups=6.42, wpb=3773.3, bsz=11.1, num_updates=3700, lr=0.000185063, gnorm=0.424, clip=0, loss_scale=16, train_wall=9, gb_free=13.6, wall=563]2024-04-18 19:00:14 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:00:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 034 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 034 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.58it/s]\u001b[A\n",
            "epoch 034 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.31it/s]\u001b[A\n",
            "epoch 034 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.31it/s]\u001b[A\n",
            "epoch 034 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.83it/s]\u001b[A\n",
            "epoch 034 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.37it/s]\u001b[A\n",
            "epoch 034 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.85it/s]\u001b[A\n",
            "epoch 034 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  8.05it/s]\u001b[A\n",
            "epoch 034 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.62it/s]\u001b[A\n",
            "epoch 034 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.31it/s]\u001b[A\n",
            "epoch 034 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.51it/s]\u001b[A\n",
            "epoch 034 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.98it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:00:16 | INFO | dev | epoch 034 | valid on 'dev' subset | loss 4.368 | nll_loss 3.308 | ppl 9.91 | wps 29019.1 | wpb 3571.7 | bsz 11.1 | num_updates 3771 | best_loss 4.368\n",
            "2024-04-18 19:00:16 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
            "2024-04-18 19:00:16 | INFO | train | epoch 034 | loss 4.245 | nll_loss 3.193 | ppl 9.15 | wps 24553.5 | ups 6.6 | wpb 3721.2 | bsz 10.9 | num_updates 3771 | lr 0.000188612 | gnorm 0.45 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 14 | wall 574\n",
            "2024-04-18 19:00:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:00:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 035:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:00:16 | INFO | fairseq.trainer | begin training epoch 35\n",
            "2024-04-18 19:00:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 035:  99% 110/111 [00:14<00:00,  7.52it/s, loss=4.237, nll_loss=3.182, ppl=9.08, wps=23956.3, ups=6.56, wpb=3652.6, bsz=10.6, num_updates=3800, lr=0.000190062, gnorm=0.459, clip=0, loss_scale=16, train_wall=9, gb_free=13.4, wall=578]2024-04-18 19:00:31 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:00:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 035 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 035 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.69it/s]\u001b[A\n",
            "epoch 035 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.25it/s]\u001b[A\n",
            "epoch 035 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.18it/s]\u001b[A\n",
            "epoch 035 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.74it/s]\u001b[A\n",
            "epoch 035 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.38it/s]\u001b[A\n",
            "epoch 035 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.37it/s]\u001b[A\n",
            "epoch 035 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.99it/s]\u001b[A\n",
            "epoch 035 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.49it/s]\u001b[A\n",
            "epoch 035 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.33it/s]\u001b[A\n",
            "epoch 035 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.47it/s]\u001b[A\n",
            "epoch 035 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  8.04it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:00:33 | INFO | dev | epoch 035 | valid on 'dev' subset | loss 4.361 | nll_loss 3.305 | ppl 9.88 | wps 29080.5 | wpb 3571.7 | bsz 11.1 | num_updates 3882 | best_loss 4.361\n",
            "2024-04-18 19:00:33 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
            "2024-04-18 19:00:33 | INFO | train | epoch 035 | loss 4.234 | nll_loss 3.178 | ppl 9.05 | wps 24667.3 | ups 6.63 | wpb 3721.2 | bsz 10.9 | num_updates 3882 | lr 0.000194161 | gnorm 0.43 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.4 | wall 591\n",
            "2024-04-18 19:00:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:00:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 036:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:00:33 | INFO | fairseq.trainer | begin training epoch 36\n",
            "2024-04-18 19:00:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 036:  99% 110/111 [00:15<00:00,  6.15it/s, loss=4.233, nll_loss=3.177, ppl=9.04, wps=24503.4, ups=6.52, wpb=3760.3, bsz=11.2, num_updates=3900, lr=0.000195061, gnorm=0.428, clip=0, loss_scale=16, train_wall=9, gb_free=13.5, wall=593]2024-04-18 19:00:48 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:00:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 036 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 036 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  5.41it/s]\u001b[A\n",
            "epoch 036 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.21it/s]\u001b[A\n",
            "epoch 036 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.58it/s]\u001b[A\n",
            "epoch 036 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.36it/s]\u001b[A\n",
            "epoch 036 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.88it/s]\u001b[A\n",
            "epoch 036 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.09it/s]\u001b[A\n",
            "epoch 036 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.73it/s]\u001b[A\n",
            "epoch 036 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.36it/s]\u001b[A\n",
            "epoch 036 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.21it/s]\u001b[A\n",
            "epoch 036 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.27it/s]\u001b[A\n",
            "epoch 036 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.83it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:00:50 | INFO | dev | epoch 036 | valid on 'dev' subset | loss 4.362 | nll_loss 3.303 | ppl 9.87 | wps 28226 | wpb 3571.7 | bsz 11.1 | num_updates 3993 | best_loss 4.362\n",
            "2024-04-18 19:00:50 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
            "2024-04-18 19:00:50 | INFO | train | epoch 036 | loss 4.223 | nll_loss 3.163 | ppl 8.96 | wps 23984 | ups 6.45 | wpb 3721.2 | bsz 10.9 | num_updates 3993 | lr 0.00019971 | gnorm 0.436 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.3 | wall 608\n",
            "2024-04-18 19:00:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:00:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 037:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:00:50 | INFO | fairseq.trainer | begin training epoch 37\n",
            "2024-04-18 19:00:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 037:  99% 110/111 [00:14<00:00,  8.00it/s, loss=4.214, nll_loss=3.15, ppl=8.88, wps=27937.2, ups=7.47, wpb=3740.6, bsz=11, num_updates=4100, lr=0.000205059, gnorm=0.468, clip=0, loss_scale=16, train_wall=9, gb_free=13.6, wall=622]2024-04-18 19:01:05 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:01:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 037 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 037 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  6.06it/s]\u001b[A\n",
            "epoch 037 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.47it/s]\u001b[A\n",
            "epoch 037 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.07it/s]\u001b[A\n",
            "epoch 037 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.47it/s]\u001b[A\n",
            "epoch 037 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.81it/s]\u001b[A\n",
            "epoch 037 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.34it/s]\u001b[A\n",
            "epoch 037 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.54it/s]\u001b[A\n",
            "epoch 037 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.53it/s]\u001b[A\n",
            "epoch 037 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.21it/s]\u001b[A\n",
            "epoch 037 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.07it/s]\u001b[A\n",
            "epoch 037 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.16it/s]\u001b[A\n",
            "epoch 037 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:01:07 | INFO | dev | epoch 037 | valid on 'dev' subset | loss 4.36 | nll_loss 3.297 | ppl 9.83 | wps 27744.7 | wpb 3571.7 | bsz 11.1 | num_updates 4104 | best_loss 4.36\n",
            "2024-04-18 19:01:07 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
            "2024-04-18 19:01:07 | INFO | train | epoch 037 | loss 4.211 | nll_loss 3.147 | ppl 8.86 | wps 24837.6 | ups 6.67 | wpb 3721.2 | bsz 10.9 | num_updates 4104 | lr 0.000205259 | gnorm 0.464 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.1 | wall 625\n",
            "2024-04-18 19:01:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:01:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 038:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:01:07 | INFO | fairseq.trainer | begin training epoch 38\n",
            "2024-04-18 19:01:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 038:  99% 110/111 [00:14<00:00,  7.47it/s, loss=4.203, nll_loss=3.135, ppl=8.79, wps=24128.6, ups=6.47, wpb=3728.6, bsz=11, num_updates=4200, lr=0.000210058, gnorm=0.753, clip=1, loss_scale=16, train_wall=9, gb_free=13.1, wall=638]2024-04-18 19:01:22 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:01:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 038 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 038 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.94it/s]\u001b[A\n",
            "epoch 038 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.36it/s]\u001b[A\n",
            "epoch 038 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.05it/s]\u001b[A\n",
            "epoch 038 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.59it/s]\u001b[A\n",
            "epoch 038 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.06it/s]\u001b[A\n",
            "epoch 038 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.57it/s]\u001b[A\n",
            "epoch 038 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.88it/s]\u001b[A\n",
            "epoch 038 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.49it/s]\u001b[A\n",
            "epoch 038 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.43it/s]\u001b[A\n",
            "epoch 038 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.48it/s]\u001b[A\n",
            "epoch 038 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.86it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:01:23 | INFO | dev | epoch 038 | valid on 'dev' subset | loss 4.358 | nll_loss 3.287 | ppl 9.76 | wps 28337.8 | wpb 3571.7 | bsz 11.1 | num_updates 4215 | best_loss 4.358\n",
            "2024-04-18 19:01:23 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
            "2024-04-18 19:01:23 | INFO | train | epoch 038 | loss 4.2 | nll_loss 3.132 | ppl 8.76 | wps 24495.8 | ups 6.58 | wpb 3721.2 | bsz 10.9 | num_updates 4215 | lr 0.000210808 | gnorm 0.72 | clip 0.9 | loss_scale 16 | train_wall 10 | gb_free 12.9 | wall 641\n",
            "2024-04-18 19:01:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:01:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 039:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:01:24 | INFO | fairseq.trainer | begin training epoch 39\n",
            "2024-04-18 19:01:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 039:  99% 110/111 [00:15<00:00,  7.47it/s, loss=4.183, nll_loss=3.108, ppl=8.62, wps=24389.1, ups=6.45, wpb=3779.5, bsz=11.1, num_updates=4300, lr=0.000215057, gnorm=0.458, clip=0, loss_scale=16, train_wall=9, gb_free=13.5, wall=653]2024-04-18 19:01:39 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:01:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 039 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 039 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.70it/s]\u001b[A\n",
            "epoch 039 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.41it/s]\u001b[A\n",
            "epoch 039 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.24it/s]\u001b[A\n",
            "epoch 039 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.63it/s]\u001b[A\n",
            "epoch 039 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.12it/s]\u001b[A\n",
            "epoch 039 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.13it/s]\u001b[A\n",
            "epoch 039 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  8.00it/s]\u001b[A\n",
            "epoch 039 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.53it/s]\u001b[A\n",
            "epoch 039 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.31it/s]\u001b[A\n",
            "epoch 039 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.37it/s]\u001b[A\n",
            "epoch 039 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.82it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:01:40 | INFO | dev | epoch 039 | valid on 'dev' subset | loss 4.358 | nll_loss 3.289 | ppl 9.78 | wps 28698.3 | wpb 3571.7 | bsz 11.1 | num_updates 4326 | best_loss 4.358\n",
            "2024-04-18 19:01:40 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
            "2024-04-18 19:01:40 | INFO | train | epoch 039 | loss 4.188 | nll_loss 3.116 | ppl 8.67 | wps 24530 | ups 6.59 | wpb 3721.2 | bsz 10.9 | num_updates 4326 | lr 0.000216357 | gnorm 0.461 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.7 | wall 658\n",
            "2024-04-18 19:01:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:01:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 040:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:01:40 | INFO | fairseq.trainer | begin training epoch 40\n",
            "2024-04-18 19:01:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 040:  99% 110/111 [00:14<00:00,  6.87it/s, loss=4.188, nll_loss=3.115, ppl=8.66, wps=24308.6, ups=6.73, wpb=3611.3, bsz=10.5, num_updates=4400, lr=0.000220056, gnorm=0.462, clip=0, loss_scale=16, train_wall=9, gb_free=12.8, wall=668]2024-04-18 19:01:55 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:01:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 040 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 040 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.62it/s]\u001b[A\n",
            "epoch 040 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  5.93it/s]\u001b[A\n",
            "epoch 040 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.51it/s]\u001b[A\n",
            "epoch 040 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  6.91it/s]\u001b[A\n",
            "epoch 040 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.46it/s]\u001b[A\n",
            "epoch 040 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  7.89it/s]\u001b[A\n",
            "epoch 040 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.46it/s]\u001b[A\n",
            "epoch 040 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.04it/s]\u001b[A\n",
            "epoch 040 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.53it/s]\u001b[A\n",
            "epoch 040 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.39it/s]\u001b[A\n",
            "epoch 040 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.42it/s]\u001b[A\n",
            "epoch 040 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  6.79it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:01:57 | INFO | dev | epoch 040 | valid on 'dev' subset | loss 4.353 | nll_loss 3.277 | ppl 9.69 | wps 25378.5 | wpb 3571.7 | bsz 11.1 | num_updates 4437 | best_loss 4.353\n",
            "2024-04-18 19:01:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 4437 updates\n",
            "2024-04-18 19:01:57 | INFO | fairseq.trainer | Saving checkpoint to /content/SAVE_DIR_TRAINING/checkpoint40.pt\n",
            "2024-04-18 19:01:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/SAVE_DIR_TRAINING/checkpoint40.pt\n",
            "2024-04-18 19:01:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/SAVE_DIR_TRAINING/checkpoint40.pt (epoch 40 @ 4437 updates, score 4.353) (writing took 1.8903441600000406 seconds)\n",
            "2024-04-18 19:01:59 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
            "2024-04-18 19:01:59 | INFO | train | epoch 040 | loss 4.179 | nll_loss 3.103 | ppl 8.59 | wps 21909.3 | ups 5.89 | wpb 3721.2 | bsz 10.9 | num_updates 4437 | lr 0.000221906 | gnorm 0.458 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.8 | wall 677\n",
            "2024-04-18 19:01:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:01:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 041:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:01:59 | INFO | fairseq.trainer | begin training epoch 41\n",
            "2024-04-18 19:01:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 041:  98% 109/111 [00:14<00:00,  6.37it/s, loss=4.167, nll_loss=3.087, ppl=8.5, wps=20952.1, ups=5.67, wpb=3698.3, bsz=10.4, num_updates=4500, lr=0.000225055, gnorm=0.452, clip=0, loss_scale=16, train_wall=9, gb_free=12.7, wall=686]2024-04-18 19:02:14 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:02:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 041 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 041 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  5.13it/s]\u001b[A\n",
            "epoch 041 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  5.75it/s]\u001b[A\n",
            "epoch 041 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.52it/s]\u001b[A\n",
            "epoch 041 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  6.91it/s]\u001b[A\n",
            "epoch 041 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.45it/s]\u001b[A\n",
            "epoch 041 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.11it/s]\u001b[A\n",
            "epoch 041 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.72it/s]\u001b[A\n",
            "epoch 041 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.29it/s]\u001b[A\n",
            "epoch 041 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.24it/s]\u001b[A\n",
            "epoch 041 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.38it/s]\u001b[A\n",
            "epoch 041 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.91it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:02:16 | INFO | dev | epoch 041 | valid on 'dev' subset | loss 4.346 | nll_loss 3.275 | ppl 9.68 | wps 27771.6 | wpb 3571.7 | bsz 11.1 | num_updates 4548 | best_loss 4.346\n",
            "2024-04-18 19:02:16 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
            "2024-04-18 19:02:16 | INFO | train | epoch 041 | loss 4.169 | nll_loss 3.088 | ppl 8.5 | wps 24312.8 | ups 6.53 | wpb 3721.2 | bsz 10.9 | num_updates 4548 | lr 0.000227455 | gnorm 0.454 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.4 | wall 694\n",
            "2024-04-18 19:02:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:02:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 042:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:02:16 | INFO | fairseq.trainer | begin training epoch 42\n",
            "2024-04-18 19:02:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 042:  99% 110/111 [00:15<00:00,  8.75it/s, loss=4.166, nll_loss=3.084, ppl=8.48, wps=24258.1, ups=6.28, wpb=3862.9, bsz=11.5, num_updates=4600, lr=0.000230054, gnorm=0.45, clip=0, loss_scale=16, train_wall=9, gb_free=13.6, wall=702]2024-04-18 19:02:32 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:02:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 042 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 042 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  5.47it/s]\u001b[A\n",
            "epoch 042 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.23it/s]\u001b[A\n",
            "epoch 042 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.15it/s]\u001b[A\n",
            "epoch 042 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.69it/s]\u001b[A\n",
            "epoch 042 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.06it/s]\u001b[A\n",
            "epoch 042 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.54it/s]\u001b[A\n",
            "epoch 042 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.88it/s]\u001b[A\n",
            "epoch 042 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.45it/s]\u001b[A\n",
            "epoch 042 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.30it/s]\u001b[A\n",
            "epoch 042 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.40it/s]\u001b[A\n",
            "epoch 042 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.93it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:02:33 | INFO | dev | epoch 042 | valid on 'dev' subset | loss 4.359 | nll_loss 3.288 | ppl 9.77 | wps 28527.2 | wpb 3571.7 | bsz 11.1 | num_updates 4659 | best_loss 4.353\n",
            "2024-04-18 19:02:33 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
            "2024-04-18 19:02:33 | INFO | train | epoch 042 | loss 4.157 | nll_loss 3.073 | ppl 8.42 | wps 24288.2 | ups 6.53 | wpb 3721.2 | bsz 10.9 | num_updates 4659 | lr 0.000233003 | gnorm 0.455 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 14 | wall 711\n",
            "2024-04-18 19:02:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:02:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 043:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:02:33 | INFO | fairseq.trainer | begin training epoch 43\n",
            "2024-04-18 19:02:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 043:  99% 110/111 [00:14<00:00,  7.53it/s, loss=4.152, nll_loss=3.065, ppl=8.37, wps=24348.1, ups=6.64, wpb=3668, bsz=10.9, num_updates=4700, lr=0.000235053, gnorm=0.463, clip=0, loss_scale=16, train_wall=9, gb_free=13, wall=717]2024-04-18 19:02:48 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:02:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 043 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 043 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.87it/s]\u001b[A\n",
            "epoch 043 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.29it/s]\u001b[A\n",
            "epoch 043 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.04it/s]\u001b[A\n",
            "epoch 043 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.56it/s]\u001b[A\n",
            "epoch 043 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.13it/s]\u001b[A\n",
            "epoch 043 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.59it/s]\u001b[A\n",
            "epoch 043 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.95it/s]\u001b[A\n",
            "epoch 043 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.54it/s]\u001b[A\n",
            "epoch 043 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.12it/s]\u001b[A\n",
            "epoch 043 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.28it/s]\u001b[A\n",
            "epoch 043 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.77it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:02:50 | INFO | dev | epoch 043 | valid on 'dev' subset | loss 4.363 | nll_loss 3.281 | ppl 9.72 | wps 28090.4 | wpb 3571.7 | bsz 11.1 | num_updates 4770 | best_loss 4.353\n",
            "2024-04-18 19:02:50 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
            "2024-04-18 19:02:50 | INFO | train | epoch 043 | loss 4.147 | nll_loss 3.058 | ppl 8.33 | wps 24619.8 | ups 6.62 | wpb 3721.2 | bsz 10.9 | num_updates 4770 | lr 0.000238552 | gnorm 0.482 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.1 | wall 728\n",
            "2024-04-18 19:02:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:02:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 044:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:02:50 | INFO | fairseq.trainer | begin training epoch 44\n",
            "2024-04-18 19:02:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 044:  99% 110/111 [00:14<00:00,  7.53it/s, loss=4.147, nll_loss=3.059, ppl=8.33, wps=24352.2, ups=6.48, wpb=3758.8, bsz=10.7, num_updates=4800, lr=0.000240052, gnorm=0.478, clip=0, loss_scale=16, train_wall=9, gb_free=13.8, wall=732]2024-04-18 19:03:05 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:03:05 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 044 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 044 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  6.00it/s]\u001b[A\n",
            "epoch 044 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.35it/s]\u001b[A\n",
            "epoch 044 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.96it/s]\u001b[A\n",
            "epoch 044 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.32it/s]\u001b[A\n",
            "epoch 044 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.76it/s]\u001b[A\n",
            "epoch 044 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.31it/s]\u001b[A\n",
            "epoch 044 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  8.02it/s]\u001b[A\n",
            "epoch 044 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.47it/s]\u001b[A\n",
            "epoch 044 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.20it/s]\u001b[A\n",
            "epoch 044 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.23it/s]\u001b[A\n",
            "epoch 044 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.71it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:03:07 | INFO | dev | epoch 044 | valid on 'dev' subset | loss 4.36 | nll_loss 3.281 | ppl 9.72 | wps 27801.1 | wpb 3571.7 | bsz 11.1 | num_updates 4881 | best_loss 4.353\n",
            "2024-04-18 19:03:07 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
            "2024-04-18 19:03:07 | INFO | train | epoch 044 | loss 4.133 | nll_loss 3.04 | ppl 8.23 | wps 24580.8 | ups 6.61 | wpb 3721.2 | bsz 10.9 | num_updates 4881 | lr 0.000244101 | gnorm 0.514 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.8 | wall 745\n",
            "2024-04-18 19:03:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:03:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 045:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:03:07 | INFO | fairseq.trainer | begin training epoch 45\n",
            "2024-04-18 19:03:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 045:  98% 109/111 [00:14<00:00,  8.58it/s, loss=4.124, nll_loss=3.027, ppl=8.15, wps=24189.4, ups=6.61, wpb=3658.2, bsz=11, num_updates=4900, lr=0.000245051, gnorm=0.539, clip=0, loss_scale=16, train_wall=9, gb_free=13.6, wall=747]2024-04-18 19:03:22 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:03:22 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 045 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 045 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.82it/s]\u001b[A\n",
            "epoch 045 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.10it/s]\u001b[A\n",
            "epoch 045 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.83it/s]\u001b[A\n",
            "epoch 045 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.31it/s]\u001b[A\n",
            "epoch 045 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.88it/s]\u001b[A\n",
            "epoch 045 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.06it/s]\u001b[A\n",
            "epoch 045 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.81it/s]\u001b[A\n",
            "epoch 045 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.27it/s]\u001b[A\n",
            "epoch 045 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.10it/s]\u001b[A\n",
            "epoch 045 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.20it/s]\u001b[A\n",
            "epoch 045 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.74it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:03:24 | INFO | dev | epoch 045 | valid on 'dev' subset | loss 4.359 | nll_loss 3.28 | ppl 9.72 | wps 27879.6 | wpb 3571.7 | bsz 11.1 | num_updates 4992 | best_loss 4.353\n",
            "2024-04-18 19:03:24 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
            "2024-04-18 19:03:24 | INFO | train | epoch 045 | loss 4.123 | nll_loss 3.025 | ppl 8.14 | wps 24636.7 | ups 6.62 | wpb 3721.2 | bsz 10.9 | num_updates 4992 | lr 0.00024965 | gnorm 0.651 | clip 0.9 | loss_scale 16 | train_wall 10 | gb_free 14 | wall 761\n",
            "2024-04-18 19:03:24 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:03:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 046:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:03:24 | INFO | fairseq.trainer | begin training epoch 46\n",
            "2024-04-18 19:03:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 046:  99% 110/111 [00:14<00:00,  7.50it/s, loss=4.107, nll_loss=3.004, ppl=8.02, wps=27843.3, ups=7.49, wpb=3716.9, bsz=11.1, num_updates=5100, lr=0.000255049, gnorm=0.501, clip=0, loss_scale=16, train_wall=9, gb_free=12.8, wall=776]2024-04-18 19:03:39 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:03:39 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 046 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 046 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.67it/s]\u001b[A\n",
            "epoch 046 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.38it/s]\u001b[A\n",
            "epoch 046 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.38it/s]\u001b[A\n",
            "epoch 046 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.93it/s]\u001b[A\n",
            "epoch 046 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.38it/s]\u001b[A\n",
            "epoch 046 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.79it/s]\u001b[A\n",
            "epoch 046 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.08it/s]\u001b[A\n",
            "epoch 046 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.56it/s]\u001b[A\n",
            "epoch 046 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.78it/s]\u001b[A\n",
            "epoch 046 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.69it/s]\u001b[A\n",
            "epoch 046 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.92it/s]\u001b[A\n",
            "epoch 046 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.52it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:03:40 | INFO | dev | epoch 046 | valid on 'dev' subset | loss 4.366 | nll_loss 3.278 | ppl 9.7 | wps 27885.2 | wpb 3571.7 | bsz 11.1 | num_updates 5103 | best_loss 4.353\n",
            "2024-04-18 19:03:40 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
            "2024-04-18 19:03:40 | INFO | train | epoch 046 | loss 4.108 | nll_loss 3.006 | ppl 8.03 | wps 24473.1 | ups 6.58 | wpb 3721.2 | bsz 10.9 | num_updates 5103 | lr 0.000255199 | gnorm 0.497 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.9 | wall 778\n",
            "2024-04-18 19:03:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:03:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 047:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:03:41 | INFO | fairseq.trainer | begin training epoch 47\n",
            "2024-04-18 19:03:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 047:  98% 109/111 [00:14<00:00,  7.33it/s, loss=4.093, nll_loss=2.984, ppl=7.91, wps=24027.7, ups=6.51, wpb=3692.1, bsz=10.9, num_updates=5200, lr=0.000260048, gnorm=0.507, clip=0, loss_scale=16, train_wall=9, gb_free=12.7, wall=792]2024-04-18 19:03:56 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:03:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 047 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 047 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  6.03it/s]\u001b[A\n",
            "epoch 047 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.46it/s]\u001b[A\n",
            "epoch 047 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.25it/s]\u001b[A\n",
            "epoch 047 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.76it/s]\u001b[A\n",
            "epoch 047 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.34it/s]\u001b[A\n",
            "epoch 047 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.40it/s]\u001b[A\n",
            "epoch 047 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  8.01it/s]\u001b[A\n",
            "epoch 047 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.55it/s]\u001b[A\n",
            "epoch 047 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.28it/s]\u001b[A\n",
            "epoch 047 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.49it/s]\u001b[A\n",
            "epoch 047 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  8.00it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:03:57 | INFO | dev | epoch 047 | valid on 'dev' subset | loss 4.368 | nll_loss 3.289 | ppl 9.77 | wps 28999.2 | wpb 3571.7 | bsz 11.1 | num_updates 5214 | best_loss 4.353\n",
            "2024-04-18 19:03:57 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
            "2024-04-18 19:03:57 | INFO | train | epoch 047 | loss 4.097 | nll_loss 2.99 | ppl 7.94 | wps 24565.6 | ups 6.6 | wpb 3721.2 | bsz 10.9 | num_updates 5214 | lr 0.000260748 | gnorm 0.507 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12 | wall 795\n",
            "2024-04-18 19:03:57 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:03:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 048:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:03:57 | INFO | fairseq.trainer | begin training epoch 48\n",
            "2024-04-18 19:03:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 048:  99% 110/111 [00:15<00:00,  8.17it/s, loss=4.093, nll_loss=2.984, ppl=7.91, wps=24429.1, ups=6.45, wpb=3785.9, bsz=11.1, num_updates=5300, lr=0.000265047, gnorm=0.49, clip=0, loss_scale=16, train_wall=9, gb_free=13.2, wall=807]2024-04-18 19:04:12 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:04:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 048 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 048 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  6.00it/s]\u001b[A\n",
            "epoch 048 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.60it/s]\u001b[A\n",
            "epoch 048 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.43it/s]\u001b[A\n",
            "epoch 048 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.99it/s]\u001b[A\n",
            "epoch 048 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.55it/s]\u001b[A\n",
            "epoch 048 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.61it/s]\u001b[A\n",
            "epoch 048 | valid on 'dev' subset:  67% 8/12 [00:00<00:00,  8.09it/s]\u001b[A\n",
            "epoch 048 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.52it/s]\u001b[A\n",
            "epoch 048 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.32it/s]\u001b[A\n",
            "epoch 048 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.45it/s]\u001b[A\n",
            "epoch 048 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.99it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:04:14 | INFO | dev | epoch 048 | valid on 'dev' subset | loss 4.376 | nll_loss 3.296 | ppl 9.82 | wps 29261.4 | wpb 3571.7 | bsz 11.1 | num_updates 5325 | best_loss 4.353\n",
            "2024-04-18 19:04:14 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
            "2024-04-18 19:04:14 | INFO | train | epoch 048 | loss 4.085 | nll_loss 2.973 | ppl 7.85 | wps 24577.6 | ups 6.6 | wpb 3721.2 | bsz 10.9 | num_updates 5325 | lr 0.000266297 | gnorm 0.54 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.6 | wall 812\n",
            "2024-04-18 19:04:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:04:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 049:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:04:14 | INFO | fairseq.trainer | begin training epoch 49\n",
            "2024-04-18 19:04:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 049:  99% 110/111 [00:14<00:00,  6.69it/s, loss=4.066, nll_loss=2.946, ppl=7.71, wps=24360.3, ups=6.72, wpb=3626.5, bsz=10.6, num_updates=5400, lr=0.000270046, gnorm=0.556, clip=0, loss_scale=16, train_wall=9, gb_free=13.9, wall=822]2024-04-18 19:04:29 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:04:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 049 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 049 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  6.07it/s]\u001b[A\n",
            "epoch 049 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.45it/s]\u001b[A\n",
            "epoch 049 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.14it/s]\u001b[A\n",
            "epoch 049 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.39it/s]\u001b[A\n",
            "epoch 049 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.86it/s]\u001b[A\n",
            "epoch 049 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.97it/s]\u001b[A\n",
            "epoch 049 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.85it/s]\u001b[A\n",
            "epoch 049 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.38it/s]\u001b[A\n",
            "epoch 049 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.27it/s]\u001b[A\n",
            "epoch 049 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.34it/s]\u001b[A\n",
            "epoch 049 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.83it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:04:31 | INFO | dev | epoch 049 | valid on 'dev' subset | loss 4.385 | nll_loss 3.307 | ppl 9.9 | wps 28236.6 | wpb 3571.7 | bsz 11.1 | num_updates 5436 | best_loss 4.353\n",
            "2024-04-18 19:04:31 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
            "2024-04-18 19:04:31 | INFO | train | epoch 049 | loss 4.073 | nll_loss 2.956 | ppl 7.76 | wps 24783.7 | ups 6.66 | wpb 3721.2 | bsz 10.9 | num_updates 5436 | lr 0.000271846 | gnorm 0.494 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.6 | wall 829\n",
            "2024-04-18 19:04:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:04:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 050:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:04:31 | INFO | fairseq.trainer | begin training epoch 50\n",
            "2024-04-18 19:04:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 050:  99% 110/111 [00:14<00:00,  8.16it/s, loss=4.066, nll_loss=2.947, ppl=7.71, wps=24681.3, ups=6.47, wpb=3812.2, bsz=11.2, num_updates=5500, lr=0.000275045, gnorm=0.529, clip=0, loss_scale=16, train_wall=9, gb_free=13.5, wall=837]2024-04-18 19:04:46 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:04:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 050 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 050 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  6.08it/s]\u001b[A\n",
            "epoch 050 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.54it/s]\u001b[A\n",
            "epoch 050 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.45it/s]\u001b[A\n",
            "epoch 050 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.87it/s]\u001b[A\n",
            "epoch 050 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.18it/s]\u001b[A\n",
            "epoch 050 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.61it/s]\u001b[A\n",
            "epoch 050 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.97it/s]\u001b[A\n",
            "epoch 050 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.58it/s]\u001b[A\n",
            "epoch 050 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.27it/s]\u001b[A\n",
            "epoch 050 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.33it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:04:47 | INFO | dev | epoch 050 | valid on 'dev' subset | loss 4.383 | nll_loss 3.305 | ppl 9.88 | wps 28584.1 | wpb 3571.7 | bsz 11.1 | num_updates 5547 | best_loss 4.353\n",
            "2024-04-18 19:04:47 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
            "2024-04-18 19:04:47 | INFO | train | epoch 050 | loss 4.056 | nll_loss 2.934 | ppl 7.64 | wps 24993.1 | ups 6.72 | wpb 3721.2 | bsz 10.9 | num_updates 5547 | lr 0.000277395 | gnorm 0.54 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.4 | wall 845\n",
            "2024-04-18 19:04:47 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:04:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 051:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:04:47 | INFO | fairseq.trainer | begin training epoch 51\n",
            "2024-04-18 19:04:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 051:  99% 110/111 [00:14<00:00,  7.56it/s, loss=4.054, nll_loss=2.93, ppl=7.62, wps=24695.5, ups=6.62, wpb=3731.9, bsz=11, num_updates=5600, lr=0.000280044, gnorm=0.514, clip=0, loss_scale=16, train_wall=9, gb_free=13.4, wall=853]2024-04-18 19:05:02 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:05:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 051 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 051 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.73it/s]\u001b[A\n",
            "epoch 051 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.42it/s]\u001b[A\n",
            "epoch 051 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.31it/s]\u001b[A\n",
            "epoch 051 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.96it/s]\u001b[A\n",
            "epoch 051 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.46it/s]\u001b[A\n",
            "epoch 051 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.90it/s]\u001b[A\n",
            "epoch 051 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  8.08it/s]\u001b[A\n",
            "epoch 051 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.45it/s]\u001b[A\n",
            "epoch 051 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.23it/s]\u001b[A\n",
            "epoch 051 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.34it/s]\u001b[A\n",
            "epoch 051 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.93it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:05:04 | INFO | dev | epoch 051 | valid on 'dev' subset | loss 4.394 | nll_loss 3.311 | ppl 9.92 | wps 28798 | wpb 3571.7 | bsz 11.1 | num_updates 5658 | best_loss 4.353\n",
            "2024-04-18 19:05:04 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)\n",
            "2024-04-18 19:05:04 | INFO | train | epoch 051 | loss 4.046 | nll_loss 2.918 | ppl 7.56 | wps 25182.3 | ups 6.77 | wpb 3721.2 | bsz 10.9 | num_updates 5658 | lr 0.000282943 | gnorm 0.521 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.6 | wall 862\n",
            "2024-04-18 19:05:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:05:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 052:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:05:04 | INFO | fairseq.trainer | begin training epoch 52\n",
            "2024-04-18 19:05:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 052:  99% 110/111 [00:14<00:00,  8.64it/s, loss=4.033, nll_loss=2.901, ppl=7.47, wps=24520.9, ups=6.78, wpb=3616.8, bsz=10.3, num_updates=5700, lr=0.000285043, gnorm=0.555, clip=0, loss_scale=16, train_wall=9, gb_free=12.9, wall=867]2024-04-18 19:05:19 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:05:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 052 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 052 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  6.09it/s]\u001b[A\n",
            "epoch 052 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.64it/s]\u001b[A\n",
            "epoch 052 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.33it/s]\u001b[A\n",
            "epoch 052 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.71it/s]\u001b[A\n",
            "epoch 052 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.26it/s]\u001b[A\n",
            "epoch 052 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.21it/s]\u001b[A\n",
            "epoch 052 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.92it/s]\u001b[A\n",
            "epoch 052 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.49it/s]\u001b[A\n",
            "epoch 052 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.38it/s]\u001b[A\n",
            "epoch 052 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.42it/s]\u001b[A\n",
            "epoch 052 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  8.00it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:05:20 | INFO | dev | epoch 052 | valid on 'dev' subset | loss 4.399 | nll_loss 3.315 | ppl 9.95 | wps 28892.8 | wpb 3571.7 | bsz 11.1 | num_updates 5769 | best_loss 4.353\n",
            "2024-04-18 19:05:20 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)\n",
            "2024-04-18 19:05:20 | INFO | train | epoch 052 | loss 4.029 | nll_loss 2.895 | ppl 7.44 | wps 24668.8 | ups 6.63 | wpb 3721.2 | bsz 10.9 | num_updates 5769 | lr 0.000288492 | gnorm 0.555 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13 | wall 878\n",
            "2024-04-18 19:05:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:05:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 053:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:05:20 | INFO | fairseq.trainer | begin training epoch 53\n",
            "2024-04-18 19:05:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 053:  99% 110/111 [00:15<00:00,  6.41it/s, loss=4.024, nll_loss=2.888, ppl=7.4, wps=24424.4, ups=6.45, wpb=3789.1, bsz=11.3, num_updates=5800, lr=0.000290042, gnorm=0.525, clip=0, loss_scale=16, train_wall=9, gb_free=12.8, wall=883]2024-04-18 19:05:36 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:05:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 053 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 053 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.68it/s]\u001b[A\n",
            "epoch 053 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  5.95it/s]\u001b[A\n",
            "epoch 053 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n",
            "epoch 053 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.11it/s]\u001b[A\n",
            "epoch 053 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.59it/s]\u001b[A\n",
            "epoch 053 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.08it/s]\u001b[A\n",
            "epoch 053 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.49it/s]\u001b[A\n",
            "epoch 053 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.02it/s]\u001b[A\n",
            "epoch 053 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.84it/s]\u001b[A\n",
            "epoch 053 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.81it/s]\u001b[A\n",
            "epoch 053 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.23it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:05:38 | INFO | dev | epoch 053 | valid on 'dev' subset | loss 4.41 | nll_loss 3.329 | ppl 10.05 | wps 26316.5 | wpb 3571.7 | bsz 11.1 | num_updates 5880 | best_loss 4.353\n",
            "2024-04-18 19:05:38 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)\n",
            "2024-04-18 19:05:38 | INFO | train | epoch 053 | loss 4.014 | nll_loss 2.875 | ppl 7.34 | wps 23259.9 | ups 6.25 | wpb 3721.2 | bsz 10.9 | num_updates 5880 | lr 0.000294041 | gnorm 0.528 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.9 | wall 896\n",
            "2024-04-18 19:05:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:05:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 054:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:05:38 | INFO | fairseq.trainer | begin training epoch 54\n",
            "2024-04-18 19:05:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 054:  99% 110/111 [00:16<00:00,  7.90it/s, loss=4.018, nll_loss=2.88, ppl=7.36, wps=22543.4, ups=6.03, wpb=3739.3, bsz=11, num_updates=5900, lr=0.000295041, gnorm=0.53, clip=0, loss_scale=16, train_wall=9, gb_free=12.7, wall=899]2024-04-18 19:05:54 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:05:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 054 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 054 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.51it/s]\u001b[A\n",
            "epoch 054 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.03it/s]\u001b[A\n",
            "epoch 054 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.64it/s]\u001b[A\n",
            "epoch 054 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.12it/s]\u001b[A\n",
            "epoch 054 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.58it/s]\u001b[A\n",
            "epoch 054 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.07it/s]\u001b[A\n",
            "epoch 054 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.41it/s]\u001b[A\n",
            "epoch 054 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.02it/s]\u001b[A\n",
            "epoch 054 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.79it/s]\u001b[A\n",
            "epoch 054 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.90it/s]\u001b[A\n",
            "epoch 054 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.46it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:05:56 | INFO | dev | epoch 054 | valid on 'dev' subset | loss 4.419 | nll_loss 3.333 | ppl 10.08 | wps 26637.3 | wpb 3571.7 | bsz 11.1 | num_updates 5991 | best_loss 4.353\n",
            "2024-04-18 19:05:56 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)\n",
            "2024-04-18 19:05:56 | INFO | train | epoch 054 | loss 4.001 | nll_loss 2.856 | ppl 7.24 | wps 22830.5 | ups 6.14 | wpb 3721.2 | bsz 10.9 | num_updates 5991 | lr 0.00029959 | gnorm 0.54 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.6 | wall 914\n",
            "2024-04-18 19:05:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:05:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 055:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:05:56 | INFO | fairseq.trainer | begin training epoch 55\n",
            "2024-04-18 19:05:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 055:  99% 110/111 [00:16<00:00,  6.83it/s, loss=3.984, nll_loss=2.831, ppl=7.12, wps=25391, ups=6.91, wpb=3675.9, bsz=10.8, num_updates=6100, lr=0.000305039, gnorm=0.57, clip=0, loss_scale=16, train_wall=9, gb_free=13.3, wall=930]2024-04-18 19:06:13 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:06:13 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 055 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 055 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  5.49it/s]\u001b[A\n",
            "epoch 055 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.19it/s]\u001b[A\n",
            "epoch 055 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.04it/s]\u001b[A\n",
            "epoch 055 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.39it/s]\u001b[A\n",
            "epoch 055 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.84it/s]\u001b[A\n",
            "epoch 055 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.25it/s]\u001b[A\n",
            "epoch 055 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.75it/s]\u001b[A\n",
            "epoch 055 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.07it/s]\u001b[A\n",
            "epoch 055 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.66it/s]\u001b[A\n",
            "epoch 055 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.56it/s]\u001b[A\n",
            "epoch 055 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.66it/s]\u001b[A\n",
            "epoch 055 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.17it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:06:14 | INFO | dev | epoch 055 | valid on 'dev' subset | loss 4.432 | nll_loss 3.351 | ppl 10.21 | wps 26629.9 | wpb 3571.7 | bsz 11.1 | num_updates 6102 | best_loss 4.353\n",
            "2024-04-18 19:06:14 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)\n",
            "2024-04-18 19:06:14 | INFO | train | epoch 055 | loss 3.985 | nll_loss 2.833 | ppl 7.13 | wps 22747.1 | ups 6.11 | wpb 3721.2 | bsz 10.9 | num_updates 6102 | lr 0.000305139 | gnorm 0.567 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.9 | wall 932\n",
            "2024-04-18 19:06:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:06:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 056:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:06:14 | INFO | fairseq.trainer | begin training epoch 56\n",
            "2024-04-18 19:06:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 056:  99% 110/111 [00:16<00:00,  6.08it/s, loss=3.966, nll_loss=2.808, ppl=7, wps=22388, ups=6.04, wpb=3704.3, bsz=10.8, num_updates=6200, lr=0.000310038, gnorm=0.582, clip=0, loss_scale=16, train_wall=9, gb_free=13.4, wall=947]2024-04-18 19:06:31 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:06:31 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 056 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 056 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  4.91it/s]\u001b[A\n",
            "epoch 056 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  5.68it/s]\u001b[A\n",
            "epoch 056 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.40it/s]\u001b[A\n",
            "epoch 056 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  6.80it/s]\u001b[A\n",
            "epoch 056 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.29it/s]\u001b[A\n",
            "epoch 056 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  7.87it/s]\u001b[A\n",
            "epoch 056 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.39it/s]\u001b[A\n",
            "epoch 056 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.90it/s]\u001b[A\n",
            "epoch 056 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.65it/s]\u001b[A\n",
            "epoch 056 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.72it/s]\u001b[A\n",
            "epoch 056 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.11it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:06:33 | INFO | dev | epoch 056 | valid on 'dev' subset | loss 4.43 | nll_loss 3.346 | ppl 10.17 | wps 25981.8 | wpb 3571.7 | bsz 11.1 | num_updates 6213 | best_loss 4.353\n",
            "2024-04-18 19:06:33 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)\n",
            "2024-04-18 19:06:33 | INFO | train | epoch 056 | loss 3.97 | nll_loss 2.813 | ppl 7.03 | wps 22579.9 | ups 6.07 | wpb 3721.2 | bsz 10.9 | num_updates 6213 | lr 0.000310688 | gnorm 0.58 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.6 | wall 951\n",
            "2024-04-18 19:06:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:06:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 057:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:06:33 | INFO | fairseq.trainer | begin training epoch 57\n",
            "2024-04-18 19:06:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 057:  99% 110/111 [00:16<00:00,  7.16it/s, loss=3.961, nll_loss=2.8, ppl=6.97, wps=22824.4, ups=5.99, wpb=3811.9, bsz=11.3, num_updates=6300, lr=0.000315037, gnorm=0.568, clip=0, loss_scale=16, train_wall=9, gb_free=13.7, wall=963]2024-04-18 19:06:49 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:06:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 057 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 057 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  5.49it/s]\u001b[A\n",
            "epoch 057 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.16it/s]\u001b[A\n",
            "epoch 057 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.73it/s]\u001b[A\n",
            "epoch 057 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.19it/s]\u001b[A\n",
            "epoch 057 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.59it/s]\u001b[A\n",
            "epoch 057 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.15it/s]\u001b[A\n",
            "epoch 057 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.38it/s]\u001b[A\n",
            "epoch 057 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.84it/s]\u001b[A\n",
            "epoch 057 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.63it/s]\u001b[A\n",
            "epoch 057 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.77it/s]\u001b[A\n",
            "epoch 057 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.33it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:06:51 | INFO | dev | epoch 057 | valid on 'dev' subset | loss 4.45 | nll_loss 3.372 | ppl 10.35 | wps 26384.5 | wpb 3571.7 | bsz 11.1 | num_updates 6324 | best_loss 4.353\n",
            "2024-04-18 19:06:51 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)\n",
            "2024-04-18 19:06:51 | INFO | train | epoch 057 | loss 3.953 | nll_loss 2.79 | ppl 6.92 | wps 22951.4 | ups 6.17 | wpb 3721.2 | bsz 10.9 | num_updates 6324 | lr 0.000316237 | gnorm 0.576 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.5 | wall 969\n",
            "2024-04-18 19:06:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:06:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 058:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:06:51 | INFO | fairseq.trainer | begin training epoch 58\n",
            "2024-04-18 19:06:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 058:  99% 110/111 [00:15<00:00,  6.66it/s, loss=3.941, nll_loss=2.772, ppl=6.83, wps=23268.6, ups=6.25, wpb=3723.5, bsz=11.1, num_updates=6400, lr=0.000320036, gnorm=0.596, clip=0, loss_scale=16, train_wall=9, gb_free=13.5, wall=979]2024-04-18 19:07:06 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:07:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 058 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 058 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.58it/s]\u001b[A\n",
            "epoch 058 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  5.99it/s]\u001b[A\n",
            "epoch 058 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.70it/s]\u001b[A\n",
            "epoch 058 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.15it/s]\u001b[A\n",
            "epoch 058 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.65it/s]\u001b[A\n",
            "epoch 058 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.13it/s]\u001b[A\n",
            "epoch 058 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.56it/s]\u001b[A\n",
            "epoch 058 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.05it/s]\u001b[A\n",
            "epoch 058 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.87it/s]\u001b[A\n",
            "epoch 058 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.87it/s]\u001b[A\n",
            "epoch 058 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.35it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:07:08 | INFO | dev | epoch 058 | valid on 'dev' subset | loss 4.453 | nll_loss 3.365 | ppl 10.3 | wps 26638.8 | wpb 3571.7 | bsz 11.1 | num_updates 6435 | best_loss 4.353\n",
            "2024-04-18 19:07:08 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)\n",
            "2024-04-18 19:07:08 | INFO | train | epoch 058 | loss 3.941 | nll_loss 2.771 | ppl 6.83 | wps 23947.9 | ups 6.44 | wpb 3721.2 | bsz 10.9 | num_updates 6435 | lr 0.000321786 | gnorm 0.598 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.8 | wall 986\n",
            "2024-04-18 19:07:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:07:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 059:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:07:08 | INFO | fairseq.trainer | begin training epoch 59\n",
            "2024-04-18 19:07:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 059:  99% 110/111 [00:15<00:00,  8.39it/s, loss=3.922, nll_loss=2.746, ppl=6.71, wps=23587.8, ups=6.4, wpb=3687.5, bsz=10.6, num_updates=6500, lr=0.000325035, gnorm=0.596, clip=0, loss_scale=16, train_wall=9, gb_free=12.4, wall=995]2024-04-18 19:07:23 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:07:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 059 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 059 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.58it/s]\u001b[A\n",
            "epoch 059 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.09it/s]\u001b[A\n",
            "epoch 059 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.85it/s]\u001b[A\n",
            "epoch 059 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.33it/s]\u001b[A\n",
            "epoch 059 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.93it/s]\u001b[A\n",
            "epoch 059 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.42it/s]\u001b[A\n",
            "epoch 059 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.75it/s]\u001b[A\n",
            "epoch 059 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.34it/s]\u001b[A\n",
            "epoch 059 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.15it/s]\u001b[A\n",
            "epoch 059 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.06it/s]\u001b[A\n",
            "epoch 059 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.58it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:07:25 | INFO | dev | epoch 059 | valid on 'dev' subset | loss 4.471 | nll_loss 3.39 | ppl 10.48 | wps 27522.6 | wpb 3571.7 | bsz 11.1 | num_updates 6546 | best_loss 4.353\n",
            "2024-04-18 19:07:25 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)\n",
            "2024-04-18 19:07:25 | INFO | train | epoch 059 | loss 3.921 | nll_loss 2.743 | ppl 6.7 | wps 23924 | ups 6.43 | wpb 3721.2 | bsz 10.9 | num_updates 6546 | lr 0.000327335 | gnorm 0.595 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.6 | wall 1003\n",
            "2024-04-18 19:07:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:07:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 060:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:07:25 | INFO | fairseq.trainer | begin training epoch 60\n",
            "2024-04-18 19:07:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 060:  99% 110/111 [00:15<00:00,  7.67it/s, loss=3.906, nll_loss=2.724, ppl=6.61, wps=23293, ups=6.28, wpb=3711.9, bsz=11.1, num_updates=6600, lr=0.000330034, gnorm=0.602, clip=0, loss_scale=16, train_wall=9, gb_free=13.9, wall=1011]2024-04-18 19:07:41 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:07:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 060 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 060 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  5.32it/s]\u001b[A\n",
            "epoch 060 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.00it/s]\u001b[A\n",
            "epoch 060 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.82it/s]\u001b[A\n",
            "epoch 060 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.34it/s]\u001b[A\n",
            "epoch 060 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.85it/s]\u001b[A\n",
            "epoch 060 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.76it/s]\u001b[A\n",
            "epoch 060 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.42it/s]\u001b[A\n",
            "epoch 060 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.75it/s]\u001b[A\n",
            "epoch 060 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.45it/s]\u001b[A\n",
            "epoch 060 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.59it/s]\u001b[A\n",
            "epoch 060 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.20it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:07:42 | INFO | dev | epoch 060 | valid on 'dev' subset | loss 4.48 | nll_loss 3.397 | ppl 10.54 | wps 26524.9 | wpb 3571.7 | bsz 11.1 | num_updates 6657 | best_loss 4.353\n",
            "2024-04-18 19:07:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 6657 updates\n",
            "2024-04-18 19:07:42 | INFO | fairseq.trainer | Saving checkpoint to /content/SAVE_DIR_TRAINING/checkpoint60.pt\n",
            "2024-04-18 19:07:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/SAVE_DIR_TRAINING/checkpoint60.pt\n",
            "2024-04-18 19:07:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/SAVE_DIR_TRAINING/checkpoint60.pt (epoch 60 @ 6657 updates, score 4.48) (writing took 1.1611211419999563 seconds)\n",
            "2024-04-18 19:07:44 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)\n",
            "2024-04-18 19:07:44 | INFO | train | epoch 060 | loss 3.904 | nll_loss 2.72 | ppl 6.59 | wps 22443.9 | ups 6.03 | wpb 3721.2 | bsz 10.9 | num_updates 6657 | lr 0.000332883 | gnorm 0.611 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.5 | wall 1022\n",
            "2024-04-18 19:07:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:07:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 061:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:07:44 | INFO | fairseq.trainer | begin training epoch 61\n",
            "2024-04-18 19:07:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 061:  99% 110/111 [00:15<00:00,  7.26it/s, loss=3.901, nll_loss=2.716, ppl=6.57, wps=22327.8, ups=6.04, wpb=3693.7, bsz=11, num_updates=6700, lr=0.000335033, gnorm=0.617, clip=0, loss_scale=16, train_wall=9, gb_free=13.8, wall=1028]2024-04-18 19:07:59 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:07:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 061 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 061 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.79it/s]\u001b[A\n",
            "epoch 061 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.27it/s]\u001b[A\n",
            "epoch 061 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.19it/s]\u001b[A\n",
            "epoch 061 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.82it/s]\u001b[A\n",
            "epoch 061 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.23it/s]\u001b[A\n",
            "epoch 061 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.59it/s]\u001b[A\n",
            "epoch 061 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.78it/s]\u001b[A\n",
            "epoch 061 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.36it/s]\u001b[A\n",
            "epoch 061 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.09it/s]\u001b[A\n",
            "epoch 061 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.20it/s]\u001b[A\n",
            "epoch 061 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.73it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:08:01 | INFO | dev | epoch 061 | valid on 'dev' subset | loss 4.477 | nll_loss 3.392 | ppl 10.5 | wps 28026.5 | wpb 3571.7 | bsz 11.1 | num_updates 6768 | best_loss 4.353\n",
            "2024-04-18 19:08:01 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)\n",
            "2024-04-18 19:08:01 | INFO | train | epoch 061 | loss 3.89 | nll_loss 2.7 | ppl 6.5 | wps 23714.2 | ups 6.37 | wpb 3721.2 | bsz 10.9 | num_updates 6768 | lr 0.000338432 | gnorm 0.621 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.1 | wall 1039\n",
            "2024-04-18 19:08:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:08:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 062:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:08:01 | INFO | fairseq.trainer | begin training epoch 62\n",
            "2024-04-18 19:08:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 062:  99% 110/111 [00:15<00:00,  6.46it/s, loss=3.889, nll_loss=2.699, ppl=6.49, wps=23439.5, ups=6.22, wpb=3767.7, bsz=10.8, num_updates=6800, lr=0.000340032, gnorm=0.619, clip=0, loss_scale=16, train_wall=9, gb_free=12.6, wall=1044]2024-04-18 19:08:17 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:08:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 062 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 062 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.68it/s]\u001b[A\n",
            "epoch 062 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.20it/s]\u001b[A\n",
            "epoch 062 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.98it/s]\u001b[A\n",
            "epoch 062 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.43it/s]\u001b[A\n",
            "epoch 062 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.66it/s]\u001b[A\n",
            "epoch 062 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.82it/s]\u001b[A\n",
            "epoch 062 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.41it/s]\u001b[A\n",
            "epoch 062 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.86it/s]\u001b[A\n",
            "epoch 062 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.80it/s]\u001b[A\n",
            "epoch 062 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.92it/s]\u001b[A\n",
            "epoch 062 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.52it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:08:18 | INFO | dev | epoch 062 | valid on 'dev' subset | loss 4.501 | nll_loss 3.412 | ppl 10.65 | wps 27122.1 | wpb 3571.7 | bsz 11.1 | num_updates 6879 | best_loss 4.353\n",
            "2024-04-18 19:08:18 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)\n",
            "2024-04-18 19:08:18 | INFO | train | epoch 062 | loss 3.87 | nll_loss 2.673 | ppl 6.38 | wps 23746.7 | ups 6.38 | wpb 3721.2 | bsz 10.9 | num_updates 6879 | lr 0.000343981 | gnorm 0.923 | clip 0.9 | loss_scale 16 | train_wall 10 | gb_free 13.5 | wall 1056\n",
            "2024-04-18 19:08:18 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:08:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 063:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:08:18 | INFO | fairseq.trainer | begin training epoch 63\n",
            "2024-04-18 19:08:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 063:  99% 110/111 [00:15<00:00,  7.72it/s, loss=3.861, nll_loss=2.66, ppl=6.32, wps=23384.4, ups=6.34, wpb=3688.2, bsz=11, num_updates=6900, lr=0.000345031, gnorm=0.96, clip=1, loss_scale=16, train_wall=9, gb_free=13.5, wall=1059]2024-04-18 19:08:34 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:08:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 063 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 063 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  5.41it/s]\u001b[A\n",
            "epoch 063 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  5.95it/s]\u001b[A\n",
            "epoch 063 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.64it/s]\u001b[A\n",
            "epoch 063 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.11it/s]\u001b[A\n",
            "epoch 063 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.60it/s]\u001b[A\n",
            "epoch 063 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.19it/s]\u001b[A\n",
            "epoch 063 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.53it/s]\u001b[A\n",
            "epoch 063 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.03it/s]\u001b[A\n",
            "epoch 063 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.73it/s]\u001b[A\n",
            "epoch 063 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.80it/s]\u001b[A\n",
            "epoch 063 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.27it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:08:36 | INFO | dev | epoch 063 | valid on 'dev' subset | loss 4.522 | nll_loss 3.445 | ppl 10.89 | wps 26453.2 | wpb 3571.7 | bsz 11.1 | num_updates 6990 | best_loss 4.353\n",
            "2024-04-18 19:08:36 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)\n",
            "2024-04-18 19:08:36 | INFO | train | epoch 063 | loss 3.853 | nll_loss 2.649 | ppl 6.27 | wps 23950.4 | ups 6.44 | wpb 3721.2 | bsz 10.9 | num_updates 6990 | lr 0.00034953 | gnorm 0.639 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 14 | wall 1074\n",
            "2024-04-18 19:08:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:08:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 064:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:08:36 | INFO | fairseq.trainer | begin training epoch 64\n",
            "2024-04-18 19:08:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 064:  99% 110/111 [00:15<00:00,  7.59it/s, loss=3.837, nll_loss=2.626, ppl=6.17, wps=25974.3, ups=6.98, wpb=3723.9, bsz=10.9, num_updates=7100, lr=0.000355029, gnorm=0.713, clip=0, loss_scale=16, train_wall=9, gb_free=13.7, wall=1090]2024-04-18 19:08:52 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:08:52 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 064 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 064 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  5.12it/s]\u001b[A\n",
            "epoch 064 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  5.64it/s]\u001b[A\n",
            "epoch 064 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.43it/s]\u001b[A\n",
            "epoch 064 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  6.96it/s]\u001b[A\n",
            "epoch 064 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.52it/s]\u001b[A\n",
            "epoch 064 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  7.95it/s]\u001b[A\n",
            "epoch 064 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.50it/s]\u001b[A\n",
            "epoch 064 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.00it/s]\u001b[A\n",
            "epoch 064 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.59it/s]\u001b[A\n",
            "epoch 064 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.48it/s]\u001b[A\n",
            "epoch 064 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.57it/s]\u001b[A\n",
            "epoch 064 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.05it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:08:54 | INFO | dev | epoch 064 | valid on 'dev' subset | loss 4.525 | nll_loss 3.438 | ppl 10.84 | wps 25933.1 | wpb 3571.7 | bsz 11.1 | num_updates 7101 | best_loss 4.353\n",
            "2024-04-18 19:08:54 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)\n",
            "2024-04-18 19:08:54 | INFO | train | epoch 064 | loss 3.833 | nll_loss 2.621 | ppl 6.15 | wps 22964.7 | ups 6.17 | wpb 3721.2 | bsz 10.9 | num_updates 7101 | lr 0.000355079 | gnorm 0.706 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.6 | wall 1092\n",
            "2024-04-18 19:08:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:08:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 065:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:08:54 | INFO | fairseq.trainer | begin training epoch 65\n",
            "2024-04-18 19:08:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 065:  99% 110/111 [00:15<00:00,  7.01it/s, loss=3.806, nll_loss=2.584, ppl=5.99, wps=22752.8, ups=6.2, wpb=3667.2, bsz=10.7, num_updates=7200, lr=0.000360028, gnorm=0.675, clip=0, loss_scale=16, train_wall=9, gb_free=13.1, wall=1106]2024-04-18 19:09:10 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:09:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 065 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 065 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.66it/s]\u001b[A\n",
            "epoch 065 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.17it/s]\u001b[A\n",
            "epoch 065 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.70it/s]\u001b[A\n",
            "epoch 065 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.02it/s]\u001b[A\n",
            "epoch 065 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.57it/s]\u001b[A\n",
            "epoch 065 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  7.95it/s]\u001b[A\n",
            "epoch 065 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.45it/s]\u001b[A\n",
            "epoch 065 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  6.97it/s]\u001b[A\n",
            "epoch 065 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.77it/s]\u001b[A\n",
            "epoch 065 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.77it/s]\u001b[A\n",
            "epoch 065 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.00it/s]\u001b[A\n",
            "epoch 065 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.65it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:09:11 | INFO | dev | epoch 065 | valid on 'dev' subset | loss 4.533 | nll_loss 3.443 | ppl 10.88 | wps 26887.8 | wpb 3571.7 | bsz 11.1 | num_updates 7212 | best_loss 4.353\n",
            "2024-04-18 19:09:11 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)\n",
            "2024-04-18 19:09:11 | INFO | train | epoch 065 | loss 3.816 | nll_loss 2.596 | ppl 6.05 | wps 23326.9 | ups 6.27 | wpb 3721.2 | bsz 10.9 | num_updates 7212 | lr 0.000360628 | gnorm 0.669 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.1 | wall 1109\n",
            "2024-04-18 19:09:11 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:09:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 066:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:09:11 | INFO | fairseq.trainer | begin training epoch 66\n",
            "2024-04-18 19:09:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 066:  99% 110/111 [00:15<00:00,  8.23it/s, loss=3.799, nll_loss=2.573, ppl=5.95, wps=23356.5, ups=6.21, wpb=3762.4, bsz=11, num_updates=7300, lr=0.000365027, gnorm=0.676, clip=0, loss_scale=16, train_wall=9, gb_free=13.9, wall=1122]2024-04-18 19:09:27 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:09:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 066 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 066 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  5.39it/s]\u001b[A\n",
            "epoch 066 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.04it/s]\u001b[A\n",
            "epoch 066 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.86it/s]\u001b[A\n",
            "epoch 066 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.23it/s]\u001b[A\n",
            "epoch 066 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.84it/s]\u001b[A\n",
            "epoch 066 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.44it/s]\u001b[A\n",
            "epoch 066 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.53it/s]\u001b[A\n",
            "epoch 066 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.13it/s]\u001b[A\n",
            "epoch 066 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.94it/s]\u001b[A\n",
            "epoch 066 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.02it/s]\u001b[A\n",
            "epoch 066 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.47it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:09:29 | INFO | dev | epoch 066 | valid on 'dev' subset | loss 4.57 | nll_loss 3.495 | ppl 11.27 | wps 27141.7 | wpb 3571.7 | bsz 11.1 | num_updates 7323 | best_loss 4.353\n",
            "2024-04-18 19:09:29 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)\n",
            "2024-04-18 19:09:29 | INFO | train | epoch 066 | loss 3.795 | nll_loss 2.567 | ppl 5.93 | wps 23798.1 | ups 6.4 | wpb 3721.2 | bsz 10.9 | num_updates 7323 | lr 0.000366177 | gnorm 0.686 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 14 | wall 1127\n",
            "2024-04-18 19:09:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:09:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 067:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:09:29 | INFO | fairseq.trainer | begin training epoch 67\n",
            "2024-04-18 19:09:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 067:  99% 110/111 [00:15<00:00,  8.07it/s, loss=3.786, nll_loss=2.555, ppl=5.88, wps=24113.4, ups=6.36, wpb=3793.1, bsz=11.4, num_updates=7400, lr=0.000370026, gnorm=0.685, clip=0, loss_scale=16, train_wall=9, gb_free=12.8, wall=1138]2024-04-18 19:09:44 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:09:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 067 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 067 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.69it/s]\u001b[A\n",
            "epoch 067 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  5.89it/s]\u001b[A\n",
            "epoch 067 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.69it/s]\u001b[A\n",
            "epoch 067 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  6.99it/s]\u001b[A\n",
            "epoch 067 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.51it/s]\u001b[A\n",
            "epoch 067 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.04it/s]\u001b[A\n",
            "epoch 067 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.42it/s]\u001b[A\n",
            "epoch 067 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.07it/s]\u001b[A\n",
            "epoch 067 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.42it/s]\u001b[A\n",
            "epoch 067 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.24it/s]\u001b[A\n",
            "epoch 067 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.57it/s]\u001b[A\n",
            "epoch 067 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.17it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:09:46 | INFO | dev | epoch 067 | valid on 'dev' subset | loss 4.582 | nll_loss 3.506 | ppl 11.36 | wps 25845.5 | wpb 3571.7 | bsz 11.1 | num_updates 7434 | best_loss 4.353\n",
            "2024-04-18 19:09:46 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)\n",
            "2024-04-18 19:09:46 | INFO | train | epoch 067 | loss 3.777 | nll_loss 2.543 | ppl 5.83 | wps 24109.4 | ups 6.48 | wpb 3721.2 | bsz 10.9 | num_updates 7434 | lr 0.000371726 | gnorm 0.691 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.7 | wall 1144\n",
            "2024-04-18 19:09:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:09:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 068:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:09:46 | INFO | fairseq.trainer | begin training epoch 68\n",
            "2024-04-18 19:09:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 068:  99% 110/111 [00:15<00:00,  7.99it/s, loss=3.761, nll_loss=2.52, ppl=5.74, wps=23765.6, ups=6.46, wpb=3678.7, bsz=10.6, num_updates=7500, lr=0.000375025, gnorm=0.71, clip=0, loss_scale=16, train_wall=9, gb_free=13.1, wall=1153]2024-04-18 19:10:01 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:10:01 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 068 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 068 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  6.19it/s]\u001b[A\n",
            "epoch 068 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.67it/s]\u001b[A\n",
            "epoch 068 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.40it/s]\u001b[A\n",
            "epoch 068 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.79it/s]\u001b[A\n",
            "epoch 068 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.32it/s]\u001b[A\n",
            "epoch 068 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.29it/s]\u001b[A\n",
            "epoch 068 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.91it/s]\u001b[A\n",
            "epoch 068 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.41it/s]\u001b[A\n",
            "epoch 068 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.32it/s]\u001b[A\n",
            "epoch 068 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.44it/s]\u001b[A\n",
            "epoch 068 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.94it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:10:03 | INFO | dev | epoch 068 | valid on 'dev' subset | loss 4.598 | nll_loss 3.519 | ppl 11.46 | wps 28837.7 | wpb 3571.7 | bsz 11.1 | num_updates 7545 | best_loss 4.353\n",
            "2024-04-18 19:10:03 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)\n",
            "2024-04-18 19:10:03 | INFO | train | epoch 068 | loss 3.759 | nll_loss 2.516 | ppl 5.72 | wps 24174.8 | ups 6.5 | wpb 3721.2 | bsz 10.9 | num_updates 7545 | lr 0.000377275 | gnorm 0.71 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.8 | wall 1161\n",
            "2024-04-18 19:10:03 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:10:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 069:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:10:03 | INFO | fairseq.trainer | begin training epoch 69\n",
            "2024-04-18 19:10:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 069:  99% 110/111 [00:15<00:00,  6.99it/s, loss=3.729, nll_loss=2.476, ppl=5.56, wps=23269.4, ups=6.45, wpb=3606.3, bsz=10.5, num_updates=7600, lr=0.000380024, gnorm=0.728, clip=0, loss_scale=16, train_wall=9, gb_free=14, wall=1169]2024-04-18 19:10:19 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:10:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 069 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 069 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.91it/s]\u001b[A\n",
            "epoch 069 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.42it/s]\u001b[A\n",
            "epoch 069 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.14it/s]\u001b[A\n",
            "epoch 069 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.48it/s]\u001b[A\n",
            "epoch 069 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.98it/s]\u001b[A\n",
            "epoch 069 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.02it/s]\u001b[A\n",
            "epoch 069 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.55it/s]\u001b[A\n",
            "epoch 069 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.10it/s]\u001b[A\n",
            "epoch 069 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.00it/s]\u001b[A\n",
            "epoch 069 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.96it/s]\u001b[A\n",
            "epoch 069 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.34it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:10:20 | INFO | dev | epoch 069 | valid on 'dev' subset | loss 4.588 | nll_loss 3.521 | ppl 11.48 | wps 27300.5 | wpb 3571.7 | bsz 11.1 | num_updates 7656 | best_loss 4.353\n",
            "2024-04-18 19:10:20 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)\n",
            "2024-04-18 19:10:20 | INFO | train | epoch 069 | loss 3.738 | nll_loss 2.487 | ppl 5.6 | wps 23772.4 | ups 6.39 | wpb 3721.2 | bsz 10.9 | num_updates 7656 | lr 0.000382823 | gnorm 0.763 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.8 | wall 1178\n",
            "2024-04-18 19:10:20 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:10:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 070:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:10:20 | INFO | fairseq.trainer | begin training epoch 70\n",
            "2024-04-18 19:10:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 070:  99% 110/111 [00:15<00:00,  7.66it/s, loss=3.755, nll_loss=2.51, ppl=5.7, wps=24003.6, ups=6.15, wpb=3901.9, bsz=11.7, num_updates=7700, lr=0.000385023, gnorm=0.747, clip=0, loss_scale=16, train_wall=9, gb_free=12.6, wall=1185]2024-04-18 19:10:36 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:10:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 070 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 070 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  4.88it/s]\u001b[A\n",
            "epoch 070 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  5.83it/s]\u001b[A\n",
            "epoch 070 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.75it/s]\u001b[A\n",
            "epoch 070 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.28it/s]\u001b[A\n",
            "epoch 070 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.83it/s]\u001b[A\n",
            "epoch 070 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.44it/s]\u001b[A\n",
            "epoch 070 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.80it/s]\u001b[A\n",
            "epoch 070 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.40it/s]\u001b[A\n",
            "epoch 070 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.23it/s]\u001b[A\n",
            "epoch 070 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.22it/s]\u001b[A\n",
            "epoch 070 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.84it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:10:38 | INFO | dev | epoch 070 | valid on 'dev' subset | loss 4.625 | nll_loss 3.546 | ppl 11.68 | wps 28167.2 | wpb 3571.7 | bsz 11.1 | num_updates 7767 | best_loss 4.353\n",
            "2024-04-18 19:10:38 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)\n",
            "2024-04-18 19:10:38 | INFO | train | epoch 070 | loss 3.719 | nll_loss 2.461 | ppl 5.51 | wps 23658.7 | ups 6.36 | wpb 3721.2 | bsz 10.9 | num_updates 7767 | lr 0.000388372 | gnorm 0.731 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.6 | wall 1196\n",
            "2024-04-18 19:10:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:10:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 071:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:10:38 | INFO | fairseq.trainer | begin training epoch 71\n",
            "2024-04-18 19:10:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 071:  99% 110/111 [00:15<00:00,  7.58it/s, loss=3.717, nll_loss=2.457, ppl=5.49, wps=23789.9, ups=6.29, wpb=3780, bsz=11.3, num_updates=7800, lr=0.000390022, gnorm=0.731, clip=0, loss_scale=16, train_wall=9, gb_free=13.1, wall=1201]2024-04-18 19:10:53 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:10:53 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 071 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 071 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.66it/s]\u001b[A\n",
            "epoch 071 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.17it/s]\u001b[A\n",
            "epoch 071 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.98it/s]\u001b[A\n",
            "epoch 071 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.29it/s]\u001b[A\n",
            "epoch 071 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.66it/s]\u001b[A\n",
            "epoch 071 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.13it/s]\u001b[A\n",
            "epoch 071 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.69it/s]\u001b[A\n",
            "epoch 071 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.16it/s]\u001b[A\n",
            "epoch 071 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.89it/s]\u001b[A\n",
            "epoch 071 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.02it/s]\u001b[A\n",
            "epoch 071 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.63it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:10:55 | INFO | dev | epoch 071 | valid on 'dev' subset | loss 4.659 | nll_loss 3.584 | ppl 11.99 | wps 27223 | wpb 3571.7 | bsz 11.1 | num_updates 7878 | best_loss 4.353\n",
            "2024-04-18 19:10:55 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)\n",
            "2024-04-18 19:10:55 | INFO | train | epoch 071 | loss 3.7 | nll_loss 2.435 | ppl 5.41 | wps 24207.5 | ups 6.51 | wpb 3721.2 | bsz 10.9 | num_updates 7878 | lr 0.000393921 | gnorm 0.752 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.9 | wall 1213\n",
            "2024-04-18 19:10:55 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:10:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 072:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:10:55 | INFO | fairseq.trainer | begin training epoch 72\n",
            "2024-04-18 19:10:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 072:  99% 110/111 [00:15<00:00,  7.64it/s, loss=3.677, nll_loss=2.404, ppl=5.29, wps=23520, ups=6.58, wpb=3575.5, bsz=10.1, num_updates=7900, lr=0.000395021, gnorm=0.804, clip=0, loss_scale=16, train_wall=9, gb_free=13.9, wall=1216]2024-04-18 19:11:10 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:11:10 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 072 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 072 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  5.42it/s]\u001b[A\n",
            "epoch 072 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.22it/s]\u001b[A\n",
            "epoch 072 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.03it/s]\u001b[A\n",
            "epoch 072 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.60it/s]\u001b[A\n",
            "epoch 072 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.01it/s]\u001b[A\n",
            "epoch 072 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.49it/s]\u001b[A\n",
            "epoch 072 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.75it/s]\u001b[A\n",
            "epoch 072 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.09it/s]\u001b[A\n",
            "epoch 072 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.96it/s]\u001b[A\n",
            "epoch 072 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.06it/s]\u001b[A\n",
            "epoch 072 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.63it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:11:12 | INFO | dev | epoch 072 | valid on 'dev' subset | loss 4.672 | nll_loss 3.609 | ppl 12.21 | wps 27685.7 | wpb 3571.7 | bsz 11.1 | num_updates 7989 | best_loss 4.353\n",
            "2024-04-18 19:11:12 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)\n",
            "2024-04-18 19:11:12 | INFO | train | epoch 072 | loss 3.682 | nll_loss 2.409 | ppl 5.31 | wps 24152.4 | ups 6.49 | wpb 3721.2 | bsz 10.9 | num_updates 7989 | lr 0.00039947 | gnorm 0.786 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.4 | wall 1230\n",
            "2024-04-18 19:11:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:11:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 073:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:11:12 | INFO | fairseq.trainer | begin training epoch 73\n",
            "2024-04-18 19:11:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 073:  99% 110/111 [00:15<00:00,  7.20it/s, loss=3.671, nll_loss=2.395, ppl=5.26, wps=23595.1, ups=6.46, wpb=3654.1, bsz=10.8, num_updates=8000, lr=0.00040002, gnorm=0.763, clip=0, loss_scale=16, train_wall=9, gb_free=13.8, wall=1231]2024-04-18 19:11:27 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:11:27 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 073 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 073 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  5.43it/s]\u001b[A\n",
            "epoch 073 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.14it/s]\u001b[A\n",
            "epoch 073 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.05it/s]\u001b[A\n",
            "epoch 073 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.49it/s]\u001b[A\n",
            "epoch 073 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.87it/s]\u001b[A\n",
            "epoch 073 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.35it/s]\u001b[A\n",
            "epoch 073 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.80it/s]\u001b[A\n",
            "epoch 073 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.17it/s]\u001b[A\n",
            "epoch 073 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.86it/s]\u001b[A\n",
            "epoch 073 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.93it/s]\u001b[A\n",
            "epoch 073 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.46it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:11:29 | INFO | dev | epoch 073 | valid on 'dev' subset | loss 4.671 | nll_loss 3.608 | ppl 12.2 | wps 27282.2 | wpb 3571.7 | bsz 11.1 | num_updates 8100 | best_loss 4.353\n",
            "2024-04-18 19:11:29 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)\n",
            "2024-04-18 19:11:29 | INFO | train | epoch 073 | loss 3.664 | nll_loss 2.383 | ppl 5.22 | wps 24114.9 | ups 6.48 | wpb 3721.2 | bsz 10.9 | num_updates 8100 | lr 0.000405019 | gnorm 0.776 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.3 | wall 1247\n",
            "2024-04-18 19:11:29 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:11:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 074:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:11:29 | INFO | fairseq.trainer | begin training epoch 74\n",
            "2024-04-18 19:11:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 074:  99% 110/111 [00:15<00:00,  6.46it/s, loss=3.636, nll_loss=2.345, ppl=5.08, wps=23661.1, ups=6.36, wpb=3719.9, bsz=10.9, num_updates=8200, lr=0.000410018, gnorm=0.787, clip=0, loss_scale=16, train_wall=9, gb_free=11.8, wall=1261]2024-04-18 19:11:44 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:11:44 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 074 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 074 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.69it/s]\u001b[A\n",
            "epoch 074 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.08it/s]\u001b[A\n",
            "epoch 074 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.77it/s]\u001b[A\n",
            "epoch 074 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.46it/s]\u001b[A\n",
            "epoch 074 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.83it/s]\u001b[A\n",
            "epoch 074 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.33it/s]\u001b[A\n",
            "epoch 074 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.48it/s]\u001b[A\n",
            "epoch 074 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.99it/s]\u001b[A\n",
            "epoch 074 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.86it/s]\u001b[A\n",
            "epoch 074 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.00it/s]\u001b[A\n",
            "epoch 074 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.56it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:11:46 | INFO | dev | epoch 074 | valid on 'dev' subset | loss 4.674 | nll_loss 3.619 | ppl 12.29 | wps 26984.1 | wpb 3571.7 | bsz 11.1 | num_updates 8211 | best_loss 4.353\n",
            "2024-04-18 19:11:46 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)\n",
            "2024-04-18 19:11:46 | INFO | train | epoch 074 | loss 3.643 | nll_loss 2.355 | ppl 5.12 | wps 24059 | ups 6.47 | wpb 3721.2 | bsz 10.9 | num_updates 8211 | lr 0.000410568 | gnorm 0.787 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.7 | wall 1264\n",
            "2024-04-18 19:11:46 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:11:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 075:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:11:46 | INFO | fairseq.trainer | begin training epoch 75\n",
            "2024-04-18 19:11:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 075:  99% 110/111 [00:15<00:00,  6.62it/s, loss=3.617, nll_loss=2.32, ppl=4.99, wps=23580.9, ups=6.39, wpb=3688.3, bsz=10.9, num_updates=8300, lr=0.000415017, gnorm=0.785, clip=0, loss_scale=16, train_wall=9, gb_free=12.6, wall=1277]2024-04-18 19:12:02 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:12:02 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 075 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 075 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.66it/s]\u001b[A\n",
            "epoch 075 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.05it/s]\u001b[A\n",
            "epoch 075 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.91it/s]\u001b[A\n",
            "epoch 075 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.38it/s]\u001b[A\n",
            "epoch 075 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.01it/s]\u001b[A\n",
            "epoch 075 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.55it/s]\u001b[A\n",
            "epoch 075 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.86it/s]\u001b[A\n",
            "epoch 075 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.36it/s]\u001b[A\n",
            "epoch 075 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.05it/s]\u001b[A\n",
            "epoch 075 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.10it/s]\u001b[A\n",
            "epoch 075 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.40it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:12:04 | INFO | dev | epoch 075 | valid on 'dev' subset | loss 4.697 | nll_loss 3.635 | ppl 12.43 | wps 27356.9 | wpb 3571.7 | bsz 11.1 | num_updates 8322 | best_loss 4.353\n",
            "2024-04-18 19:12:04 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)\n",
            "2024-04-18 19:12:04 | INFO | train | epoch 075 | loss 3.624 | nll_loss 2.327 | ppl 5.02 | wps 23815.8 | ups 6.4 | wpb 3721.2 | bsz 10.9 | num_updates 8322 | lr 0.000416117 | gnorm 0.787 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.1 | wall 1282\n",
            "2024-04-18 19:12:04 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:12:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 076:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:12:04 | INFO | fairseq.trainer | begin training epoch 76\n",
            "2024-04-18 19:12:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 076:  99% 110/111 [00:15<00:00,  6.91it/s, loss=3.615, nll_loss=2.314, ppl=4.97, wps=23546.2, ups=6.26, wpb=3760.3, bsz=11, num_updates=8400, lr=0.000420016, gnorm=0.799, clip=0, loss_scale=16, train_wall=9, gb_free=12.3, wall=1293]2024-04-18 19:12:19 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:12:19 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 076 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 076 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  6.24it/s]\u001b[A\n",
            "epoch 076 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.73it/s]\u001b[A\n",
            "epoch 076 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.52it/s]\u001b[A\n",
            "epoch 076 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.74it/s]\u001b[A\n",
            "epoch 076 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.10it/s]\u001b[A\n",
            "epoch 076 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.52it/s]\u001b[A\n",
            "epoch 076 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.88it/s]\u001b[A\n",
            "epoch 076 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.47it/s]\u001b[A\n",
            "epoch 076 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.11it/s]\u001b[A\n",
            "epoch 076 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.27it/s]\u001b[A\n",
            "epoch 076 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.76it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:12:21 | INFO | dev | epoch 076 | valid on 'dev' subset | loss 4.698 | nll_loss 3.644 | ppl 12.5 | wps 28119.1 | wpb 3571.7 | bsz 11.1 | num_updates 8433 | best_loss 4.353\n",
            "2024-04-18 19:12:21 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)\n",
            "2024-04-18 19:12:21 | INFO | train | epoch 076 | loss 3.604 | nll_loss 2.3 | ppl 4.93 | wps 24129.2 | ups 6.48 | wpb 3721.2 | bsz 10.9 | num_updates 8433 | lr 0.000421666 | gnorm 0.802 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.2 | wall 1299\n",
            "2024-04-18 19:12:21 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:12:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 077:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:12:21 | INFO | fairseq.trainer | begin training epoch 77\n",
            "2024-04-18 19:12:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 077:  99% 110/111 [00:15<00:00,  6.54it/s, loss=3.579, nll_loss=2.266, ppl=4.81, wps=23764.8, ups=6.5, wpb=3655.4, bsz=10.4, num_updates=8500, lr=0.000425015, gnorm=0.814, clip=0, loss_scale=16, train_wall=9, gb_free=12.1, wall=1308]2024-04-18 19:12:36 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:12:36 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 077 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 077 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.58it/s]\u001b[A\n",
            "epoch 077 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.29it/s]\u001b[A\n",
            "epoch 077 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.03it/s]\u001b[A\n",
            "epoch 077 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.46it/s]\u001b[A\n",
            "epoch 077 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.92it/s]\u001b[A\n",
            "epoch 077 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.47it/s]\u001b[A\n",
            "epoch 077 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.71it/s]\u001b[A\n",
            "epoch 077 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.14it/s]\u001b[A\n",
            "epoch 077 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.89it/s]\u001b[A\n",
            "epoch 077 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.96it/s]\u001b[A\n",
            "epoch 077 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.47it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:12:38 | INFO | dev | epoch 077 | valid on 'dev' subset | loss 4.719 | nll_loss 3.668 | ppl 12.71 | wps 27286.1 | wpb 3571.7 | bsz 11.1 | num_updates 8544 | best_loss 4.353\n",
            "2024-04-18 19:12:38 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)\n",
            "2024-04-18 19:12:38 | INFO | train | epoch 077 | loss 3.587 | nll_loss 2.276 | ppl 4.84 | wps 23824.2 | ups 6.4 | wpb 3721.2 | bsz 10.9 | num_updates 8544 | lr 0.000427215 | gnorm 0.809 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.9 | wall 1316\n",
            "2024-04-18 19:12:38 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:12:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 078:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:12:38 | INFO | fairseq.trainer | begin training epoch 78\n",
            "2024-04-18 19:12:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 078:  99% 110/111 [00:15<00:00,  7.95it/s, loss=3.57, nll_loss=2.253, ppl=4.77, wps=23137.8, ups=6.24, wpb=3708.3, bsz=11.1, num_updates=8600, lr=0.000430014, gnorm=0.816, clip=0, loss_scale=16, train_wall=9, gb_free=12.9, wall=1324]2024-04-18 19:12:54 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:12:54 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 078 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 078 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.62it/s]\u001b[A\n",
            "epoch 078 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.15it/s]\u001b[A\n",
            "epoch 078 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.87it/s]\u001b[A\n",
            "epoch 078 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.31it/s]\u001b[A\n",
            "epoch 078 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.84it/s]\u001b[A\n",
            "epoch 078 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.31it/s]\u001b[A\n",
            "epoch 078 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.58it/s]\u001b[A\n",
            "epoch 078 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.11it/s]\u001b[A\n",
            "epoch 078 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.78it/s]\u001b[A\n",
            "epoch 078 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.90it/s]\u001b[A\n",
            "epoch 078 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.46it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:12:56 | INFO | dev | epoch 078 | valid on 'dev' subset | loss 4.759 | nll_loss 3.691 | ppl 12.92 | wps 26947.6 | wpb 3571.7 | bsz 11.1 | num_updates 8655 | best_loss 4.353\n",
            "2024-04-18 19:12:56 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)\n",
            "2024-04-18 19:12:56 | INFO | train | epoch 078 | loss 3.57 | nll_loss 2.253 | ppl 4.77 | wps 23127.7 | ups 6.22 | wpb 3721.2 | bsz 10.9 | num_updates 8655 | lr 0.000432763 | gnorm 0.821 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.2 | wall 1334\n",
            "2024-04-18 19:12:56 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:12:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 079:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:12:56 | INFO | fairseq.trainer | begin training epoch 79\n",
            "2024-04-18 19:12:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 079:  99% 110/111 [00:16<00:00,  7.21it/s, loss=3.568, nll_loss=2.25, ppl=4.76, wps=22867.7, ups=6.1, wpb=3751.5, bsz=11.1, num_updates=8700, lr=0.000435013, gnorm=0.812, clip=0, loss_scale=16, train_wall=9, gb_free=13.3, wall=1340]2024-04-18 19:13:12 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:13:12 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 079 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 079 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  5.39it/s]\u001b[A\n",
            "epoch 079 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  5.97it/s]\u001b[A\n",
            "epoch 079 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.67it/s]\u001b[A\n",
            "epoch 079 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.16it/s]\u001b[A\n",
            "epoch 079 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.66it/s]\u001b[A\n",
            "epoch 079 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.19it/s]\u001b[A\n",
            "epoch 079 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.60it/s]\u001b[A\n",
            "epoch 079 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.27it/s]\u001b[A\n",
            "epoch 079 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.83it/s]\u001b[A\n",
            "epoch 079 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.73it/s]\u001b[A\n",
            "epoch 079 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.76it/s]\u001b[A\n",
            "epoch 079 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.26it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:13:14 | INFO | dev | epoch 079 | valid on 'dev' subset | loss 4.752 | nll_loss 3.693 | ppl 12.93 | wps 26715.9 | wpb 3571.7 | bsz 11.1 | num_updates 8766 | best_loss 4.353\n",
            "2024-04-18 19:13:14 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)\n",
            "2024-04-18 19:13:14 | INFO | train | epoch 079 | loss 3.549 | nll_loss 2.222 | ppl 4.67 | wps 22972.7 | ups 6.17 | wpb 3721.2 | bsz 10.9 | num_updates 8766 | lr 0.000438312 | gnorm 0.82 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 13.7 | wall 1352\n",
            "2024-04-18 19:13:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:13:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 080:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:13:14 | INFO | fairseq.trainer | begin training epoch 80\n",
            "2024-04-18 19:13:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 080:  99% 110/111 [00:15<00:00,  7.83it/s, loss=3.561, nll_loss=2.24, ppl=4.72, wps=22959.2, ups=6.01, wpb=3822.5, bsz=11.1, num_updates=8800, lr=0.000440012, gnorm=0.818, clip=0, loss_scale=16, train_wall=9, gb_free=13.4, wall=1357]2024-04-18 19:13:30 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:13:30 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 080 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 080 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.78it/s]\u001b[A\n",
            "epoch 080 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.15it/s]\u001b[A\n",
            "epoch 080 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.87it/s]\u001b[A\n",
            "epoch 080 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.19it/s]\u001b[A\n",
            "epoch 080 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.59it/s]\u001b[A\n",
            "epoch 080 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.68it/s]\u001b[A\n",
            "epoch 080 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.29it/s]\u001b[A\n",
            "epoch 080 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.77it/s]\u001b[A\n",
            "epoch 080 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.53it/s]\u001b[A\n",
            "epoch 080 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.60it/s]\u001b[A\n",
            "epoch 080 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.09it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:13:32 | INFO | dev | epoch 080 | valid on 'dev' subset | loss 4.815 | nll_loss 3.763 | ppl 13.57 | wps 26128.3 | wpb 3571.7 | bsz 11.1 | num_updates 8877 | best_loss 4.353\n",
            "2024-04-18 19:13:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 8877 updates\n",
            "2024-04-18 19:13:32 | INFO | fairseq.trainer | Saving checkpoint to /content/SAVE_DIR_TRAINING/checkpoint80.pt\n",
            "2024-04-18 19:13:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/SAVE_DIR_TRAINING/checkpoint80.pt\n",
            "2024-04-18 19:13:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/SAVE_DIR_TRAINING/checkpoint80.pt (epoch 80 @ 8877 updates, score 4.815) (writing took 1.138214543000231 seconds)\n",
            "2024-04-18 19:13:33 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)\n",
            "2024-04-18 19:13:33 | INFO | train | epoch 080 | loss 3.528 | nll_loss 2.195 | ppl 4.58 | wps 21625.6 | ups 5.81 | wpb 3721.2 | bsz 10.9 | num_updates 8877 | lr 0.000443861 | gnorm 0.844 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 14 | wall 1371\n",
            "2024-04-18 19:13:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:13:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 081:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:13:33 | INFO | fairseq.trainer | begin training epoch 81\n",
            "2024-04-18 19:13:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 081:  98% 109/111 [00:15<00:00,  6.44it/s, loss=3.509, nll_loss=2.168, ppl=4.49, wps=21111.5, ups=5.84, wpb=3614.7, bsz=10.7, num_updates=8900, lr=0.000445011, gnorm=0.862, clip=0, loss_scale=16, train_wall=9, gb_free=13.2, wall=1374]2024-04-18 19:13:48 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:13:48 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 081 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 081 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  6.04it/s]\u001b[A\n",
            "epoch 081 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.42it/s]\u001b[A\n",
            "epoch 081 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.19it/s]\u001b[A\n",
            "epoch 081 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.73it/s]\u001b[A\n",
            "epoch 081 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.21it/s]\u001b[A\n",
            "epoch 081 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.65it/s]\u001b[A\n",
            "epoch 081 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.97it/s]\u001b[A\n",
            "epoch 081 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.49it/s]\u001b[A\n",
            "epoch 081 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.24it/s]\u001b[A\n",
            "epoch 081 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.36it/s]\u001b[A\n",
            "epoch 081 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.93it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:13:50 | INFO | dev | epoch 081 | valid on 'dev' subset | loss 4.799 | nll_loss 3.744 | ppl 13.4 | wps 28417.2 | wpb 3571.7 | bsz 11.1 | num_updates 8988 | best_loss 4.353\n",
            "2024-04-18 19:13:50 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)\n",
            "2024-04-18 19:13:50 | INFO | train | epoch 081 | loss 3.516 | nll_loss 2.177 | ppl 4.52 | wps 24113.7 | ups 6.48 | wpb 3721.2 | bsz 10.9 | num_updates 8988 | lr 0.00044941 | gnorm 0.887 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.7 | wall 1388\n",
            "2024-04-18 19:13:50 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:13:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 082:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:13:50 | INFO | fairseq.trainer | begin training epoch 82\n",
            "2024-04-18 19:13:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 082:  99% 110/111 [00:15<00:00,  7.11it/s, loss=3.522, nll_loss=2.185, ppl=4.55, wps=24037.6, ups=6.42, wpb=3741.9, bsz=10.9, num_updates=9000, lr=0.00045001, gnorm=0.886, clip=0, loss_scale=16, train_wall=9, gb_free=13.7, wall=1390]2024-04-18 19:14:06 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:14:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 082 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 082 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.55it/s]\u001b[A\n",
            "epoch 082 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.19it/s]\u001b[A\n",
            "epoch 082 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.89it/s]\u001b[A\n",
            "epoch 082 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.43it/s]\u001b[A\n",
            "epoch 082 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.05it/s]\u001b[A\n",
            "epoch 082 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.09it/s]\u001b[A\n",
            "epoch 082 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.63it/s]\u001b[A\n",
            "epoch 082 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.05it/s]\u001b[A\n",
            "epoch 082 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.85it/s]\u001b[A\n",
            "epoch 082 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.91it/s]\u001b[A\n",
            "epoch 082 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.45it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:14:07 | INFO | dev | epoch 082 | valid on 'dev' subset | loss 4.811 | nll_loss 3.769 | ppl 13.64 | wps 27414.2 | wpb 3571.7 | bsz 11.1 | num_updates 9099 | best_loss 4.353\n",
            "2024-04-18 19:14:07 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)\n",
            "2024-04-18 19:14:07 | INFO | train | epoch 082 | loss 3.497 | nll_loss 2.151 | ppl 4.44 | wps 24023.2 | ups 6.46 | wpb 3721.2 | bsz 10.9 | num_updates 9099 | lr 0.000454959 | gnorm 0.864 | clip 0 | loss_scale 16 | train_wall 10 | gb_free 12.7 | wall 1405\n",
            "2024-04-18 19:14:07 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:14:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 083:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:14:07 | INFO | fairseq.trainer | begin training epoch 83\n",
            "2024-04-18 19:14:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 083:  99% 110/111 [00:15<00:00,  6.89it/s, loss=3.475, nll_loss=2.122, ppl=4.35, wps=26203.9, ups=7.06, wpb=3712.8, bsz=10.8, num_updates=9200, lr=0.000460008, gnorm=1.045, clip=1, loss_scale=16, train_wall=9, gb_free=12.6, wall=1420]2024-04-18 19:14:23 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:14:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 083 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 083 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  5.48it/s]\u001b[A\n",
            "epoch 083 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.21it/s]\u001b[A\n",
            "epoch 083 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.04it/s]\u001b[A\n",
            "epoch 083 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.42it/s]\u001b[A\n",
            "epoch 083 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.86it/s]\u001b[A\n",
            "epoch 083 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.05it/s]\u001b[A\n",
            "epoch 083 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.51it/s]\u001b[A\n",
            "epoch 083 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.09it/s]\u001b[A\n",
            "epoch 083 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.90it/s]\u001b[A\n",
            "epoch 083 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.08it/s]\u001b[A\n",
            "epoch 083 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.67it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:14:25 | INFO | dev | epoch 083 | valid on 'dev' subset | loss 4.801 | nll_loss 3.747 | ppl 13.42 | wps 27363.2 | wpb 3571.7 | bsz 11.1 | num_updates 9210 | best_loss 4.353\n",
            "2024-04-18 19:14:25 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)\n",
            "2024-04-18 19:14:25 | INFO | train | epoch 083 | loss 3.48 | nll_loss 2.128 | ppl 4.37 | wps 23429.4 | ups 6.3 | wpb 3721.2 | bsz 10.9 | num_updates 9210 | lr 0.000460508 | gnorm 1.026 | clip 0.9 | loss_scale 16 | train_wall 10 | gb_free 13.6 | wall 1423\n",
            "2024-04-18 19:14:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:14:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 084:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:14:25 | INFO | fairseq.trainer | begin training epoch 84\n",
            "2024-04-18 19:14:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 084:  85% 94/111 [00:13<00:01,  8.68it/s, loss=3.461, nll_loss=2.1, ppl=4.29, wps=23539.2, ups=6.34, wpb=3713.9, bsz=11, num_updates=9300, lr=0.000465007, gnorm=0.869, clip=0, loss_scale=16, train_wall=9, gb_free=13.3, wall=1436]2024-04-18 19:14:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
            "epoch 084:  99% 110/111 [00:15<00:00,  7.94it/s, loss=3.461, nll_loss=2.1, ppl=4.29, wps=23539.2, ups=6.34, wpb=3713.9, bsz=11, num_updates=9300, lr=0.000465007, gnorm=0.869, clip=0, loss_scale=16, train_wall=9, gb_free=13.3, wall=1436]2024-04-18 19:14:40 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:14:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 084 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 084 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.60it/s]\u001b[A\n",
            "epoch 084 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.25it/s]\u001b[A\n",
            "epoch 084 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.87it/s]\u001b[A\n",
            "epoch 084 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.45it/s]\u001b[A\n",
            "epoch 084 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.85it/s]\u001b[A\n",
            "epoch 084 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.27it/s]\u001b[A\n",
            "epoch 084 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.73it/s]\u001b[A\n",
            "epoch 084 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.14it/s]\u001b[A\n",
            "epoch 084 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.78it/s]\u001b[A\n",
            "epoch 084 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.73it/s]\u001b[A\n",
            "epoch 084 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.85it/s]\u001b[A\n",
            "epoch 084 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.53it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:14:42 | INFO | dev | epoch 084 | valid on 'dev' subset | loss 4.858 | nll_loss 3.813 | ppl 14.06 | wps 27170.4 | wpb 3571.7 | bsz 11.1 | num_updates 9320 | best_loss 4.353\n",
            "2024-04-18 19:14:42 | INFO | fairseq_cli.train | end of epoch 84 (average epoch stats below)\n",
            "2024-04-18 19:14:42 | INFO | train | epoch 084 | loss 3.465 | nll_loss 2.106 | ppl 4.31 | wps 23763.7 | ups 6.38 | wpb 3726.6 | bsz 10.9 | num_updates 9320 | lr 0.000466007 | gnorm 0.869 | clip 0 | loss_scale 8 | train_wall 10 | gb_free 12.7 | wall 1440\n",
            "2024-04-18 19:14:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:14:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 085:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:14:42 | INFO | fairseq.trainer | begin training epoch 85\n",
            "2024-04-18 19:14:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 085:  99% 110/111 [00:15<00:00,  6.84it/s, loss=3.467, nll_loss=2.107, ppl=4.31, wps=23983.3, ups=6.32, wpb=3792, bsz=11.3, num_updates=9400, lr=0.000470006, gnorm=0.863, clip=0, loss_scale=8, train_wall=9, gb_free=13.1, wall=1451]2024-04-18 19:14:58 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:14:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 085 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 085 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.51it/s]\u001b[A\n",
            "epoch 085 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.13it/s]\u001b[A\n",
            "epoch 085 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.85it/s]\u001b[A\n",
            "epoch 085 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.50it/s]\u001b[A\n",
            "epoch 085 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.83it/s]\u001b[A\n",
            "epoch 085 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.34it/s]\u001b[A\n",
            "epoch 085 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.67it/s]\u001b[A\n",
            "epoch 085 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.15it/s]\u001b[A\n",
            "epoch 085 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.08it/s]\u001b[A\n",
            "epoch 085 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.05it/s]\u001b[A\n",
            "epoch 085 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.50it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:14:59 | INFO | dev | epoch 085 | valid on 'dev' subset | loss 4.846 | nll_loss 3.803 | ppl 13.96 | wps 27313.4 | wpb 3571.7 | bsz 11.1 | num_updates 9431 | best_loss 4.353\n",
            "2024-04-18 19:14:59 | INFO | fairseq_cli.train | end of epoch 85 (average epoch stats below)\n",
            "2024-04-18 19:14:59 | INFO | train | epoch 085 | loss 3.448 | nll_loss 2.083 | ppl 4.24 | wps 24150.6 | ups 6.49 | wpb 3721.2 | bsz 10.9 | num_updates 9431 | lr 0.000471556 | gnorm 0.883 | clip 0 | loss_scale 8 | train_wall 10 | gb_free 13.1 | wall 1457\n",
            "2024-04-18 19:14:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:14:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 086:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:14:59 | INFO | fairseq.trainer | begin training epoch 86\n",
            "2024-04-18 19:14:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 086:  44% 49/111 [00:06<00:08,  7.14it/s]2024-04-18 19:15:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0\n",
            "epoch 086:  99% 110/111 [00:15<00:00,  6.54it/s, loss=3.422, nll_loss=2.047, ppl=4.13, wps=23739.3, ups=6.44, wpb=3686.8, bsz=10.4, num_updates=9500, lr=0.000475005, gnorm=0.926, clip=0, loss_scale=4, train_wall=9, gb_free=13.7, wall=1467]2024-04-18 19:15:15 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:15:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 086 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 086 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.91it/s]\u001b[A\n",
            "epoch 086 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.58it/s]\u001b[A\n",
            "epoch 086 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.20it/s]\u001b[A\n",
            "epoch 086 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.41it/s]\u001b[A\n",
            "epoch 086 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.94it/s]\u001b[A\n",
            "epoch 086 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.54it/s]\u001b[A\n",
            "epoch 086 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.80it/s]\u001b[A\n",
            "epoch 086 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.27it/s]\u001b[A\n",
            "epoch 086 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.04it/s]\u001b[A\n",
            "epoch 086 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.15it/s]\u001b[A\n",
            "epoch 086 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.56it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:15:16 | INFO | dev | epoch 086 | valid on 'dev' subset | loss 4.866 | nll_loss 3.83 | ppl 14.22 | wps 27590.7 | wpb 3571.7 | bsz 11.1 | num_updates 9541 | best_loss 4.353\n",
            "2024-04-18 19:15:16 | INFO | fairseq_cli.train | end of epoch 86 (average epoch stats below)\n",
            "2024-04-18 19:15:16 | INFO | train | epoch 086 | loss 3.433 | nll_loss 2.062 | ppl 4.18 | wps 24078.9 | ups 6.45 | wpb 3732.2 | bsz 10.9 | num_updates 9541 | lr 0.000477055 | gnorm 0.914 | clip 0 | loss_scale 4 | train_wall 10 | gb_free 13.4 | wall 1474\n",
            "2024-04-18 19:15:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:15:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 087:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:15:16 | INFO | fairseq.trainer | begin training epoch 87\n",
            "2024-04-18 19:15:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 087:  99% 110/111 [00:15<00:00,  7.44it/s, loss=3.411, nll_loss=2.032, ppl=4.09, wps=23432.8, ups=6.38, wpb=3671.5, bsz=10.9, num_updates=9600, lr=0.000480004, gnorm=1.478, clip=1, loss_scale=4, train_wall=9, gb_free=12.5, wall=1483]2024-04-18 19:15:32 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:15:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 087 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 087 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.92it/s]\u001b[A\n",
            "epoch 087 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.21it/s]\u001b[A\n",
            "epoch 087 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.86it/s]\u001b[A\n",
            "epoch 087 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.36it/s]\u001b[A\n",
            "epoch 087 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.76it/s]\u001b[A\n",
            "epoch 087 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.32it/s]\u001b[A\n",
            "epoch 087 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.66it/s]\u001b[A\n",
            "epoch 087 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.10it/s]\u001b[A\n",
            "epoch 087 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.92it/s]\u001b[A\n",
            "epoch 087 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.93it/s]\u001b[A\n",
            "epoch 087 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.46it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:15:34 | INFO | dev | epoch 087 | valid on 'dev' subset | loss 4.858 | nll_loss 3.828 | ppl 14.2 | wps 26933.5 | wpb 3571.7 | bsz 11.1 | num_updates 9652 | best_loss 4.353\n",
            "2024-04-18 19:15:34 | INFO | fairseq_cli.train | end of epoch 87 (average epoch stats below)\n",
            "2024-04-18 19:15:34 | INFO | train | epoch 087 | loss 3.415 | nll_loss 2.037 | ppl 4.1 | wps 23942.2 | ups 6.43 | wpb 3721.2 | bsz 10.9 | num_updates 9652 | lr 0.000482603 | gnorm 1.426 | clip 0.9 | loss_scale 4 | train_wall 10 | gb_free 12.9 | wall 1492\n",
            "2024-04-18 19:15:34 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:15:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 088:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:15:34 | INFO | fairseq.trainer | begin training epoch 88\n",
            "2024-04-18 19:15:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 088:  98% 109/111 [00:15<00:00,  6.98it/s, loss=3.416, nll_loss=2.039, ppl=4.11, wps=23863.6, ups=6.32, wpb=3773.8, bsz=11.2, num_updates=9700, lr=0.000485003, gnorm=0.896, clip=0, loss_scale=4, train_wall=9, gb_free=13.6, wall=1498]2024-04-18 19:15:49 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:15:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 088 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 088 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.73it/s]\u001b[A\n",
            "epoch 088 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.38it/s]\u001b[A\n",
            "epoch 088 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.16it/s]\u001b[A\n",
            "epoch 088 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.68it/s]\u001b[A\n",
            "epoch 088 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.12it/s]\u001b[A\n",
            "epoch 088 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.25it/s]\u001b[A\n",
            "epoch 088 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.95it/s]\u001b[A\n",
            "epoch 088 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.33it/s]\u001b[A\n",
            "epoch 088 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.16it/s]\u001b[A\n",
            "epoch 088 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.29it/s]\u001b[A\n",
            "epoch 088 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.82it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:15:51 | INFO | dev | epoch 088 | valid on 'dev' subset | loss 4.892 | nll_loss 3.855 | ppl 14.47 | wps 28513 | wpb 3571.7 | bsz 11.1 | num_updates 9763 | best_loss 4.353\n",
            "2024-04-18 19:15:51 | INFO | fairseq_cli.train | end of epoch 88 (average epoch stats below)\n",
            "2024-04-18 19:15:51 | INFO | train | epoch 088 | loss 3.4 | nll_loss 2.017 | ppl 4.05 | wps 23963.7 | ups 6.44 | wpb 3721.2 | bsz 10.9 | num_updates 9763 | lr 0.000488152 | gnorm 0.894 | clip 0 | loss_scale 4 | train_wall 10 | gb_free 12.7 | wall 1509\n",
            "2024-04-18 19:15:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:15:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 089:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:15:51 | INFO | fairseq.trainer | begin training epoch 89\n",
            "2024-04-18 19:15:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 089:  99% 110/111 [00:15<00:00,  8.63it/s, loss=3.394, nll_loss=2.008, ppl=4.02, wps=23893.7, ups=6.44, wpb=3707.8, bsz=11.2, num_updates=9800, lr=0.000490002, gnorm=0.988, clip=0, loss_scale=4, train_wall=9, gb_free=13.4, wall=1514]2024-04-18 19:16:06 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:16:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 089 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 089 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.92it/s]\u001b[A\n",
            "epoch 089 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.32it/s]\u001b[A\n",
            "epoch 089 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.11it/s]\u001b[A\n",
            "epoch 089 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.64it/s]\u001b[A\n",
            "epoch 089 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.20it/s]\u001b[A\n",
            "epoch 089 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.56it/s]\u001b[A\n",
            "epoch 089 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.86it/s]\u001b[A\n",
            "epoch 089 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.42it/s]\u001b[A\n",
            "epoch 089 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  6.99it/s]\u001b[A\n",
            "epoch 089 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.68it/s]\u001b[A\n",
            "epoch 089 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.94it/s]\u001b[A\n",
            "epoch 089 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.58it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:16:08 | INFO | dev | epoch 089 | valid on 'dev' subset | loss 4.914 | nll_loss 3.884 | ppl 14.76 | wps 27587.2 | wpb 3571.7 | bsz 11.1 | num_updates 9874 | best_loss 4.353\n",
            "2024-04-18 19:16:08 | INFO | fairseq_cli.train | end of epoch 89 (average epoch stats below)\n",
            "2024-04-18 19:16:08 | INFO | train | epoch 089 | loss 3.387 | nll_loss 1.999 | ppl 4 | wps 24131.1 | ups 6.48 | wpb 3721.2 | bsz 10.9 | num_updates 9874 | lr 0.000493701 | gnorm 0.979 | clip 0 | loss_scale 4 | train_wall 10 | gb_free 12.8 | wall 1526\n",
            "2024-04-18 19:16:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:16:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 090:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:16:08 | INFO | fairseq.trainer | begin training epoch 90\n",
            "2024-04-18 19:16:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 090:  99% 110/111 [00:15<00:00,  6.65it/s, loss=3.381, nll_loss=1.99, ppl=3.97, wps=23412.2, ups=6.32, wpb=3702.6, bsz=10.3, num_updates=9900, lr=0.000495001, gnorm=0.91, clip=0, loss_scale=4, train_wall=9, gb_free=13.9, wall=1530]2024-04-18 19:16:23 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:16:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 090 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 090 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.71it/s]\u001b[A\n",
            "epoch 090 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.23it/s]\u001b[A\n",
            "epoch 090 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.02it/s]\u001b[A\n",
            "epoch 090 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.30it/s]\u001b[A\n",
            "epoch 090 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.82it/s]\u001b[A\n",
            "epoch 090 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.41it/s]\u001b[A\n",
            "epoch 090 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.82it/s]\u001b[A\n",
            "epoch 090 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.47it/s]\u001b[A\n",
            "epoch 090 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.24it/s]\u001b[A\n",
            "epoch 090 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.28it/s]\u001b[A\n",
            "epoch 090 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.75it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:16:25 | INFO | dev | epoch 090 | valid on 'dev' subset | loss 4.898 | nll_loss 3.873 | ppl 14.66 | wps 27901.9 | wpb 3571.7 | bsz 11.1 | num_updates 9985 | best_loss 4.353\n",
            "2024-04-18 19:16:25 | INFO | fairseq_cli.train | end of epoch 90 (average epoch stats below)\n",
            "2024-04-18 19:16:25 | INFO | train | epoch 090 | loss 3.374 | nll_loss 1.98 | ppl 3.95 | wps 24054.6 | ups 6.46 | wpb 3721.2 | bsz 10.9 | num_updates 9985 | lr 0.00049925 | gnorm 1.302 | clip 0.9 | loss_scale 4 | train_wall 10 | gb_free 13.7 | wall 1543\n",
            "2024-04-18 19:16:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:16:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 091:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:16:25 | INFO | fairseq.trainer | begin training epoch 91\n",
            "2024-04-18 19:16:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 091:  99% 110/111 [00:15<00:00,  7.91it/s, loss=3.381, nll_loss=1.99, ppl=3.97, wps=24085.4, ups=6.47, wpb=3720.7, bsz=11.2, num_updates=10000, lr=0.0005, gnorm=1.343, clip=1, loss_scale=4, train_wall=9, gb_free=13.9, wall=1545]2024-04-18 19:16:40 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:16:40 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 091 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 091 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.76it/s]\u001b[A\n",
            "epoch 091 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.30it/s]\u001b[A\n",
            "epoch 091 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.01it/s]\u001b[A\n",
            "epoch 091 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.60it/s]\u001b[A\n",
            "epoch 091 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.81it/s]\u001b[A\n",
            "epoch 091 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.37it/s]\u001b[A\n",
            "epoch 091 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.78it/s]\u001b[A\n",
            "epoch 091 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.22it/s]\u001b[A\n",
            "epoch 091 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.09it/s]\u001b[A\n",
            "epoch 091 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.26it/s]\u001b[A\n",
            "epoch 091 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.84it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:16:42 | INFO | dev | epoch 091 | valid on 'dev' subset | loss 4.948 | nll_loss 3.929 | ppl 15.23 | wps 27867.9 | wpb 3571.7 | bsz 11.1 | num_updates 10096 | best_loss 4.353\n",
            "2024-04-18 19:16:42 | INFO | fairseq_cli.train | end of epoch 91 (average epoch stats below)\n",
            "2024-04-18 19:16:42 | INFO | train | epoch 091 | loss 3.357 | nll_loss 1.958 | ppl 3.89 | wps 24250.3 | ups 6.52 | wpb 3721.2 | bsz 10.9 | num_updates 10096 | lr 0.000497617 | gnorm 0.942 | clip 0 | loss_scale 4 | train_wall 10 | gb_free 13.6 | wall 1560\n",
            "2024-04-18 19:16:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:16:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 092:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:16:42 | INFO | fairseq.trainer | begin training epoch 92\n",
            "2024-04-18 19:16:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 092:  99% 110/111 [00:15<00:00,  6.24it/s, loss=3.328, nll_loss=1.917, ppl=3.78, wps=27124.5, ups=7.32, wpb=3706.5, bsz=10.9, num_updates=10200, lr=0.000495074, gnorm=0.908, clip=0, loss_scale=4, train_wall=9, gb_free=13.6, wall=1574]2024-04-18 19:16:58 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:16:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 092 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 092 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.75it/s]\u001b[A\n",
            "epoch 092 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.31it/s]\u001b[A\n",
            "epoch 092 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.15it/s]\u001b[A\n",
            "epoch 092 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.91it/s]\u001b[A\n",
            "epoch 092 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.06it/s]\u001b[A\n",
            "epoch 092 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.44it/s]\u001b[A\n",
            "epoch 092 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.78it/s]\u001b[A\n",
            "epoch 092 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.36it/s]\u001b[A\n",
            "epoch 092 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.96it/s]\u001b[A\n",
            "epoch 092 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  6.94it/s]\u001b[A\n",
            "epoch 092 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.52it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:16:59 | INFO | dev | epoch 092 | valid on 'dev' subset | loss 4.936 | nll_loss 3.915 | ppl 15.08 | wps 27574.8 | wpb 3571.7 | bsz 11.1 | num_updates 10207 | best_loss 4.353\n",
            "2024-04-18 19:16:59 | INFO | fairseq_cli.train | end of epoch 92 (average epoch stats below)\n",
            "2024-04-18 19:16:59 | INFO | train | epoch 092 | loss 3.333 | nll_loss 1.924 | ppl 3.8 | wps 24112.9 | ups 6.48 | wpb 3721.2 | bsz 10.9 | num_updates 10207 | lr 0.000494904 | gnorm 0.958 | clip 0 | loss_scale 4 | train_wall 10 | gb_free 13.2 | wall 1577\n",
            "2024-04-18 19:16:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:16:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 093:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:16:59 | INFO | fairseq.trainer | begin training epoch 93\n",
            "2024-04-18 19:16:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 093:  99% 110/111 [00:15<00:00,  7.06it/s, loss=3.325, nll_loss=1.914, ppl=3.77, wps=23765.1, ups=6.31, wpb=3764.3, bsz=11.1, num_updates=10300, lr=0.000492665, gnorm=0.966, clip=0, loss_scale=4, train_wall=9, gb_free=13, wall=1590]2024-04-18 19:17:15 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:17:15 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 093 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 093 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.85it/s]\u001b[A\n",
            "epoch 093 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.42it/s]\u001b[A\n",
            "epoch 093 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.27it/s]\u001b[A\n",
            "epoch 093 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.67it/s]\u001b[A\n",
            "epoch 093 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.10it/s]\u001b[A\n",
            "epoch 093 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.41it/s]\u001b[A\n",
            "epoch 093 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.95it/s]\u001b[A\n",
            "epoch 093 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.41it/s]\u001b[A\n",
            "epoch 093 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.17it/s]\u001b[A\n",
            "epoch 093 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.28it/s]\u001b[A\n",
            "epoch 093 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.78it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:17:16 | INFO | dev | epoch 093 | valid on 'dev' subset | loss 4.982 | nll_loss 3.958 | ppl 15.54 | wps 28162.1 | wpb 3571.7 | bsz 11.1 | num_updates 10318 | best_loss 4.353\n",
            "2024-04-18 19:17:16 | INFO | fairseq_cli.train | end of epoch 93 (average epoch stats below)\n",
            "2024-04-18 19:17:16 | INFO | train | epoch 093 | loss 3.319 | nll_loss 1.905 | ppl 3.75 | wps 24099.5 | ups 6.48 | wpb 3721.2 | bsz 10.9 | num_updates 10318 | lr 0.000492235 | gnorm 0.987 | clip 0 | loss_scale 4 | train_wall 10 | gb_free 13.9 | wall 1594\n",
            "2024-04-18 19:17:16 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:17:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 094:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:17:16 | INFO | fairseq.trainer | begin training epoch 94\n",
            "2024-04-18 19:17:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 094:  99% 110/111 [00:15<00:00,  7.90it/s, loss=3.287, nll_loss=1.861, ppl=3.63, wps=23897.6, ups=6.48, wpb=3690.3, bsz=10.6, num_updates=10400, lr=0.00049029, gnorm=1.001, clip=0, loss_scale=4, train_wall=9, gb_free=13.6, wall=1606]2024-04-18 19:17:32 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:17:32 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 094 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 094 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.54it/s]\u001b[A\n",
            "epoch 094 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.22it/s]\u001b[A\n",
            "epoch 094 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.86it/s]\u001b[A\n",
            "epoch 094 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.67it/s]\u001b[A\n",
            "epoch 094 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.09it/s]\u001b[A\n",
            "epoch 094 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.51it/s]\u001b[A\n",
            "epoch 094 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.83it/s]\u001b[A\n",
            "epoch 094 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.44it/s]\u001b[A\n",
            "epoch 094 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.07it/s]\u001b[A\n",
            "epoch 094 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.83it/s]\u001b[A\n",
            "epoch 094 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.03it/s]\u001b[A\n",
            "epoch 094 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.69it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:17:33 | INFO | dev | epoch 094 | valid on 'dev' subset | loss 4.981 | nll_loss 3.961 | ppl 15.57 | wps 27866.7 | wpb 3571.7 | bsz 11.1 | num_updates 10429 | best_loss 4.353\n",
            "2024-04-18 19:17:33 | INFO | fairseq_cli.train | end of epoch 94 (average epoch stats below)\n",
            "2024-04-18 19:17:33 | INFO | train | epoch 094 | loss 3.295 | nll_loss 1.873 | ppl 3.66 | wps 24175.1 | ups 6.5 | wpb 3721.2 | bsz 10.9 | num_updates 10429 | lr 0.000489608 | gnorm 0.925 | clip 0 | loss_scale 4 | train_wall 10 | gb_free 13.4 | wall 1611\n",
            "2024-04-18 19:17:33 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:17:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 095:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:17:34 | INFO | fairseq.trainer | begin training epoch 95\n",
            "2024-04-18 19:17:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 095:  99% 110/111 [00:15<00:00,  6.84it/s, loss=3.279, nll_loss=1.851, ppl=3.61, wps=23847.5, ups=6.44, wpb=3703.3, bsz=10.8, num_updates=10500, lr=0.00048795, gnorm=0.936, clip=0, loss_scale=4, train_wall=9, gb_free=13.7, wall=1621]2024-04-18 19:17:49 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:17:49 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 095 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 095 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.80it/s]\u001b[A\n",
            "epoch 095 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.21it/s]\u001b[A\n",
            "epoch 095 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.88it/s]\u001b[A\n",
            "epoch 095 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.27it/s]\u001b[A\n",
            "epoch 095 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.78it/s]\u001b[A\n",
            "epoch 095 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.30it/s]\u001b[A\n",
            "epoch 095 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  8.74it/s]\u001b[A\n",
            "epoch 095 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.37it/s]\u001b[A\n",
            "epoch 095 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.06it/s]\u001b[A\n",
            "epoch 095 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.99it/s]\u001b[A\n",
            "epoch 095 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.06it/s]\u001b[A\n",
            "epoch 095 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:17:51 | INFO | dev | epoch 095 | valid on 'dev' subset | loss 4.95 | nll_loss 3.933 | ppl 15.28 | wps 27541.6 | wpb 3571.7 | bsz 11.1 | num_updates 10540 | best_loss 4.353\n",
            "2024-04-18 19:17:51 | INFO | fairseq_cli.train | end of epoch 95 (average epoch stats below)\n",
            "2024-04-18 19:17:51 | INFO | train | epoch 095 | loss 3.28 | nll_loss 1.852 | ppl 3.61 | wps 24046.7 | ups 6.46 | wpb 3721.2 | bsz 10.9 | num_updates 10540 | lr 0.000487023 | gnorm 0.955 | clip 0 | loss_scale 4 | train_wall 10 | gb_free 12.6 | wall 1629\n",
            "2024-04-18 19:17:51 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:17:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 096:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:17:51 | INFO | fairseq.trainer | begin training epoch 96\n",
            "2024-04-18 19:17:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 096:  98% 109/111 [00:15<00:00,  6.98it/s, loss=3.268, nll_loss=1.836, ppl=3.57, wps=23631, ups=6.38, wpb=3701.7, bsz=11.1, num_updates=10600, lr=0.000485643, gnorm=1.023, clip=0, loss_scale=4, train_wall=9, gb_free=13.8, wall=1637]2024-04-18 19:18:06 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:18:06 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 096 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 096 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.81it/s]\u001b[A\n",
            "epoch 096 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.40it/s]\u001b[A\n",
            "epoch 096 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.34it/s]\u001b[A\n",
            "epoch 096 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.54it/s]\u001b[A\n",
            "epoch 096 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.01it/s]\u001b[A\n",
            "epoch 096 | valid on 'dev' subset:  58% 7/12 [00:00<00:00,  9.33it/s]\u001b[A\n",
            "epoch 096 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.83it/s]\u001b[A\n",
            "epoch 096 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.25it/s]\u001b[A\n",
            "epoch 096 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.10it/s]\u001b[A\n",
            "epoch 096 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.19it/s]\u001b[A\n",
            "epoch 096 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.63it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:18:08 | INFO | dev | epoch 096 | valid on 'dev' subset | loss 5.003 | nll_loss 3.98 | ppl 15.78 | wps 28125.7 | wpb 3571.7 | bsz 11.1 | num_updates 10651 | best_loss 4.353\n",
            "2024-04-18 19:18:08 | INFO | fairseq_cli.train | end of epoch 96 (average epoch stats below)\n",
            "2024-04-18 19:18:08 | INFO | train | epoch 096 | loss 3.267 | nll_loss 1.834 | ppl 3.57 | wps 24238.3 | ups 6.51 | wpb 3721.2 | bsz 10.9 | num_updates 10651 | lr 0.000484479 | gnorm 1.038 | clip 0 | loss_scale 4 | train_wall 10 | gb_free 13.1 | wall 1646\n",
            "2024-04-18 19:18:08 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:18:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 097:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:18:08 | INFO | fairseq.trainer | begin training epoch 97\n",
            "2024-04-18 19:18:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 097:  85% 94/111 [00:13<00:02,  6.45it/s, loss=3.262, nll_loss=1.828, ppl=3.55, wps=23838.1, ups=6.31, wpb=3777.6, bsz=10.9, num_updates=10700, lr=0.000483368, gnorm=0.973, clip=0, loss_scale=4, train_wall=9, gb_free=11.5, wall=1653]2024-04-18 19:18:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0\n",
            "epoch 097:  99% 110/111 [00:15<00:00,  8.25it/s, loss=3.262, nll_loss=1.828, ppl=3.55, wps=23838.1, ups=6.31, wpb=3777.6, bsz=10.9, num_updates=10700, lr=0.000483368, gnorm=0.973, clip=0, loss_scale=4, train_wall=9, gb_free=11.5, wall=1653]2024-04-18 19:18:23 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:18:23 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 097 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 097 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.84it/s]\u001b[A\n",
            "epoch 097 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.18it/s]\u001b[A\n",
            "epoch 097 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.87it/s]\u001b[A\n",
            "epoch 097 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.39it/s]\u001b[A\n",
            "epoch 097 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.92it/s]\u001b[A\n",
            "epoch 097 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.47it/s]\u001b[A\n",
            "epoch 097 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.66it/s]\u001b[A\n",
            "epoch 097 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.26it/s]\u001b[A\n",
            "epoch 097 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.09it/s]\u001b[A\n",
            "epoch 097 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.12it/s]\u001b[A\n",
            "epoch 097 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.54it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:18:25 | INFO | dev | epoch 097 | valid on 'dev' subset | loss 5.044 | nll_loss 4.04 | ppl 16.45 | wps 27348.4 | wpb 3571.7 | bsz 11.1 | num_updates 10761 | best_loss 4.353\n",
            "2024-04-18 19:18:25 | INFO | fairseq_cli.train | end of epoch 97 (average epoch stats below)\n",
            "2024-04-18 19:18:25 | INFO | train | epoch 097 | loss 3.242 | nll_loss 1.801 | ppl 3.48 | wps 23447.4 | ups 6.31 | wpb 3715.9 | bsz 10.9 | num_updates 10761 | lr 0.000481996 | gnorm 0.935 | clip 0 | loss_scale 2 | train_wall 10 | gb_free 13.1 | wall 1663\n",
            "2024-04-18 19:18:25 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:18:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 098:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:18:25 | INFO | fairseq.trainer | begin training epoch 98\n",
            "2024-04-18 19:18:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 098:  99% 110/111 [00:15<00:00,  8.36it/s, loss=3.242, nll_loss=1.8, ppl=3.48, wps=23361.5, ups=6.28, wpb=3721, bsz=10.9, num_updates=10800, lr=0.000481125, gnorm=0.945, clip=0, loss_scale=2, train_wall=9, gb_free=12.2, wall=1669]2024-04-18 19:18:41 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:18:41 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 098 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 098 | valid on 'dev' subset:   8% 1/12 [00:00<00:02,  5.45it/s]\u001b[A\n",
            "epoch 098 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.12it/s]\u001b[A\n",
            "epoch 098 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.99it/s]\u001b[A\n",
            "epoch 098 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.49it/s]\u001b[A\n",
            "epoch 098 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.18it/s]\u001b[A\n",
            "epoch 098 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.70it/s]\u001b[A\n",
            "epoch 098 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  8.05it/s]\u001b[A\n",
            "epoch 098 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.53it/s]\u001b[A\n",
            "epoch 098 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.18it/s]\u001b[A\n",
            "epoch 098 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.26it/s]\u001b[A\n",
            "epoch 098 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.80it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:18:42 | INFO | dev | epoch 098 | valid on 'dev' subset | loss 5.043 | nll_loss 4.041 | ppl 16.46 | wps 28324.9 | wpb 3571.7 | bsz 11.1 | num_updates 10872 | best_loss 4.353\n",
            "2024-04-18 19:18:42 | INFO | fairseq_cli.train | end of epoch 98 (average epoch stats below)\n",
            "2024-04-18 19:18:42 | INFO | train | epoch 098 | loss 3.225 | nll_loss 1.778 | ppl 3.43 | wps 23821.1 | ups 6.4 | wpb 3721.2 | bsz 10.9 | num_updates 10872 | lr 0.000479529 | gnorm 0.938 | clip 0 | loss_scale 2 | train_wall 10 | gb_free 11.5 | wall 1680\n",
            "2024-04-18 19:18:42 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:18:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 099:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:18:43 | INFO | fairseq.trainer | begin training epoch 99\n",
            "2024-04-18 19:18:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 099:  99% 110/111 [00:14<00:00,  7.82it/s, loss=3.229, nll_loss=1.783, ppl=3.44, wps=23913.8, ups=6.41, wpb=3730, bsz=11, num_updates=10900, lr=0.000478913, gnorm=0.944, clip=0, loss_scale=2, train_wall=9, gb_free=13.5, wall=1684]2024-04-18 19:18:58 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:18:58 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 099 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 099 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.92it/s]\u001b[A\n",
            "epoch 099 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.47it/s]\u001b[A\n",
            "epoch 099 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  7.23it/s]\u001b[A\n",
            "epoch 099 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.64it/s]\u001b[A\n",
            "epoch 099 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  8.01it/s]\u001b[A\n",
            "epoch 099 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  8.35it/s]\u001b[A\n",
            "epoch 099 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.82it/s]\u001b[A\n",
            "epoch 099 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.40it/s]\u001b[A\n",
            "epoch 099 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  7.09it/s]\u001b[A\n",
            "epoch 099 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.03it/s]\u001b[A\n",
            "epoch 099 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.57it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:18:59 | INFO | dev | epoch 099 | valid on 'dev' subset | loss 5.051 | nll_loss 4.056 | ppl 16.63 | wps 27644.1 | wpb 3571.7 | bsz 11.1 | num_updates 10983 | best_loss 4.353\n",
            "2024-04-18 19:18:59 | INFO | fairseq_cli.train | end of epoch 99 (average epoch stats below)\n",
            "2024-04-18 19:18:59 | INFO | train | epoch 099 | loss 3.212 | nll_loss 1.759 | ppl 3.39 | wps 24688.1 | ups 6.63 | wpb 3721.2 | bsz 10.9 | num_updates 10983 | lr 0.0004771 | gnorm 0.993 | clip 0 | loss_scale 2 | train_wall 10 | gb_free 12.8 | wall 1697\n",
            "2024-04-18 19:18:59 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:18:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 111\n",
            "epoch 100:   0% 0/111 [00:00<?, ?it/s]2024-04-18 19:18:59 | INFO | fairseq.trainer | begin training epoch 100\n",
            "2024-04-18 19:18:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 100:  99% 110/111 [00:14<00:00,  7.32it/s, loss=3.201, nll_loss=1.745, ppl=3.35, wps=24150.8, ups=6.57, wpb=3677.2, bsz=10.8, num_updates=11000, lr=0.000476731, gnorm=0.996, clip=0, loss_scale=2, train_wall=9, gb_free=12.3, wall=1699]2024-04-18 19:19:14 | INFO | fairseq_cli.train | begin validation on \"dev\" subset\n",
            "2024-04-18 19:19:14 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "\n",
            "epoch 100 | valid on 'dev' subset:   0% 0/12 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 100 | valid on 'dev' subset:   8% 1/12 [00:00<00:01,  5.79it/s]\u001b[A\n",
            "epoch 100 | valid on 'dev' subset:  17% 2/12 [00:00<00:01,  6.19it/s]\u001b[A\n",
            "epoch 100 | valid on 'dev' subset:  25% 3/12 [00:00<00:01,  6.99it/s]\u001b[A\n",
            "epoch 100 | valid on 'dev' subset:  33% 4/12 [00:00<00:01,  7.46it/s]\u001b[A\n",
            "epoch 100 | valid on 'dev' subset:  42% 5/12 [00:00<00:00,  7.91it/s]\u001b[A\n",
            "epoch 100 | valid on 'dev' subset:  50% 6/12 [00:00<00:00,  7.83it/s]\u001b[A\n",
            "epoch 100 | valid on 'dev' subset:  67% 8/12 [00:01<00:00,  7.55it/s]\u001b[A\n",
            "epoch 100 | valid on 'dev' subset:  75% 9/12 [00:01<00:00,  7.23it/s]\u001b[A\n",
            "epoch 100 | valid on 'dev' subset:  83% 10/12 [00:01<00:00,  6.94it/s]\u001b[A\n",
            "epoch 100 | valid on 'dev' subset:  92% 11/12 [00:01<00:00,  7.00it/s]\u001b[A\n",
            "epoch 100 | valid on 'dev' subset: 100% 12/12 [00:01<00:00,  7.54it/s]\u001b[A\n",
            "                                                                      \u001b[A2024-04-18 19:19:16 | INFO | dev | epoch 100 | valid on 'dev' subset | loss 5.058 | nll_loss 4.054 | ppl 16.61 | wps 27112.5 | wpb 3571.7 | bsz 11.1 | num_updates 11094 | best_loss 4.353\n",
            "2024-04-18 19:19:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 100 @ 11094 updates\n",
            "2024-04-18 19:19:16 | INFO | fairseq.trainer | Saving checkpoint to /content/SAVE_DIR_TRAINING/checkpoint100.pt\n",
            "2024-04-18 19:19:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/SAVE_DIR_TRAINING/checkpoint100.pt\n",
            "2024-04-18 19:19:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/SAVE_DIR_TRAINING/checkpoint100.pt (epoch 100 @ 11094 updates, score 5.058) (writing took 1.1269292639999549 seconds)\n",
            "2024-04-18 19:19:17 | INFO | fairseq_cli.train | end of epoch 100 (average epoch stats below)\n",
            "2024-04-18 19:19:17 | INFO | train | epoch 100 | loss 3.19 | nll_loss 1.73 | ppl 3.32 | wps 23064.3 | ups 6.2 | wpb 3721.2 | bsz 10.9 | num_updates 11094 | lr 0.000474707 | gnorm 0.941 | clip 0 | loss_scale 2 | train_wall 10 | gb_free 13.2 | wall 1715\n",
            "2024-04-18 19:19:17 | INFO | fairseq.tasks.fairseq_task | can_reuse_epoch_itr = True\n",
            "2024-04-18 19:19:17 | INFO | fairseq_cli.train | done training in 1711.8 seconds\n"
          ]
        }
      ],
      "source": [
        "# training about to start\n",
        "!cd /content/fairseq && fairseq-train /content/DATA_ROOT  --config-yaml /content/DATA_ROOT/config.yaml --task speech_to_speech --target-is-code --target-code-size 100 --vocoder code_hifigan --criterion speech_to_unit --label-smoothing 0.2 --arch s2ut_transformer_fisher --share-decoder-input-output-embed --dropout 0.1 --attention-dropout 0.1 --relu-dropout 0.1 --train-subset train --valid-subset dev --save-dir /content/SAVE_DIR_TRAINING --lr 0.0005 --lr-scheduler inverse_sqrt --warmup-init-lr 1e-7 --warmup-updates 10000 --optimizer adam --adam-betas \"(0.9,0.98)\" --clip-norm 10.0 --max-update 400000 --max-tokens 20000 --max-target-positions 3000 --update-freq 1 --seed 1 --fp16 --num-workers 1 --max-epoch 100 --save-interval 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPZ9OPYvsjCg"
      },
      "outputs": [],
      "source": [
        "! cp -r /content/SAVE_DIR_TRAINING /content/drive/MyDrive/attempt2works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JjwCTQKIijm"
      },
      "outputs": [],
      "source": [
        "! cp -r /content/DATA_ROOT /content/drive/MyDrive/attempt2works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w09Ne7feIbTs"
      },
      "outputs": [],
      "source": [
        "! rm -rf /content/RESULT_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb5dMFIUs2YE",
        "outputId": "99b03f91-87b4-483c-80fa-c834bea537f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-18 19:25:49.901229: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-18 19:25:49.901289: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-18 19:25:49.902792: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-18 19:25:49.910710: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-18 19:25:51.077618: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=UNKNOWN_NAME\n",
            "DEBUG:hydra.core.utils:Setting JobRuntime:name=utils\n",
            "INFO:fairseq_cli.generate:{'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': '/content/SAVE_DIR_TRAINING/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': '/content/RESULT_PATH'}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 50000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 50000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False, 'debug_param_names': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 10, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 1.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': {'_name': 'wav2vec2', 'extractor_mode': 'default', 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_type': 'transformer', 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.0, 'final_dim': 0, 'layer_norm_first': False, 'conv_feature_layers': '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] + [(512,2,2)]', 'conv_bias': False, 'logit_temp': 0.1, 'quantize_targets': False, 'quantize_input': False, 'same_quantizer': False, 'target_glu': False, 'feature_grad_mult': 1.0, 'quantizer_depth': 1, 'quantizer_factor': 3, 'latent_vars': 320, 'latent_groups': 2, 'latent_dim': 0, 'mask_length': 10, 'mask_prob': 0.65, 'mask_selection': 'static', 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'require_same_masks': True, 'mask_dropout': 0.0, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_before': False, 'mask_channel_selection': 'static', 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'num_negatives': 100, 'negatives_from_everywhere': False, 'cross_sample_negatives': 0, 'codebook_negatives': 0, 'conv_pos': 128, 'conv_pos_groups': 16, 'pos_conv_depth': 1, 'latent_temp': [2.0, 0.5, 0.999995], 'max_positions': 100000, 'checkpoint_activations': False, 'required_seq_len_multiple': 1, 'crop_seq_to_multiple': 1, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False, 'adp_num': -1, 'adp_dim': 64, 'adp_act_fn': 'relu', 'adp_trf_idx': 'all'}, 'task': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', simul_type=None, scoring='bleu', task='speech_to_speech', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=50000, batch_size=None, required_batch_size_multiple=8, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=50000, batch_size_valid=None, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=False, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, path='/content/SAVE_DIR_TRAINING/checkpoint_best.pt', post_process=None, quiet=False, model_overrides='{}', results_path='/content/RESULT_PATH', beam=10, beam_mt=0, nbest=1, max_len_a=1.0, max_len_b=200, max_len_a_mt=0, max_len_b_mt=200, min_len=1, match_source_len=False, unnormalized=False, no_early_stop=False, no_beamable_mm=False, lenpen=1, lenpen_mt=1, unkpen=0, replace_unk=None, sacrebleu=False, score_reference=False, prefix_size=0, no_repeat_ngram_size=0, sampling=False, sampling_topk=-1, sampling_topp=-1.0, constraints=None, temperature=1.0, diverse_beam_groups=-1, diverse_beam_strength=0.5, diversity_rate=-1.0, print_alignment=None, print_step=False, lm_path=None, lm_weight=0.0, iter_decode_eos_penalty=0.0, iter_decode_max_iter=10, iter_decode_force_max_iter=False, iter_decode_with_beam=1, iter_decode_with_external_reranker=False, retain_iter_history=False, retain_dropout=False, retain_dropout_modules=None, decoding_format=None, no_seed_provided=False, eos_token=None, save_dir='checkpoints', restore_file='checkpoint_last.pt', continue_once=None, finetune_from_model=None, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='loss', maximize_best_checkpoint_metric=False, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, arch='wav2vec2', data='/content/DATA_ROOT', config_yaml='config.yaml', multitask_config_yaml=None, max_source_positions=6000, max_target_positions=1024, target_is_code=True, target_code_size=100, n_frames_per_step=1, eval_inference=False, eval_args='{}', eos_prob_threshold=0.5, mcd_normalize_type='targ', vocoder='code_hifigan', spec_bwd_max_iter=8, infer_target_lang='', force_anneal=None, lr_shrink=0.1, warmup_updates=0, pad=1, eos=2, unk=3, _name='speech_to_speech'), 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "INFO:fairseq.tasks.speech_to_speech:dictionary size: 104\n",
            "INFO:fairseq_cli.generate:loading model(s) from /content/SAVE_DIR_TRAINING/checkpoint_best.pt\n",
            "WARNING:fairseq.data.audio.data_cfg:Auto converting transforms into feature_transforms, but transforms will be deprecated in the future. Please update this in the config.\n",
            "INFO:fairseq.data.audio.speech_to_text_dataset:'test' has 0.00% OOV\n",
            "INFO:fairseq.data.audio.speech_to_text_dataset:SpeechToSpeechDataset(split=\"test\", n_samples=292, prepend_tgt_lang_tag=False, n_frames_per_step=1, shuffle=False, feature_transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "), waveform_transforms=None, dataset_transforms=CompositeAudioDatasetTransform(\n",
            "))\n",
            "INFO:fairseq.data.audio.speech_to_speech_dataset:SpeechToSpeechDataset(split=\"test\", n_samples=292, prepend_tgt_lang_tag=False, n_frames_per_step=1, shuffle=False, feature_transforms=CompositeAudioFeatureTransform(\n",
            "    UtteranceCMVN(norm_means=True, norm_vars=True)\n",
            "), waveform_transforms=None, dataset_transforms=CompositeAudioDatasetTransform(\n",
            "))\n",
            "INFO:fairseq.tasks.fairseq_task:can_reuse_epoch_itr = True\n",
            "INFO:fairseq.tasks.fairseq_task:reuse_dataloader = True\n",
            "INFO:fairseq.tasks.fairseq_task:rebuild_batches = False\n",
            "INFO:fairseq.tasks.fairseq_task:creating new batches for epoch 1\n",
            "  0% 0/10 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg6\n",
            "DEBUG:torio._extension.utils:Failed to load FFmpeg6 extension.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 116, in _find_ffmpeg_extension\n",
            "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
            "    _load_lib(lib)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 94, in _load_lib\n",
            "    torch.ops.load_library(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 933, in load_library\n",
            "    ctypes.CDLL(path)\n",
            "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: libavutil.so.58: cannot open shared object file: No such file or directory\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg5\n",
            "DEBUG:torio._extension.utils:Failed to load FFmpeg5 extension.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 116, in _find_ffmpeg_extension\n",
            "    ext = _find_versionsed_ffmpeg_extension(ffmpeg_ver)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 108, in _find_versionsed_ffmpeg_extension\n",
            "    _load_lib(lib)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torio/_extension/utils.py\", line 94, in _load_lib\n",
            "    torch.ops.load_library(path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 933, in load_library\n",
            "    ctypes.CDLL(path)\n",
            "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "OSError: libavutil.so.57: cannot open shared object file: No such file or directory\n",
            "DEBUG:torio._extension.utils:Loading FFmpeg4\n",
            "DEBUG:torio._extension.utils:Successfully loaded FFmpeg4\n",
            "INFO:fairseq_cli.generate:NOTE: hypothesis and token scores are output in base 2\n",
            "INFO:fairseq_cli.generate:Translated 288 sentences (134,762 tokens) in 173.8s (1.66 sentences/s, 775.44 tokens/s)\n"
          ]
        }
      ],
      "source": [
        "# training done, create RESULT_PATH\n",
        "! cd /content/fairseq && fairseq-generate /content/DATA_ROOT  --config-yaml config.yaml --task speech_to_speech --target-is-code --target-code-size 100 --vocoder code_hifigan --path /content/SAVE_DIR_TRAINING/checkpoint_best.pt  --gen-subset test --max-tokens 50000 --beam 10 --max-len-a 1 --results-path /content/RESULT_PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chmrBdFouhie"
      },
      "outputs": [],
      "source": [
        "!grep \"^D\\-\" /content/RESULT_PATH/generate-test.txt | sed 's/^D-//ig' | sort -nk1 | cut -f3  > /content/RESULT_PATH/generate-test.unit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsHjYnysu1Zw"
      },
      "outputs": [],
      "source": [
        "! cp -r /content/RESULT_PATH /content/drive/MyDrive/attempt2_kmeans100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS5DhO62u-FV"
      },
      "outputs": [],
      "source": [
        "# generating wavs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wdnjl3ju_bY",
        "outputId": "690fab07-e651-4d0c-dc99-bb16292f001f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-18 19:29:40.810989: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-18 19:29:40.811041: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-18 19:29:40.812363: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-18 19:29:40.819955: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-18 19:29:41.973130: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "INFO:fairseq.tasks.text_to_speech:Please install tensorboardX: pip install tensorboardX\n",
            "INFO:__main__:Namespace(in_code_file='/content/RESULT_PATH/generate-test.unit', vocoder='/content/drive/MyDrive/g_00500000', vocoder_cfg='/content/drive/MyDrive/config.json', results_path='/content/RESULT_PATH/REVISED_AUDIOS', dur_prediction=True, speaker_id=-1, cpu=False)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
            "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
            "Removing weight norm...\n",
            "INFO:fairseq.models.text_to_speech.vocoder:loaded CodeHiFiGAN checkpoint from /content/drive/MyDrive/g_00500000\n",
            "100% 288/288 [00:57<00:00,  5.03it/s]\n"
          ]
        }
      ],
      "source": [
        "!cd /content/fairseq && python examples/speech_to_speech/generate_waveform_from_code.py --in-code-file /content/RESULT_PATH/generate-test.unit --vocoder /content/drive/MyDrive/g_00500000 --vocoder-cfg /content/drive/MyDrive/config.json --results-path /content/RESULT_PATH/REVISED_AUDIOS --dur-prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g_s2DinvZAl"
      },
      "outputs": [],
      "source": [
        "! cp -r /content/RESULT_PATH/REVISED_AUDIOS /content/drive/MyDrive/attempt2_kmeans100/RESULT_WITH_AUDIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5R8y00ax142"
      },
      "outputs": [],
      "source": [
        "rm -rf /content/RESULT_PATH"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}